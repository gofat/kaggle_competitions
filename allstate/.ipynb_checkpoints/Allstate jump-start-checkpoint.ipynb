{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((188318, 132),\n",
       " Index([u'id', u'cat1', u'cat2', u'cat3', u'cat4', u'cat5', u'cat6', u'cat7',\n",
       "        u'cat8', u'cat9',\n",
       "        ...\n",
       "        u'cont6', u'cont7', u'cont8', u'cont9', u'cont10', u'cont11', u'cont12',\n",
       "        u'cont13', u'cont14', u'loss'],\n",
       "       dtype='object', length=132))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((125546, 131),\n",
       " Index([u'id', u'cat1', u'cat2', u'cat3', u'cat4', u'cat5', u'cat6', u'cat7',\n",
       "        u'cat8', u'cat9',\n",
       "        ...\n",
       "        u'cont5', u'cont6', u'cont7', u'cont8', u'cont9', u'cont10', u'cont11',\n",
       "        u'cont12', u'cont13', u'cont14'],\n",
       "       dtype='object', length=131))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape, test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([     1,      2,      5, ..., 587630, 587632, 587633]),\n",
       " array([     4,      6,      9, ..., 587627, 587629, 587634]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['id'].values, test['id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2213.18,  1283.6 ,  3005.09, ...,  5762.64,  1562.87,  4751.72])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['loss'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3037.3376856699792, 2115.5699999999997)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['loss'].mean(), train['loss'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probably loss is in dollars\n",
    "### Let's look to the target distribution itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#_ = plt.hist(train['loss'], bins = 100, log = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution is skewed, let's look at target logarithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#_ = plt.hist(np.log(train['loss']), bins = 100, log = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How dataset features looks like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for column in train.columns :\n",
    "#    if train[column].nunique() < 60 :\n",
    "#        print column, np.array(sorted(train[column].unique()))\n",
    "#    else :\n",
    "#        print column, train[column].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/ubuntu/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "train['low'] = [1 if x < 1000 else 0 for x in train.loss]\n",
    "train['high'] = [1 if x > 5000 else 0 for x in train.loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat10</th>\n",
       "      <th>cat100</th>\n",
       "      <th>cat101</th>\n",
       "      <th>cat102</th>\n",
       "      <th>cat103</th>\n",
       "      <th>cat104</th>\n",
       "      <th>cat105</th>\n",
       "      <th>cat106</th>\n",
       "      <th>cat107</th>\n",
       "      <th>...</th>\n",
       "      <th>cont4</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>high</th>\n",
       "      <th>id</th>\n",
       "      <th>loss</th>\n",
       "      <th>low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789639</td>\n",
       "      <td>0.310061</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2213.18</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.614134</td>\n",
       "      <td>0.885834</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1283.60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236924</td>\n",
       "      <td>0.397069</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3005.09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.373816</td>\n",
       "      <td>0.422268</td>\n",
       "      <td>0.440945</td>\n",
       "      <td>0.391128</td>\n",
       "      <td>0.31796</td>\n",
       "      <td>0.32128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>939.85</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.473202</td>\n",
       "      <td>0.704268</td>\n",
       "      <td>0.178193</td>\n",
       "      <td>0.247408</td>\n",
       "      <td>0.24564</td>\n",
       "      <td>0.22089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>2763.85</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 134 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat1  cat10  cat100  cat101  cat102  cat103  cat104  cat105  cat106  \\\n",
       "0     0      0       1       6       0       0       8       4       6   \n",
       "1     0      1      11       5       0       0       4       4       8   \n",
       "2     0      1      11      14       0       1       4       5       7   \n",
       "3     1      0       8       3       0       0       4       4       8   \n",
       "4     0      1       5       9       0       0       3       4      10   \n",
       "\n",
       "   cat107 ...      cont4     cont5     cont6     cont7    cont8    cont9  \\\n",
       "0       9 ...   0.789639  0.310061  0.718367  0.335060  0.30260  0.67135   \n",
       "1      10 ...   0.614134  0.885834  0.438917  0.436585  0.60087  0.35127   \n",
       "2       5 ...   0.236924  0.397069  0.289648  0.315545  0.27320  0.26076   \n",
       "3      10 ...   0.373816  0.422268  0.440945  0.391128  0.31796  0.32128   \n",
       "4       6 ...   0.473202  0.704268  0.178193  0.247408  0.24564  0.22089   \n",
       "\n",
       "   high  id     loss  low  \n",
       "0   0.0   1  2213.18  0.0  \n",
       "1   0.0   2  1283.60  0.0  \n",
       "2   0.0   5  3005.09  0.0  \n",
       "3   0.0  10   939.85  1.0  \n",
       "4   0.0  11  2763.85  0.0  \n",
       "\n",
       "[5 rows x 134 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for column in all_data :\n",
    "    encoder = LabelEncoder()\n",
    "    if column.startswith('cat') :\n",
    "        all_data[column] = encoder.fit_transform(all_data[column])\n",
    "        \n",
    "all_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = all_data[:train.shape[0]]\n",
    "test = all_data[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold out set for parameters tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_local, validation = train_test_split(\n",
    "    train, \n",
    "    test_size = 0.2, \n",
    "    random_state = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best constant result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#scores = []\n",
    "\n",
    "#for C in np.linspace(1000, 4000, 301) :\n",
    "#    p = np.ones(validation.shape[0]) * C\n",
    "#    score = mean_absolute_error(p, validation['loss'])\n",
    "#    scores.append((score, C))\n",
    "\n",
    "#print 'Min error: %.2f, optimal constant prediction: %.2f' % min(scores)\n",
    "#print 'Mean target: %.2f, median target: %.2f' % (validation['loss'].mean(), validation['loss'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try GBDT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = np.array([column for column in all_data.columns if column != 'loss'])\n",
    "#features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Increase iterations number and max depth, decrease learning rate. Without justification\n",
    "\n",
    "model = XGBRegressor(max_depth = 8, learning_rate = 0.1, n_estimators = 1000, silent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 30s, sys: 330 ms, total: 4min 31s\n",
      "Wall time: 2min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=8,\n",
       "       min_child_weight=1, missing=None, n_estimators=1000, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=False, subsample=1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "model.fit(\n",
    "    train_local[features].values, \n",
    "    np.log(train_local['loss'].values), \n",
    "    eval_set = [(\n",
    "        validation[features].values, \n",
    "        np.log(validation['loss'].values)\n",
    "    )], \n",
    "    early_stopping_rounds = 5,\n",
    "    verbose = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "796.68147950848993"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.exp(model.predict(validation[features].values))\n",
    "\n",
    "score = mean_absolute_error(p, validation['loss'])\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python2.7/site-packages/pandas/core/indexing.py:296: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/home/ubuntu/anaconda/lib/python2.7/site-packages/pandas/core/indexing.py:476: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "validation.loc[:, 'loss_xgboost'] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_estimators = int(model.best_iteration / 0.9)\n",
    "model.n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(train[features].values, np.log(train['loss'].values))\n",
    "test.loc[:, 'loss'] = np.exp(model.predict(test[features].values))\n",
    "test[['id', 'loss']].to_csv('xgboost_log_transform.csv', index = False)\n",
    "\n",
    "!gzip -f xgboost_log_transform.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1130.8 (Validation 1157.0) with ~750 place, much better\n",
    "## Other trick is to multiply prediction by a constant to compensate difference between MAE and MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = validation['loss_xgboost'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's find an optimal multiplication constant for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min error: 796.49, optimal prediction multiplicator: 0.992\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for prediction_multiplicator in np.linspace(0.95, 1.05, 101) :\n",
    "    score = mean_absolute_error(p * prediction_multiplicator, validation['loss'])\n",
    "    scores.append((score, prediction_multiplicator))\n",
    "\n",
    "M = min(scores)[1]\n",
    "\n",
    "validation.loc[:, 'loss_xgboost'] *= M\n",
    "print 'Min error: %.2f, optimal prediction multiplicator: %.3f' % min(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.loc[:, 'loss'] *= M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test[['id', 'loss']].to_csv('xgboost_log_transform_multiplicated.csv', index = False)\n",
    "!gzip -f xgboost_log_transform_multiplicated.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1129.3 (Validation 1155.4) with ~750 place, slightly better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors histogram: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArkAAAHfCAYAAABUG8BuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nI+QCAJCc/wV/oZdQznjcqwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3459d15610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(\n",
    "    validation['loss'] - validation.loc[:, 'loss_xgboost'], \n",
    "    bins = 40, \n",
    "    range = [-20000, 30000], \n",
    "    log = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At the competition forum we can find one more beneficial transform:\n",
    "## Target offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#offset = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.n_estimators = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time \n",
    "\n",
    "#model.fit(\n",
    "#    train_local[features].values, \n",
    "#    np.log(train_local['loss'].values + offset), \n",
    "#    eval_set = [(\n",
    "#        validation[features].values, \n",
    "#        np.log(validation['loss'].values + offset)\n",
    "#    )], \n",
    "#    early_stopping_rounds = 15,\n",
    "#    verbose = False,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2562.359375  ,  1877.08654785,  2841.8527832 , ...,  2615.13769531,\n",
       "        1159.01574707,  1175.75927734], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p = np.exp(model.predict(validation[features].values)) - offset\n",
    "#p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#scores = []\n",
    "#for prediction_multiplicator in np.linspace(0.95, 1.05, 101) :\n",
    "#    score = mean_absolute_error(p * prediction_multiplicator, validation['loss'])\n",
    "#    scores.append((score, prediction_multiplicator))\n",
    "#\n",
    "#M = min(scores)[1]\n",
    "#print 'Min error: %.2f, optimal prediction multiplicator: %.3f' % min(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.n_estimators = int(model.best_iteration / 0.9)\n",
    "#model.n_estimators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=8,\n",
       "       min_child_weight=1, missing=None, n_estimators=186, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=False, subsample=1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.fit(train[features].values, np.log(train['loss'].values + offset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.loc[:, 'loss'] = np.exp(model.predict(test[features].values)) - offset\n",
    "test.loc[:, 'loss'] *= M\n",
    "\n",
    "test.loc[:, 'loss_xgboost'] = test.loc[:, 'loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test[['id', 'loss']].to_csv('xgboost_log_transform_multiplicated_offset.csv', index = False)\n",
    "!gzip -f xgboost_log_transform_multiplicated_offset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1128.1 (Validation 1151.90) with ~750 place, slightly better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try other GBDT model\n",
    "### You will need LightGBM executive and \"pylightgbm\" package to run this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pylightgbm.models import GBMRegressor\n",
    "os.environ['LIGHTGBM_EXEC'] = \"/home/ubuntu/LightGBM/lightgbm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = GBMRegressor(\n",
    "    num_threads=-1,\n",
    "    learning_rate = 0.005,\n",
    "    num_iterations = 5000, \n",
    "    verbose = False, \n",
    "    early_stopping_round = 50,\n",
    "    feature_fraction = 0.8,\n",
    "    bagging_fraction = 0.8,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-245d8ea3812e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu\"\\nmodel.fit(\\n    train_local[features].values, \\n    np.log(train_local['loss'].values + offset), \\n    test_data = [(\\n        validation[features].values, \\n        np.log(validation['loss'].values + offset)\\n    )]\\n)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/pylightgbm/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, test_data, init_scores)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# wait for the subprocess to exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "model.fit(\n",
    "    train_local[features].values, \n",
    "    np.log(train_local['loss'].values + offset), \n",
    "    test_data = [(\n",
    "        validation[features].values, \n",
    "        np.log(validation['loss'].values + offset)\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2865.48072668,  2002.8205407 ,  2859.33554779, ...,  2563.04111452,\n",
       "        1107.20165121,  1120.97264155])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.exp(model.predict(validation[features].values)) - offset\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min error: 1140.82, optimal prediction multiplicator: 1.028\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for prediction_multiplicator in np.linspace(0.95, 1.05, 101) :\n",
    "    score = mean_absolute_error(p * prediction_multiplicator, validation['loss'])\n",
    "    scores.append((score, prediction_multiplicator))\n",
    "\n",
    "M = min(scores)[1]\n",
    "validation.loc[:, 'loss_lightgbm'] = p * M\n",
    "print 'Min error: %.2f, optimal prediction multiplicator: %.3f' % min(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = GBMRegressor(\n",
    "    num_threads=-1,\n",
    "    learning_rate = 0.03,\n",
    "    num_iterations = int(model.best_round / 0.9), \n",
    "    verbose = False, \n",
    "    early_stopping_round = 50,\n",
    "    feature_fraction = 0.8,\n",
    "    bagging_fraction = 0.8,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(train[features].values, np.log(train['loss'].values + offset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.loc[:, 'loss_lightgbm'] = np.exp(model.predict(test[features].values)) - offset\n",
    "test.loc[:, 'loss_lightgbm'] *= M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.loc[:, 'loss'] = test['loss_lightgbm']\n",
    "\n",
    "test[['id', 'loss']].to_csv('lightgbm_log_transform_multiplicated_offset.csv', index = False)\n",
    "!gzip -f lightgbm_log_transform_multiplicated_offset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1119.2 (Validation 1142.3) with ~550 place\n",
    "## Let's try to blend two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1140.6507271335397, 0.90000000000000002)\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for alpha in np.linspace(0.1, 0.9, 9) :\n",
    "    p = validation['loss_lightgbm'] * alpha + validation['loss_xgboost'] * (1 - alpha)\n",
    "    score = mean_absolute_error(p, validation['loss'])\n",
    "    scores.append((score, alpha))\n",
    "    \n",
    "print min(scores)\n",
    "alpha = min(scores)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.loc[:, 'loss'] = test['loss_lightgbm'] * alpha + test['loss_xgboost'] * (1 - alpha)\n",
    "test[['id', 'loss']].to_csv('blending_log_transform_multiplicated_offset.csv', index = False)\n",
    "!gzip -f blending_log_transform_multiplicated_offset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1118.2 with ~500 place\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
