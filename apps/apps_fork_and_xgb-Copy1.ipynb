{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize libraries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Bag of apps categories\n",
    "# Bag of labels categories\n",
    "# Include phone brand and model device\n",
    "\n",
    "print(\"Initialize libraries\")\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics as skmetrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import ensemble\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import gc\n",
    "from scipy import sparse\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, chi2, SelectKBest\n",
    "from sklearn import ensemble\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------- Write functions ----------------------------------------\n",
    "\n",
    "def rstr(df): return df.dtypes, df.head(3) ,df.apply(lambda x: [x.unique()]), df.apply(lambda x: [len(x.unique())]),df.shape\n",
    "\n",
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    #chenglong code for fiting from generator (https://www.kaggle.com/c/talkingdata-mobile-user-demographics/forums/t/22567/neural-network-for-sparse-matrices)\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "\n",
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X[batch_index, :].toarray()\n",
    "        counter += 1\n",
    "        yield X_batch\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ----- PART 1 ----- ###\n",
      "# Read app events\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32473067 entries, 0 to 32473066\n",
      "Data columns (total 4 columns):\n",
      "event_id        int64\n",
      "app_id          int64\n",
      "is_installed    int64\n",
      "is_active       int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 991.0 MB\n",
      "# Read Events\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1488096 entries, 1 to 3252947\n",
      "Data columns (total 2 columns):\n",
      "device_id    1488096 non-null object\n",
      "app_id       1488096 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 34.1+ MB\n",
      "#Part1 formed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#------------------------------------------------ Read data from source files ------------------------------------\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "datadir = ''\n",
    "\n",
    "print(\"### ----- PART 1 ----- ###\")\n",
    "\n",
    "# Data - Events data\n",
    "# Bag of apps\n",
    "print(\"# Read app events\")\n",
    "app_events = pd.read_csv(os.path.join(datadir,'app_events.csv'), dtype={'device_id' : np.str})\n",
    "app_events.head(5)\n",
    "app_events.info()\n",
    "#print(rstr(app_events))\n",
    "\n",
    "# remove duplicates(app_id)\n",
    "app_events= app_events.groupby(\"event_id\")[\"app_id\"].apply(\n",
    "    lambda x: \" \".join(set(\"app_id:\" + str(s) for s in x)))\n",
    "app_events.head(5)\n",
    "\n",
    "print(\"# Read Events\")\n",
    "events = pd.read_csv(os.path.join(datadir,'events.csv'), dtype={'device_id': np.str})\n",
    "events.head(5)\n",
    "events[\"app_id\"] = events[\"event_id\"].map(app_events)\n",
    "events = events.dropna()\n",
    "del app_events\n",
    "\n",
    "events = events[[\"device_id\", \"app_id\"]]\n",
    "events.info()\n",
    "# 1Gb reduced to 34 Mb\n",
    "\n",
    "# remove duplicates(app_id)\n",
    "events.loc[:,\"device_id\"].value_counts(ascending=True)\n",
    "\n",
    "events = events.groupby(\"device_id\")[\"app_id\"].apply(\n",
    "    lambda x: \" \".join(set(str(\" \".join(str(s) for s in x)).split(\" \"))))\n",
    "events = events.reset_index(name=\"app_id\")\n",
    "\n",
    "# expand to multiple rows\n",
    "events = pd.concat([pd.Series(row['device_id'], row['app_id'].split(' '))\n",
    "                    for _, row in events.iterrows()]).reset_index()\n",
    "events.columns = ['app_id', 'device_id']\n",
    "events.head(5)\n",
    "f3 = events[[\"device_id\", \"app_id\"]]    # app_id\n",
    "\n",
    "print(\"#Part1 formed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ----- PART 2 ----- ###\n",
      "# Read App labels\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 459943 entries, 0 to 459942\n",
      "Data columns (total 2 columns):\n",
      "app_id      459943 non-null int64\n",
      "label_id    459943 non-null int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 7.0 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 930 entries, 0 to 929\n",
      "Data columns (total 2 columns):\n",
      "label_id    930 non-null int64\n",
      "category    927 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 14.6+ KB\n",
      "# App labels done\n",
      "## Handling events data for merging with app lables\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 457777 entries, 0 to 457776\n",
      "Data columns (total 2 columns):\n",
      "app_id      457777 non-null object\n",
      "category    457777 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 7.0+ MB\n",
      "## Merge\n",
      "#Expand to multiple rows\n",
      "# App labels done\n",
      "# App category part formed\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#   App labels\n",
    "##################\n",
    "\n",
    "\n",
    "print(\"### ----- PART 2 ----- ###\")\n",
    "\n",
    "print(\"# Read App labels\")\n",
    "app_labels = pd.read_csv(os.path.join(datadir,'app_labels.csv'))\n",
    "label_cat = pd.read_csv(os.path.join(datadir,'label_categories.csv'))\n",
    "app_labels.info()\n",
    "label_cat.info()\n",
    "label_cat=label_cat[['label_id','category']]\n",
    "\n",
    "app_labels=app_labels.merge(label_cat,on='label_id',how='left')\n",
    "app_labels.head(3)\n",
    "events.head(3)\n",
    "#app_labels = app_labels.loc[app_labels.smaller_cat != \"unknown_unknown\"]\n",
    "\n",
    "#app_labels = app_labels.groupby(\"app_id\")[\"category\"].apply(\n",
    "#    lambda x: \";\".join(set(\"app_cat:\" + str(s) for s in x)))\n",
    "app_labels = app_labels.groupby([\"app_id\",\"category\"]).agg('size').reset_index()\n",
    "app_labels = app_labels[['app_id','category']]\n",
    "print(\"# App labels done\")\n",
    "\n",
    "\n",
    "# Remove \"app_id:\" from column\n",
    "print(\"## Handling events data for merging with app lables\")\n",
    "events['app_id'] = events['app_id'].map(lambda x : x.lstrip('app_id:'))\n",
    "events['app_id'] = events['app_id'].astype(str)\n",
    "app_labels['app_id'] = app_labels['app_id'].astype(str)\n",
    "app_labels.info()\n",
    "\n",
    "print(\"## Merge\")\n",
    "\n",
    "events= pd.merge(events, app_labels, on = 'app_id',how='left').astype(str)\n",
    "#events['smaller_cat'].unique()\n",
    "\n",
    "# expand to multiple rows\n",
    "print(\"#Expand to multiple rows\")\n",
    "#events= pd.concat([pd.Series(row['device_id'], row['category'].split(';'))\n",
    "#                    for _, row in events.iterrows()]).reset_index()\n",
    "#events.columns = ['app_cat', 'device_id']\n",
    "#events.head(5)\n",
    "#print(events.info())\n",
    "\n",
    "events= events.groupby([\"device_id\",\"category\"]).agg('size').reset_index()\n",
    "events= events[['device_id','category']]\n",
    "events.head(10)\n",
    "print(\"# App labels done\")\n",
    "\n",
    "f5 = events[[\"device_id\", \"category\"]]    # app_id\n",
    "# Can % total share be included as well?\n",
    "print(\"# App category part formed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ----- PART 3 ----- ###\n",
      "# Read Phone Brand\n",
      "# Generate Train and Test\n",
      "### ----- PART 4 ----- ###\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#   Phone Brand\n",
    "##################\n",
    "print(\"### ----- PART 3 ----- ###\")\n",
    "\n",
    "print(\"# Read Phone Brand\")\n",
    "pbd = pd.read_csv(os.path.join(datadir,'phone_brand_device_model.csv'),\n",
    "                  dtype={'device_id': np.str})\n",
    "pbd.drop_duplicates('device_id', keep='first', inplace=True)\n",
    "\n",
    "##################\n",
    "#  Train and Test\n",
    "##################\n",
    "print(\"# Generate Train and Test\")\n",
    "\n",
    "train = pd.read_csv(os.path.join(datadir,'gender_age_train.csv'),\n",
    "                    dtype={'device_id': np.str})\n",
    "train.drop([\"age\", \"gender\"], axis=1, inplace=True)\n",
    "\n",
    "test = pd.read_csv(os.path.join(datadir,'gender_age_test.csv'),\n",
    "                   dtype={'device_id': np.str})\n",
    "test[\"group\"] = np.nan\n",
    "\n",
    "train['norm'] = train.device_id.astype(int)/len(train)\n",
    "test['norm'] = test.device_id.astype(int)/len(test)\n",
    "\n",
    "\n",
    "split_len = len(train)\n",
    "\n",
    "# Group Labels\n",
    "Y = train[\"group\"]\n",
    "lable_group = LabelEncoder()\n",
    "Y = lable_group.fit_transform(Y)\n",
    "device_id = test[\"device_id\"]\n",
    "\n",
    "# Concat\n",
    "Df = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "\n",
    "print(\"### ----- PART 4 ----- ###\")\n",
    "\n",
    "Df = pd.merge(Df, pbd, how=\"left\", on=\"device_id\")\n",
    "Df[\"phone_brand\"] = Df[\"phone_brand\"].apply(lambda x: \"phone_brand:\" + str(x))\n",
    "Df[\"device_model\"] = Df[\"device_model\"].apply(\n",
    "    lambda x: \"device_model:\" + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Concat all features\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6677856 entries, 0 to 6677855\n",
      "Data columns (total 2 columns):\n",
      "device_id    object\n",
      "feature      object\n",
      "dtypes: object(2)\n",
      "memory usage: 101.9+ MB\n",
      "# User-Item-Feature\n",
      "# Sparse matrix done\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "#  Concat Feature\n",
    "###################\n",
    "\n",
    "print(\"# Concat all features\")\n",
    "\n",
    "f1 = Df[[\"device_id\", \"phone_brand\"]]   # phone_brand\n",
    "f2 = Df[[\"device_id\", \"device_model\"]]  # device_model\n",
    "\n",
    "events = None\n",
    "#Df = None\n",
    "\n",
    "f1.columns.values[1] = \"feature\"\n",
    "f2.columns.values[1] = \"feature\"\n",
    "f5.columns.values[1] = \"feature\"\n",
    "f3.columns.values[1] = \"feature\"\n",
    "\n",
    "FLS = pd.concat((f1, f2, f3, f5), axis=0, ignore_index=True)\n",
    "\n",
    "FLS.info()\n",
    "\n",
    "###################\n",
    "# User-Item Feature\n",
    "###################\n",
    "print(\"# User-Item-Feature\")\n",
    "\n",
    "device_ids = FLS[\"device_id\"].unique()\n",
    "feature_cs = FLS[\"feature\"].unique()\n",
    "\n",
    "data = np.ones(len(FLS))\n",
    "len(data)\n",
    "\n",
    "dec = LabelEncoder().fit(FLS[\"device_id\"])\n",
    "row = dec.transform(FLS[\"device_id\"])\n",
    "col = LabelEncoder().fit_transform(FLS[\"feature\"])\n",
    "sparse_matrix = sparse.csr_matrix(\n",
    "    (data, (row, col)), shape=(len(device_ids), len(feature_cs)))\n",
    "sparse_matrix.shape\n",
    "sys.getsizeof(sparse_matrix)\n",
    "\n",
    "sparse_matrix = sparse_matrix[:, sparse_matrix.getnnz(0) > 0]\n",
    "print(\"# Sparse matrix done\")\n",
    "\n",
    "#del FLS\n",
    "#del data\n",
    "f1 = [1]\n",
    "f5 = [1]\n",
    "f2 = [1]\n",
    "f3 = [1]\n",
    "\n",
    "events = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#row_ids = [i for i in xrange(sparse_matrix.shape[0])]\n",
    "#row_ids = np.array(row_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#inv_trans = dec.inverse_transform(row_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ids = Df.device_id.values\n",
    "norm_vals = Df.norm.values\n",
    "\n",
    "#norm_dict = {}\n",
    "\n",
    "#for n, i in enumerate(df_ids):\n",
    "#    norm_dict[i] = norm_vals[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import time\n",
    "#st = time.time()\n",
    "#norms = []\n",
    "#for i in inv_trans:\n",
    "#    try:\n",
    "#        norms.append(norm_dict[i])\n",
    "#    except:\n",
    "#        norms.append(0)\n",
    "#print time.time() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#norms = np.array(norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sparse = sparse.csr_matrix(norm_vals[:len(train)])\n",
    "train_sparse = sparse.csr_matrix.transpose(train_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sparse = sparse.csr_matrix(norm_vals[len(train):])\n",
    "test_sparse = sparse.csr_matrix.transpose(test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ndevice_ids = device_ids.astype(int)\n",
    "\n",
    "#train_devid_array = np.zeros((len(ndevice_ids), 1))\n",
    "\n",
    "#train_devid_array[:] = ndevice_ids/len(ndevice_ids)\n",
    "\n",
    "#for i in range(len(ndevice_ids)):\n",
    "#    s = \"%.20d\" % abs(ndevice_ids[i])\n",
    "#    \n",
    "#    for j in range(0, 20):\n",
    "#        train_devid_array[i][j] = int(s[j])\n",
    "#        \n",
    "#    if ndevice_ids[i] < 0:\n",
    "#        train_devid_array[i][0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_sparse = sparse.csr_matrix(norms)\n",
    "#train_sparse = sparse.csr_matrix.transpose(train_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#type(train_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#type(sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sparse_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sparse_matrix_1 = sparse.hstack((sparse_matrix, train_sparse), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#type(sparse_matrix_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.device_id = train.device_id.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Split data\n",
      "# Feature Selection\n",
      "('# Num of Features: ', 21425)\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#      Data\n",
    "##################\n",
    "\n",
    "print(\"# Split data\")\n",
    "train_row = dec.transform(train[\"device_id\"])\n",
    "train_sp = sparse_matrix[train_row, :]\n",
    "\n",
    "test_row = dec.transform(test[\"device_id\"])\n",
    "test_sp = sparse_matrix[test_row, :]\n",
    "\n",
    "train_sp1 = sparse.hstack((train_sp, train_sparse), format='csr')\n",
    "test_sp1 = sparse.hstack((test_sp, test_sparse), format='csr')\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_sp, Y, train_size=0.999, random_state=10)\n",
    "\n",
    "##################\n",
    "#   Feature Sel\n",
    "##################\n",
    "print(\"# Feature Selection\")\n",
    "#selector = SelectPercentile(f_classif, percentile=53)\n",
    "\n",
    "#selector.fit(X_train, y_train)\n",
    "#X_train.shape\n",
    "#X_train = selector.transform(X_train)\n",
    "#X_train.shape\n",
    "#X_val = selector.transform(X_val)\n",
    "#X_val.shape\n",
    "\n",
    "# Selection using chi-square\n",
    "# selector = SelectKBest(chi2, k=11155).fit(X_train, y_train)\n",
    "# X_train.shape\n",
    "# X_train = selector.transform(X_train)\n",
    "# X_train.shape\n",
    "# X_val = selector.transform(X_val)\n",
    "# X_val.shape\n",
    "\n",
    "print(\"# Num of Features: \", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import time\n",
    "\n",
    "def run_xgb(train, test, target, eta=0.5, random_state=0):\n",
    "    #eta = 0.1\n",
    "    max_depth = 5\n",
    "    subsample = 0.7\n",
    "    colsample_bytree = 0.7\n",
    "    start_time = time.time()\n",
    "\n",
    "    print('XGBoost params. ETA: {}, MAX_DEPTH: {}, SUBSAMPLE: {}, COLSAMPLE_BY_TREE: {}'.format(eta, max_depth, subsample, colsample_bytree))\n",
    "    params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"num_class\": 12,\n",
    "        \"booster\" : \"gbtree\",\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"eta\": eta,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"subsample\": subsample,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"silent\": 1,\n",
    "        \"seed\": random_state,\n",
    "    }\n",
    "    num_boost_round = 2000\n",
    "    early_stopping_rounds = 100\n",
    "    test_size = 0.3\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(train, target, test_size=test_size, random_state=random_state)\n",
    "    # TODO change split \n",
    "    print('Length train:', X_train.shape[0])\n",
    "    print('Length valid:', X_valid.shape[0])\n",
    "    #y_train = X_train[target]\n",
    "    #y_valid = X_valid[target]\n",
    "    # TODO delete upper code\n",
    "    dtrain = xgb.DMatrix(X_train, y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, y_valid)\n",
    "    # sparse matrix ???\n",
    "\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "    gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n",
    "\n",
    "    print(\"Validating...\")\n",
    "    check = gbm.predict(xgb.DMatrix(X_valid), ntree_limit=gbm.best_iteration)\n",
    "    score = log_loss(y_valid.tolist(), check)\n",
    "\n",
    "    print(\"Predict test set...\")\n",
    "    test_prediction = gbm.predict(xgb.DMatrix(test), ntree_limit=gbm.best_iteration)\n",
    "\n",
    "    print('Training time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "    return test_prediction.tolist(), score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#last = pd.read_csv('keras_2.26110777855.csv')\n",
    "#last['device_id'] = last.device_id.astype(str)\n",
    "#last = last.set_index(\"device_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/17\n",
      "16s - loss: 2.4615 - acc: 0.1209 - val_loss: 2.4465 - val_acc: 0.1067\n",
      "Epoch 2/17\n",
      "17s - loss: 2.4293 - acc: 0.1332 - val_loss: 2.4347 - val_acc: 0.1200\n",
      "Epoch 3/17\n",
      "16s - loss: 2.4131 - acc: 0.1401 - val_loss: 2.4385 - val_acc: 0.0533\n",
      "Epoch 4/17\n",
      "16s - loss: 2.3909 - acc: 0.1473 - val_loss: 2.4153 - val_acc: 0.0667\n",
      "Epoch 5/17\n",
      "16s - loss: 2.3647 - acc: 0.1579 - val_loss: 2.3970 - val_acc: 0.0400\n",
      "Epoch 6/17\n",
      "16s - loss: 2.3379 - acc: 0.1697 - val_loss: 2.3658 - val_acc: 0.0667\n",
      "Epoch 7/17\n",
      "16s - loss: 2.3206 - acc: 0.1753 - val_loss: 2.3441 - val_acc: 0.0933\n",
      "Epoch 8/17\n",
      "16s - loss: 2.3077 - acc: 0.1801 - val_loss: 2.3319 - val_acc: 0.0933\n",
      "Epoch 9/17\n",
      "16s - loss: 2.2942 - acc: 0.1855 - val_loss: 2.3345 - val_acc: 0.0533\n",
      "Epoch 10/17\n",
      "16s - loss: 2.2859 - acc: 0.1869 - val_loss: 2.3220 - val_acc: 0.0667\n",
      "Epoch 11/17\n",
      "16s - loss: 2.2804 - acc: 0.1913 - val_loss: 2.3271 - val_acc: 0.0667\n",
      "Epoch 12/17\n",
      "16s - loss: 2.2752 - acc: 0.1906 - val_loss: 2.3087 - val_acc: 0.1333\n",
      "Epoch 13/17\n",
      "16s - loss: 2.2697 - acc: 0.1943 - val_loss: 2.3096 - val_acc: 0.1200\n",
      "Epoch 14/17\n",
      "16s - loss: 2.2599 - acc: 0.1969 - val_loss: 2.3069 - val_acc: 0.0800\n",
      "Epoch 15/17\n",
      "16s - loss: 2.2584 - acc: 0.1966 - val_loss: 2.3056 - val_acc: 0.1333\n",
      "Epoch 16/17\n",
      "16s - loss: 2.2523 - acc: 0.2011 - val_loss: 2.3010 - val_acc: 0.1600\n",
      "Epoch 17/17\n",
      "17s - loss: 2.2474 - acc: 0.2029 - val_loss: 2.3074 - val_acc: 0.0667\n",
      "logloss val 2.30742456436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:24: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "def fifth_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(152, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.42))\n",
    "    model.add(Dense(52, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.18))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_5 = fifth_model()\n",
    "\n",
    "fit_5 = model_5.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=17,\n",
    "                         samples_per_epoch=69884,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "scores_val_5 = model_5.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val_5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Final prediction\n",
      "       F23-    F24-26    F27-28    F29-32  F33-42      F43+     M22-  \\\n",
      "0  0.000128  0.000512  0.001606  0.005495  0.0214  0.048626  0.00132   \n",
      "\n",
      "     M23-26    M27-28    M29-31    M32-38      M39+            device_id  \n",
      "0  0.014742  0.033229  0.097074  0.270042  0.505826  1002079943728939269  \n"
     ]
    }
   ],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_55 = model_5.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_55 = pd.DataFrame(scores_55 , columns=lable_group.classes_)\n",
    "result_55[\"device_id\"] = device_id\n",
    "print(result_55.head(1))\n",
    "result_55 = result_55.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result_3.to_csv('keras_' + str(log_loss(y_val, scores_val_3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost params. ETA: 0.07, MAX_DEPTH: 5, SUBSAMPLE: 0.7, COLSAMPLE_BY_TREE: 0.7\n",
      "('Length train:', 52251)\n",
      "('Length valid:', 22394)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 100 rounds.\n",
      "[0]\ttrain-mlogloss:2.472980\teval-mlogloss:2.475275\n",
      "[1]\ttrain-mlogloss:2.461815\teval-mlogloss:2.466241\n",
      "[2]\ttrain-mlogloss:2.451713\teval-mlogloss:2.458025\n",
      "[3]\ttrain-mlogloss:2.442165\teval-mlogloss:2.450348\n",
      "[4]\ttrain-mlogloss:2.433060\teval-mlogloss:2.443388\n",
      "[5]\ttrain-mlogloss:2.424465\teval-mlogloss:2.436804\n",
      "[6]\ttrain-mlogloss:2.416336\teval-mlogloss:2.430594\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-0cb140c2d7d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_xgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.07\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-c5194fa4c095>\u001b[0m in \u001b[0;36mrun_xgb\u001b[0;34m(train, test, target, eta, random_state)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mwatchlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'eval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mgbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwatchlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validating...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'eta'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearning_rates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0mnboost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mbst_eval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/xgboost/core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res, score = run_xgb(train_sp1, test_sp1, Y, eta=0.07, random_state=0)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost params. ETA: 0.07, MAX_DEPTH: 5, SUBSAMPLE: 0.7, COLSAMPLE_BY_TREE: 0.7\n",
      "('Length train:', 52251)\n",
      "('Length valid:', 22394)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 100 rounds.\n",
      "[0]\ttrain-mlogloss:2.473099\teval-mlogloss:2.474913\n",
      "[1]\ttrain-mlogloss:2.461848\teval-mlogloss:2.465437\n",
      "[2]\ttrain-mlogloss:2.451689\teval-mlogloss:2.457276\n",
      "[3]\ttrain-mlogloss:2.442170\teval-mlogloss:2.449705\n",
      "[4]\ttrain-mlogloss:2.433220\teval-mlogloss:2.442586\n",
      "[5]\ttrain-mlogloss:2.424968\teval-mlogloss:2.435954\n",
      "[6]\ttrain-mlogloss:2.417095\teval-mlogloss:2.429746\n",
      "[7]\ttrain-mlogloss:2.409938\teval-mlogloss:2.424076\n",
      "[8]\ttrain-mlogloss:2.402843\teval-mlogloss:2.418708\n",
      "[9]\ttrain-mlogloss:2.396090\teval-mlogloss:2.413619\n",
      "[10]\ttrain-mlogloss:2.389982\teval-mlogloss:2.408947\n",
      "[11]\ttrain-mlogloss:2.383952\teval-mlogloss:2.404484\n",
      "[12]\ttrain-mlogloss:2.378389\teval-mlogloss:2.400587\n",
      "[13]\ttrain-mlogloss:2.372930\teval-mlogloss:2.396583\n",
      "[14]\ttrain-mlogloss:2.367733\teval-mlogloss:2.392945\n",
      "[15]\ttrain-mlogloss:2.362571\teval-mlogloss:2.389339\n",
      "[16]\ttrain-mlogloss:2.357821\teval-mlogloss:2.386017\n",
      "[17]\ttrain-mlogloss:2.353369\teval-mlogloss:2.382823\n",
      "[18]\ttrain-mlogloss:2.348857\teval-mlogloss:2.379905\n",
      "[19]\ttrain-mlogloss:2.344420\teval-mlogloss:2.376901\n",
      "[20]\ttrain-mlogloss:2.340485\teval-mlogloss:2.374099\n",
      "[21]\ttrain-mlogloss:2.336690\teval-mlogloss:2.371524\n",
      "[22]\ttrain-mlogloss:2.332865\teval-mlogloss:2.369071\n",
      "[23]\ttrain-mlogloss:2.329308\teval-mlogloss:2.366585\n",
      "[24]\ttrain-mlogloss:2.325783\teval-mlogloss:2.364415\n",
      "[25]\ttrain-mlogloss:2.322320\teval-mlogloss:2.362159\n",
      "[26]\ttrain-mlogloss:2.319011\teval-mlogloss:2.360138\n",
      "[27]\ttrain-mlogloss:2.315597\teval-mlogloss:2.358234\n",
      "[28]\ttrain-mlogloss:2.312699\teval-mlogloss:2.356637\n",
      "[29]\ttrain-mlogloss:2.309879\teval-mlogloss:2.355039\n",
      "[30]\ttrain-mlogloss:2.306978\teval-mlogloss:2.353396\n",
      "[31]\ttrain-mlogloss:2.304317\teval-mlogloss:2.351972\n",
      "[32]\ttrain-mlogloss:2.301328\teval-mlogloss:2.350282\n",
      "[33]\ttrain-mlogloss:2.298713\teval-mlogloss:2.348937\n",
      "[34]\ttrain-mlogloss:2.296137\teval-mlogloss:2.347531\n",
      "[35]\ttrain-mlogloss:2.293557\teval-mlogloss:2.346102\n",
      "[36]\ttrain-mlogloss:2.291171\teval-mlogloss:2.344927\n",
      "[37]\ttrain-mlogloss:2.288690\teval-mlogloss:2.343649\n",
      "[38]\ttrain-mlogloss:2.286380\teval-mlogloss:2.342466\n",
      "[39]\ttrain-mlogloss:2.284140\teval-mlogloss:2.341301\n",
      "[40]\ttrain-mlogloss:2.281885\teval-mlogloss:2.340256\n",
      "[41]\ttrain-mlogloss:2.279833\teval-mlogloss:2.339259\n",
      "[42]\ttrain-mlogloss:2.277742\teval-mlogloss:2.338151\n",
      "[43]\ttrain-mlogloss:2.275567\teval-mlogloss:2.337069\n",
      "[44]\ttrain-mlogloss:2.273513\teval-mlogloss:2.336043\n",
      "[45]\ttrain-mlogloss:2.271622\teval-mlogloss:2.335234\n",
      "[46]\ttrain-mlogloss:2.269687\teval-mlogloss:2.334438\n",
      "[47]\ttrain-mlogloss:2.267847\teval-mlogloss:2.333716\n",
      "[48]\ttrain-mlogloss:2.266011\teval-mlogloss:2.332903\n",
      "[49]\ttrain-mlogloss:2.264239\teval-mlogloss:2.332201\n",
      "[50]\ttrain-mlogloss:2.262438\teval-mlogloss:2.331337\n",
      "[51]\ttrain-mlogloss:2.260794\teval-mlogloss:2.330606\n",
      "[52]\ttrain-mlogloss:2.259243\teval-mlogloss:2.329898\n",
      "[53]\ttrain-mlogloss:2.257717\teval-mlogloss:2.329172\n",
      "[54]\ttrain-mlogloss:2.255955\teval-mlogloss:2.328354\n",
      "[55]\ttrain-mlogloss:2.254351\teval-mlogloss:2.327656\n",
      "[56]\ttrain-mlogloss:2.252688\teval-mlogloss:2.326938\n",
      "[57]\ttrain-mlogloss:2.251127\teval-mlogloss:2.326281\n",
      "[58]\ttrain-mlogloss:2.249464\teval-mlogloss:2.325659\n",
      "[59]\ttrain-mlogloss:2.247885\teval-mlogloss:2.325123\n",
      "[60]\ttrain-mlogloss:2.246456\teval-mlogloss:2.324658\n",
      "[61]\ttrain-mlogloss:2.244951\teval-mlogloss:2.324056\n",
      "[62]\ttrain-mlogloss:2.243408\teval-mlogloss:2.323499\n",
      "[63]\ttrain-mlogloss:2.241933\teval-mlogloss:2.322928\n",
      "[64]\ttrain-mlogloss:2.240525\teval-mlogloss:2.322412\n",
      "[65]\ttrain-mlogloss:2.239037\teval-mlogloss:2.321886\n",
      "[66]\ttrain-mlogloss:2.237643\teval-mlogloss:2.321358\n",
      "[67]\ttrain-mlogloss:2.236271\teval-mlogloss:2.320765\n",
      "[68]\ttrain-mlogloss:2.234918\teval-mlogloss:2.320261\n",
      "[69]\ttrain-mlogloss:2.233646\teval-mlogloss:2.319937\n",
      "[70]\ttrain-mlogloss:2.232331\teval-mlogloss:2.319495\n",
      "[71]\ttrain-mlogloss:2.230871\teval-mlogloss:2.319107\n",
      "[72]\ttrain-mlogloss:2.229564\teval-mlogloss:2.318547\n",
      "[73]\ttrain-mlogloss:2.228300\teval-mlogloss:2.318001\n",
      "[74]\ttrain-mlogloss:2.227100\teval-mlogloss:2.317581\n",
      "[75]\ttrain-mlogloss:2.225890\teval-mlogloss:2.317199\n",
      "[76]\ttrain-mlogloss:2.224730\teval-mlogloss:2.316800\n",
      "[77]\ttrain-mlogloss:2.223474\teval-mlogloss:2.316409\n",
      "[78]\ttrain-mlogloss:2.222259\teval-mlogloss:2.316041\n",
      "[79]\ttrain-mlogloss:2.221187\teval-mlogloss:2.315680\n",
      "[80]\ttrain-mlogloss:2.219878\teval-mlogloss:2.315271\n",
      "[81]\ttrain-mlogloss:2.218797\teval-mlogloss:2.314867\n",
      "[82]\ttrain-mlogloss:2.217606\teval-mlogloss:2.314479\n",
      "[83]\ttrain-mlogloss:2.216447\teval-mlogloss:2.314125\n",
      "[84]\ttrain-mlogloss:2.215256\teval-mlogloss:2.313761\n",
      "[85]\ttrain-mlogloss:2.214039\teval-mlogloss:2.313426\n",
      "[86]\ttrain-mlogloss:2.212877\teval-mlogloss:2.312980\n",
      "[87]\ttrain-mlogloss:2.211743\teval-mlogloss:2.312583\n",
      "[88]\ttrain-mlogloss:2.210756\teval-mlogloss:2.312265\n",
      "[89]\ttrain-mlogloss:2.209825\teval-mlogloss:2.311939\n",
      "[90]\ttrain-mlogloss:2.208697\teval-mlogloss:2.311605\n",
      "[91]\ttrain-mlogloss:2.207631\teval-mlogloss:2.311275\n",
      "[92]\ttrain-mlogloss:2.206434\teval-mlogloss:2.310907\n",
      "[93]\ttrain-mlogloss:2.205247\teval-mlogloss:2.310560\n",
      "[94]\ttrain-mlogloss:2.204252\teval-mlogloss:2.310176\n",
      "[95]\ttrain-mlogloss:2.203184\teval-mlogloss:2.309908\n",
      "[96]\ttrain-mlogloss:2.202101\teval-mlogloss:2.309645\n",
      "[97]\ttrain-mlogloss:2.201117\teval-mlogloss:2.309465\n",
      "[98]\ttrain-mlogloss:2.200064\teval-mlogloss:2.309194\n",
      "[99]\ttrain-mlogloss:2.199185\teval-mlogloss:2.308915\n",
      "[100]\ttrain-mlogloss:2.198214\teval-mlogloss:2.308705\n",
      "[101]\ttrain-mlogloss:2.197357\teval-mlogloss:2.308441\n",
      "[102]\ttrain-mlogloss:2.196309\teval-mlogloss:2.308238\n",
      "[103]\ttrain-mlogloss:2.195173\teval-mlogloss:2.307990\n",
      "[104]\ttrain-mlogloss:2.194098\teval-mlogloss:2.307678\n",
      "[105]\ttrain-mlogloss:2.193126\teval-mlogloss:2.307373\n",
      "[106]\ttrain-mlogloss:2.192110\teval-mlogloss:2.307142\n",
      "[107]\ttrain-mlogloss:2.191237\teval-mlogloss:2.306885\n",
      "[108]\ttrain-mlogloss:2.190263\teval-mlogloss:2.306643\n",
      "[109]\ttrain-mlogloss:2.189343\teval-mlogloss:2.306424\n",
      "[110]\ttrain-mlogloss:2.188434\teval-mlogloss:2.306159\n",
      "[111]\ttrain-mlogloss:2.187490\teval-mlogloss:2.305935\n",
      "[112]\ttrain-mlogloss:2.186392\teval-mlogloss:2.305679\n",
      "[113]\ttrain-mlogloss:2.185400\teval-mlogloss:2.305356\n",
      "[114]\ttrain-mlogloss:2.184538\teval-mlogloss:2.305173\n",
      "[115]\ttrain-mlogloss:2.183660\teval-mlogloss:2.304965\n",
      "[116]\ttrain-mlogloss:2.182786\teval-mlogloss:2.304760\n",
      "[117]\ttrain-mlogloss:2.181924\teval-mlogloss:2.304593\n",
      "[118]\ttrain-mlogloss:2.180992\teval-mlogloss:2.304291\n",
      "[119]\ttrain-mlogloss:2.180154\teval-mlogloss:2.304104\n",
      "[120]\ttrain-mlogloss:2.179302\teval-mlogloss:2.303883\n",
      "[121]\ttrain-mlogloss:2.178436\teval-mlogloss:2.303638\n",
      "[122]\ttrain-mlogloss:2.177609\teval-mlogloss:2.303562\n",
      "[123]\ttrain-mlogloss:2.176807\teval-mlogloss:2.303342\n",
      "[124]\ttrain-mlogloss:2.176054\teval-mlogloss:2.303131\n",
      "[125]\ttrain-mlogloss:2.175194\teval-mlogloss:2.302956\n",
      "[126]\ttrain-mlogloss:2.174386\teval-mlogloss:2.302733\n",
      "[127]\ttrain-mlogloss:2.173499\teval-mlogloss:2.302509\n",
      "[128]\ttrain-mlogloss:2.172710\teval-mlogloss:2.302368\n",
      "[129]\ttrain-mlogloss:2.171924\teval-mlogloss:2.302177\n",
      "[130]\ttrain-mlogloss:2.171073\teval-mlogloss:2.301968\n",
      "[131]\ttrain-mlogloss:2.170069\teval-mlogloss:2.301742\n",
      "[132]\ttrain-mlogloss:2.169199\teval-mlogloss:2.301502\n",
      "[133]\ttrain-mlogloss:2.168329\teval-mlogloss:2.301250\n",
      "[134]\ttrain-mlogloss:2.167576\teval-mlogloss:2.301187\n",
      "[135]\ttrain-mlogloss:2.166866\teval-mlogloss:2.300990\n",
      "[136]\ttrain-mlogloss:2.166109\teval-mlogloss:2.300792\n",
      "[137]\ttrain-mlogloss:2.165187\teval-mlogloss:2.300516\n",
      "[138]\ttrain-mlogloss:2.164522\teval-mlogloss:2.300336\n",
      "[139]\ttrain-mlogloss:2.163683\teval-mlogloss:2.300189\n",
      "[140]\ttrain-mlogloss:2.163014\teval-mlogloss:2.300065\n",
      "[141]\ttrain-mlogloss:2.162298\teval-mlogloss:2.299924\n",
      "[142]\ttrain-mlogloss:2.161561\teval-mlogloss:2.299745\n",
      "[143]\ttrain-mlogloss:2.160787\teval-mlogloss:2.299562\n",
      "[144]\ttrain-mlogloss:2.159998\teval-mlogloss:2.299394\n",
      "[145]\ttrain-mlogloss:2.159303\teval-mlogloss:2.299246\n",
      "[146]\ttrain-mlogloss:2.158618\teval-mlogloss:2.299083\n",
      "[147]\ttrain-mlogloss:2.157788\teval-mlogloss:2.298868\n",
      "[148]\ttrain-mlogloss:2.157068\teval-mlogloss:2.298814\n",
      "[149]\ttrain-mlogloss:2.156320\teval-mlogloss:2.298633\n",
      "[150]\ttrain-mlogloss:2.155475\teval-mlogloss:2.298444\n",
      "[151]\ttrain-mlogloss:2.154754\teval-mlogloss:2.298273\n",
      "[152]\ttrain-mlogloss:2.153968\teval-mlogloss:2.298115\n",
      "[153]\ttrain-mlogloss:2.153274\teval-mlogloss:2.298019\n",
      "[154]\ttrain-mlogloss:2.152592\teval-mlogloss:2.297952\n",
      "[155]\ttrain-mlogloss:2.151873\teval-mlogloss:2.297761\n",
      "[156]\ttrain-mlogloss:2.151110\teval-mlogloss:2.297626\n",
      "[157]\ttrain-mlogloss:2.150443\teval-mlogloss:2.297508\n",
      "[158]\ttrain-mlogloss:2.149711\teval-mlogloss:2.297343\n",
      "[159]\ttrain-mlogloss:2.149021\teval-mlogloss:2.297262\n",
      "[160]\ttrain-mlogloss:2.148370\teval-mlogloss:2.297152\n",
      "[161]\ttrain-mlogloss:2.147613\teval-mlogloss:2.296940\n",
      "[162]\ttrain-mlogloss:2.146912\teval-mlogloss:2.296828\n",
      "[163]\ttrain-mlogloss:2.146291\teval-mlogloss:2.296699\n",
      "[164]\ttrain-mlogloss:2.145690\teval-mlogloss:2.296539\n",
      "[165]\ttrain-mlogloss:2.145030\teval-mlogloss:2.296398\n",
      "[166]\ttrain-mlogloss:2.144249\teval-mlogloss:2.296227\n",
      "[167]\ttrain-mlogloss:2.143617\teval-mlogloss:2.296166\n",
      "[168]\ttrain-mlogloss:2.142859\teval-mlogloss:2.296056\n",
      "[169]\ttrain-mlogloss:2.142187\teval-mlogloss:2.295954\n",
      "[170]\ttrain-mlogloss:2.141491\teval-mlogloss:2.295851\n",
      "[171]\ttrain-mlogloss:2.140892\teval-mlogloss:2.295754\n",
      "[172]\ttrain-mlogloss:2.140326\teval-mlogloss:2.295589\n",
      "[173]\ttrain-mlogloss:2.139586\teval-mlogloss:2.295366\n",
      "[174]\ttrain-mlogloss:2.138858\teval-mlogloss:2.295216\n",
      "[175]\ttrain-mlogloss:2.138227\teval-mlogloss:2.295118\n",
      "[176]\ttrain-mlogloss:2.137534\teval-mlogloss:2.294998\n",
      "[177]\ttrain-mlogloss:2.136917\teval-mlogloss:2.294807\n",
      "[178]\ttrain-mlogloss:2.136246\teval-mlogloss:2.294621\n",
      "[179]\ttrain-mlogloss:2.135574\teval-mlogloss:2.294557\n",
      "[180]\ttrain-mlogloss:2.135015\teval-mlogloss:2.294478\n",
      "[181]\ttrain-mlogloss:2.134452\teval-mlogloss:2.294453\n",
      "[182]\ttrain-mlogloss:2.133892\teval-mlogloss:2.294389\n",
      "[183]\ttrain-mlogloss:2.133250\teval-mlogloss:2.294240\n",
      "[184]\ttrain-mlogloss:2.132669\teval-mlogloss:2.294124\n",
      "[185]\ttrain-mlogloss:2.132049\teval-mlogloss:2.294036\n",
      "[186]\ttrain-mlogloss:2.131427\teval-mlogloss:2.293940\n",
      "[187]\ttrain-mlogloss:2.130874\teval-mlogloss:2.293862\n",
      "[188]\ttrain-mlogloss:2.130238\teval-mlogloss:2.293806\n",
      "[189]\ttrain-mlogloss:2.129526\teval-mlogloss:2.293606\n",
      "[190]\ttrain-mlogloss:2.128861\teval-mlogloss:2.293532\n",
      "[191]\ttrain-mlogloss:2.128113\teval-mlogloss:2.293388\n",
      "[192]\ttrain-mlogloss:2.127542\teval-mlogloss:2.293306\n",
      "[193]\ttrain-mlogloss:2.126963\teval-mlogloss:2.293202\n",
      "[194]\ttrain-mlogloss:2.126373\teval-mlogloss:2.293071\n",
      "[195]\ttrain-mlogloss:2.125822\teval-mlogloss:2.292914\n",
      "[196]\ttrain-mlogloss:2.125170\teval-mlogloss:2.292856\n",
      "[197]\ttrain-mlogloss:2.124621\teval-mlogloss:2.292737\n",
      "[198]\ttrain-mlogloss:2.123880\teval-mlogloss:2.292657\n",
      "[199]\ttrain-mlogloss:2.123273\teval-mlogloss:2.292494\n",
      "[200]\ttrain-mlogloss:2.122611\teval-mlogloss:2.292316\n",
      "[201]\ttrain-mlogloss:2.122025\teval-mlogloss:2.292287\n",
      "[202]\ttrain-mlogloss:2.121372\teval-mlogloss:2.292168\n",
      "[203]\ttrain-mlogloss:2.120665\teval-mlogloss:2.292035\n",
      "[204]\ttrain-mlogloss:2.120106\teval-mlogloss:2.291919\n",
      "[205]\ttrain-mlogloss:2.119579\teval-mlogloss:2.291830\n",
      "[206]\ttrain-mlogloss:2.119035\teval-mlogloss:2.291731\n",
      "[207]\ttrain-mlogloss:2.118465\teval-mlogloss:2.291667\n",
      "[208]\ttrain-mlogloss:2.117875\teval-mlogloss:2.291557\n",
      "[209]\ttrain-mlogloss:2.117271\teval-mlogloss:2.291446\n",
      "[210]\ttrain-mlogloss:2.116749\teval-mlogloss:2.291347\n",
      "[211]\ttrain-mlogloss:2.116194\teval-mlogloss:2.291255\n",
      "[212]\ttrain-mlogloss:2.115586\teval-mlogloss:2.291202\n",
      "[213]\ttrain-mlogloss:2.114999\teval-mlogloss:2.291102\n",
      "[214]\ttrain-mlogloss:2.114418\teval-mlogloss:2.291023\n",
      "[215]\ttrain-mlogloss:2.113772\teval-mlogloss:2.290878\n",
      "[216]\ttrain-mlogloss:2.113254\teval-mlogloss:2.290794\n",
      "[217]\ttrain-mlogloss:2.112719\teval-mlogloss:2.290655\n",
      "[218]\ttrain-mlogloss:2.112175\teval-mlogloss:2.290624\n",
      "[219]\ttrain-mlogloss:2.111590\teval-mlogloss:2.290552\n",
      "[220]\ttrain-mlogloss:2.111102\teval-mlogloss:2.290460\n",
      "[221]\ttrain-mlogloss:2.110585\teval-mlogloss:2.290377\n",
      "[222]\ttrain-mlogloss:2.109989\teval-mlogloss:2.290294\n",
      "[223]\ttrain-mlogloss:2.109407\teval-mlogloss:2.290236\n",
      "[224]\ttrain-mlogloss:2.108781\teval-mlogloss:2.290209\n",
      "[225]\ttrain-mlogloss:2.108274\teval-mlogloss:2.290180\n",
      "[226]\ttrain-mlogloss:2.107736\teval-mlogloss:2.290112\n",
      "[227]\ttrain-mlogloss:2.107080\teval-mlogloss:2.290028\n",
      "[228]\ttrain-mlogloss:2.106435\teval-mlogloss:2.290008\n",
      "[229]\ttrain-mlogloss:2.105857\teval-mlogloss:2.289960\n",
      "[230]\ttrain-mlogloss:2.105350\teval-mlogloss:2.289907\n",
      "[231]\ttrain-mlogloss:2.104784\teval-mlogloss:2.289775\n",
      "[232]\ttrain-mlogloss:2.104258\teval-mlogloss:2.289626\n",
      "[233]\ttrain-mlogloss:2.103612\teval-mlogloss:2.289539\n",
      "[234]\ttrain-mlogloss:2.103085\teval-mlogloss:2.289444\n",
      "[235]\ttrain-mlogloss:2.102505\teval-mlogloss:2.289387\n",
      "[236]\ttrain-mlogloss:2.102053\teval-mlogloss:2.289371\n",
      "[237]\ttrain-mlogloss:2.101568\teval-mlogloss:2.289278\n",
      "[238]\ttrain-mlogloss:2.100963\teval-mlogloss:2.289210\n",
      "[239]\ttrain-mlogloss:2.100399\teval-mlogloss:2.289173\n",
      "[240]\ttrain-mlogloss:2.099897\teval-mlogloss:2.289162\n",
      "[241]\ttrain-mlogloss:2.099482\teval-mlogloss:2.289157\n",
      "[242]\ttrain-mlogloss:2.098956\teval-mlogloss:2.289077\n",
      "[243]\ttrain-mlogloss:2.098299\teval-mlogloss:2.289008\n",
      "[244]\ttrain-mlogloss:2.097759\teval-mlogloss:2.288935\n",
      "[245]\ttrain-mlogloss:2.097186\teval-mlogloss:2.288861\n",
      "[246]\ttrain-mlogloss:2.096686\teval-mlogloss:2.288784\n",
      "[247]\ttrain-mlogloss:2.096163\teval-mlogloss:2.288691\n",
      "[248]\ttrain-mlogloss:2.095657\teval-mlogloss:2.288605\n",
      "[249]\ttrain-mlogloss:2.095105\teval-mlogloss:2.288505\n",
      "[250]\ttrain-mlogloss:2.094633\teval-mlogloss:2.288411\n",
      "[251]\ttrain-mlogloss:2.094139\teval-mlogloss:2.288330\n",
      "[252]\ttrain-mlogloss:2.093647\teval-mlogloss:2.288315\n",
      "[253]\ttrain-mlogloss:2.093047\teval-mlogloss:2.288202\n",
      "[254]\ttrain-mlogloss:2.092514\teval-mlogloss:2.288093\n",
      "[255]\ttrain-mlogloss:2.091970\teval-mlogloss:2.288063\n",
      "[256]\ttrain-mlogloss:2.091466\teval-mlogloss:2.287986\n",
      "[257]\ttrain-mlogloss:2.090919\teval-mlogloss:2.287851\n",
      "[258]\ttrain-mlogloss:2.090397\teval-mlogloss:2.287792\n",
      "[259]\ttrain-mlogloss:2.089864\teval-mlogloss:2.287740\n",
      "[260]\ttrain-mlogloss:2.089363\teval-mlogloss:2.287649\n",
      "[261]\ttrain-mlogloss:2.088918\teval-mlogloss:2.287599\n",
      "[262]\ttrain-mlogloss:2.088373\teval-mlogloss:2.287485\n",
      "[263]\ttrain-mlogloss:2.087958\teval-mlogloss:2.287434\n",
      "[264]\ttrain-mlogloss:2.087436\teval-mlogloss:2.287446\n",
      "[265]\ttrain-mlogloss:2.086950\teval-mlogloss:2.287360\n",
      "[266]\ttrain-mlogloss:2.086392\teval-mlogloss:2.287237\n",
      "[267]\ttrain-mlogloss:2.085961\teval-mlogloss:2.287221\n",
      "[268]\ttrain-mlogloss:2.085522\teval-mlogloss:2.287124\n",
      "[269]\ttrain-mlogloss:2.085032\teval-mlogloss:2.287036\n",
      "[270]\ttrain-mlogloss:2.084609\teval-mlogloss:2.286984\n",
      "[271]\ttrain-mlogloss:2.084150\teval-mlogloss:2.286921\n",
      "[272]\ttrain-mlogloss:2.083675\teval-mlogloss:2.286860\n",
      "[273]\ttrain-mlogloss:2.083197\teval-mlogloss:2.286799\n",
      "[274]\ttrain-mlogloss:2.082652\teval-mlogloss:2.286732\n",
      "[275]\ttrain-mlogloss:2.082139\teval-mlogloss:2.286719\n",
      "[276]\ttrain-mlogloss:2.081668\teval-mlogloss:2.286613\n",
      "[277]\ttrain-mlogloss:2.081170\teval-mlogloss:2.286523\n",
      "[278]\ttrain-mlogloss:2.080739\teval-mlogloss:2.286416\n",
      "[279]\ttrain-mlogloss:2.080205\teval-mlogloss:2.286358\n",
      "[280]\ttrain-mlogloss:2.079756\teval-mlogloss:2.286298\n",
      "[281]\ttrain-mlogloss:2.079280\teval-mlogloss:2.286234\n",
      "[282]\ttrain-mlogloss:2.078863\teval-mlogloss:2.286161\n",
      "[283]\ttrain-mlogloss:2.078360\teval-mlogloss:2.286115\n",
      "[284]\ttrain-mlogloss:2.077861\teval-mlogloss:2.285979\n",
      "[285]\ttrain-mlogloss:2.077332\teval-mlogloss:2.285891\n",
      "[286]\ttrain-mlogloss:2.076860\teval-mlogloss:2.285817\n",
      "[287]\ttrain-mlogloss:2.076396\teval-mlogloss:2.285746\n",
      "[288]\ttrain-mlogloss:2.075781\teval-mlogloss:2.285569\n",
      "[289]\ttrain-mlogloss:2.075238\teval-mlogloss:2.285528\n",
      "[290]\ttrain-mlogloss:2.074781\teval-mlogloss:2.285502\n",
      "[291]\ttrain-mlogloss:2.074273\teval-mlogloss:2.285453\n",
      "[292]\ttrain-mlogloss:2.073726\teval-mlogloss:2.285379\n",
      "[293]\ttrain-mlogloss:2.073294\teval-mlogloss:2.285341\n",
      "[294]\ttrain-mlogloss:2.072914\teval-mlogloss:2.285273\n",
      "[295]\ttrain-mlogloss:2.072524\teval-mlogloss:2.285313\n",
      "[296]\ttrain-mlogloss:2.072139\teval-mlogloss:2.285239\n",
      "[297]\ttrain-mlogloss:2.071708\teval-mlogloss:2.285135\n",
      "[298]\ttrain-mlogloss:2.071253\teval-mlogloss:2.285099\n",
      "[299]\ttrain-mlogloss:2.070814\teval-mlogloss:2.285075\n",
      "[300]\ttrain-mlogloss:2.070314\teval-mlogloss:2.284980\n",
      "[301]\ttrain-mlogloss:2.069863\teval-mlogloss:2.284859\n",
      "[302]\ttrain-mlogloss:2.069490\teval-mlogloss:2.284839\n",
      "[303]\ttrain-mlogloss:2.069011\teval-mlogloss:2.284825\n",
      "[304]\ttrain-mlogloss:2.068502\teval-mlogloss:2.284737\n",
      "[305]\ttrain-mlogloss:2.067949\teval-mlogloss:2.284690\n",
      "[306]\ttrain-mlogloss:2.067509\teval-mlogloss:2.284658\n",
      "[307]\ttrain-mlogloss:2.067007\teval-mlogloss:2.284616\n",
      "[308]\ttrain-mlogloss:2.066556\teval-mlogloss:2.284525\n",
      "[309]\ttrain-mlogloss:2.066031\teval-mlogloss:2.284479\n",
      "[310]\ttrain-mlogloss:2.065503\teval-mlogloss:2.284363\n",
      "[311]\ttrain-mlogloss:2.065068\teval-mlogloss:2.284325\n",
      "[312]\ttrain-mlogloss:2.064574\teval-mlogloss:2.284279\n",
      "[313]\ttrain-mlogloss:2.064181\teval-mlogloss:2.284254\n",
      "[314]\ttrain-mlogloss:2.063688\teval-mlogloss:2.284250\n",
      "[315]\ttrain-mlogloss:2.063217\teval-mlogloss:2.284187\n",
      "[316]\ttrain-mlogloss:2.062736\teval-mlogloss:2.284125\n",
      "[317]\ttrain-mlogloss:2.062319\teval-mlogloss:2.284100\n",
      "[318]\ttrain-mlogloss:2.061820\teval-mlogloss:2.283997\n",
      "[319]\ttrain-mlogloss:2.061350\teval-mlogloss:2.283991\n",
      "[320]\ttrain-mlogloss:2.060959\teval-mlogloss:2.283974\n",
      "[321]\ttrain-mlogloss:2.060446\teval-mlogloss:2.283963\n",
      "[322]\ttrain-mlogloss:2.059963\teval-mlogloss:2.283931\n",
      "[323]\ttrain-mlogloss:2.059495\teval-mlogloss:2.283816\n",
      "[324]\ttrain-mlogloss:2.059076\teval-mlogloss:2.283731\n",
      "[325]\ttrain-mlogloss:2.058647\teval-mlogloss:2.283708\n",
      "[326]\ttrain-mlogloss:2.058211\teval-mlogloss:2.283707\n",
      "[327]\ttrain-mlogloss:2.057854\teval-mlogloss:2.283644\n",
      "[328]\ttrain-mlogloss:2.057401\teval-mlogloss:2.283596\n",
      "[329]\ttrain-mlogloss:2.056889\teval-mlogloss:2.283500\n",
      "[330]\ttrain-mlogloss:2.056433\teval-mlogloss:2.283450\n",
      "[331]\ttrain-mlogloss:2.055968\teval-mlogloss:2.283400\n",
      "[332]\ttrain-mlogloss:2.055492\teval-mlogloss:2.283352\n",
      "[333]\ttrain-mlogloss:2.055086\teval-mlogloss:2.283253\n",
      "[334]\ttrain-mlogloss:2.054569\teval-mlogloss:2.283140\n",
      "[335]\ttrain-mlogloss:2.054162\teval-mlogloss:2.283086\n",
      "[336]\ttrain-mlogloss:2.053834\teval-mlogloss:2.283004\n",
      "[337]\ttrain-mlogloss:2.053355\teval-mlogloss:2.282970\n",
      "[338]\ttrain-mlogloss:2.052939\teval-mlogloss:2.282957\n",
      "[339]\ttrain-mlogloss:2.052487\teval-mlogloss:2.282956\n",
      "[340]\ttrain-mlogloss:2.052050\teval-mlogloss:2.282920\n",
      "[341]\ttrain-mlogloss:2.051718\teval-mlogloss:2.282939\n",
      "[342]\ttrain-mlogloss:2.051230\teval-mlogloss:2.282882\n",
      "[343]\ttrain-mlogloss:2.050837\teval-mlogloss:2.282863\n",
      "[344]\ttrain-mlogloss:2.050445\teval-mlogloss:2.282854\n",
      "[345]\ttrain-mlogloss:2.049998\teval-mlogloss:2.282820\n",
      "[346]\ttrain-mlogloss:2.049565\teval-mlogloss:2.282820\n",
      "[347]\ttrain-mlogloss:2.049162\teval-mlogloss:2.282820\n",
      "[348]\ttrain-mlogloss:2.048771\teval-mlogloss:2.282781\n",
      "[349]\ttrain-mlogloss:2.048437\teval-mlogloss:2.282738\n",
      "[350]\ttrain-mlogloss:2.048020\teval-mlogloss:2.282653\n",
      "[351]\ttrain-mlogloss:2.047634\teval-mlogloss:2.282644\n",
      "[352]\ttrain-mlogloss:2.047170\teval-mlogloss:2.282606\n",
      "[353]\ttrain-mlogloss:2.046747\teval-mlogloss:2.282560\n",
      "[354]\ttrain-mlogloss:2.046356\teval-mlogloss:2.282491\n",
      "[355]\ttrain-mlogloss:2.045947\teval-mlogloss:2.282466\n",
      "[356]\ttrain-mlogloss:2.045506\teval-mlogloss:2.282432\n",
      "[357]\ttrain-mlogloss:2.044986\teval-mlogloss:2.282426\n",
      "[358]\ttrain-mlogloss:2.044635\teval-mlogloss:2.282341\n",
      "[359]\ttrain-mlogloss:2.044219\teval-mlogloss:2.282356\n",
      "[360]\ttrain-mlogloss:2.043766\teval-mlogloss:2.282343\n",
      "[361]\ttrain-mlogloss:2.043319\teval-mlogloss:2.282271\n",
      "[362]\ttrain-mlogloss:2.042937\teval-mlogloss:2.282225\n",
      "[363]\ttrain-mlogloss:2.042610\teval-mlogloss:2.282235\n",
      "[364]\ttrain-mlogloss:2.042163\teval-mlogloss:2.282239\n",
      "[365]\ttrain-mlogloss:2.041745\teval-mlogloss:2.282183\n",
      "[366]\ttrain-mlogloss:2.041354\teval-mlogloss:2.282136\n",
      "[367]\ttrain-mlogloss:2.040893\teval-mlogloss:2.282105\n",
      "[368]\ttrain-mlogloss:2.040472\teval-mlogloss:2.282146\n",
      "[369]\ttrain-mlogloss:2.040053\teval-mlogloss:2.282101\n",
      "[370]\ttrain-mlogloss:2.039643\teval-mlogloss:2.282013\n",
      "[371]\ttrain-mlogloss:2.039236\teval-mlogloss:2.281986\n",
      "[372]\ttrain-mlogloss:2.038885\teval-mlogloss:2.282000\n",
      "[373]\ttrain-mlogloss:2.038517\teval-mlogloss:2.281995\n",
      "[374]\ttrain-mlogloss:2.038018\teval-mlogloss:2.281924\n",
      "[375]\ttrain-mlogloss:2.037668\teval-mlogloss:2.281902\n",
      "[376]\ttrain-mlogloss:2.037202\teval-mlogloss:2.281889\n",
      "[377]\ttrain-mlogloss:2.036789\teval-mlogloss:2.281855\n",
      "[378]\ttrain-mlogloss:2.036361\teval-mlogloss:2.281835\n",
      "[379]\ttrain-mlogloss:2.035917\teval-mlogloss:2.281808\n",
      "[380]\ttrain-mlogloss:2.035524\teval-mlogloss:2.281809\n",
      "[381]\ttrain-mlogloss:2.035163\teval-mlogloss:2.281754\n",
      "[382]\ttrain-mlogloss:2.034768\teval-mlogloss:2.281708\n",
      "[383]\ttrain-mlogloss:2.034350\teval-mlogloss:2.281665\n",
      "[384]\ttrain-mlogloss:2.033913\teval-mlogloss:2.281679\n",
      "[385]\ttrain-mlogloss:2.033597\teval-mlogloss:2.281646\n",
      "[386]\ttrain-mlogloss:2.033115\teval-mlogloss:2.281608\n",
      "[387]\ttrain-mlogloss:2.032716\teval-mlogloss:2.281541\n",
      "[388]\ttrain-mlogloss:2.032273\teval-mlogloss:2.281509\n",
      "[389]\ttrain-mlogloss:2.031944\teval-mlogloss:2.281515\n",
      "[390]\ttrain-mlogloss:2.031477\teval-mlogloss:2.281533\n",
      "[391]\ttrain-mlogloss:2.031034\teval-mlogloss:2.281567\n",
      "[392]\ttrain-mlogloss:2.030585\teval-mlogloss:2.281558\n",
      "[393]\ttrain-mlogloss:2.030169\teval-mlogloss:2.281568\n",
      "[394]\ttrain-mlogloss:2.029710\teval-mlogloss:2.281529\n",
      "[395]\ttrain-mlogloss:2.029292\teval-mlogloss:2.281472\n",
      "[396]\ttrain-mlogloss:2.028958\teval-mlogloss:2.281386\n",
      "[397]\ttrain-mlogloss:2.028556\teval-mlogloss:2.281330\n",
      "[398]\ttrain-mlogloss:2.028059\teval-mlogloss:2.281318\n",
      "[399]\ttrain-mlogloss:2.027682\teval-mlogloss:2.281313\n",
      "[400]\ttrain-mlogloss:2.027226\teval-mlogloss:2.281248\n",
      "[401]\ttrain-mlogloss:2.026809\teval-mlogloss:2.281239\n",
      "[402]\ttrain-mlogloss:2.026478\teval-mlogloss:2.281240\n",
      "[403]\ttrain-mlogloss:2.026132\teval-mlogloss:2.281249\n",
      "[404]\ttrain-mlogloss:2.025739\teval-mlogloss:2.281231\n",
      "[405]\ttrain-mlogloss:2.025328\teval-mlogloss:2.281195\n",
      "[406]\ttrain-mlogloss:2.024952\teval-mlogloss:2.281182\n",
      "[407]\ttrain-mlogloss:2.024538\teval-mlogloss:2.281163\n",
      "[408]\ttrain-mlogloss:2.024152\teval-mlogloss:2.281114\n",
      "[409]\ttrain-mlogloss:2.023800\teval-mlogloss:2.281092\n",
      "[410]\ttrain-mlogloss:2.023428\teval-mlogloss:2.281074\n",
      "[411]\ttrain-mlogloss:2.023045\teval-mlogloss:2.281101\n",
      "[412]\ttrain-mlogloss:2.022604\teval-mlogloss:2.281124\n",
      "[413]\ttrain-mlogloss:2.022220\teval-mlogloss:2.281120\n",
      "[414]\ttrain-mlogloss:2.021784\teval-mlogloss:2.281112\n",
      "[415]\ttrain-mlogloss:2.021482\teval-mlogloss:2.281092\n",
      "[416]\ttrain-mlogloss:2.021131\teval-mlogloss:2.281084\n",
      "[417]\ttrain-mlogloss:2.020717\teval-mlogloss:2.281059\n",
      "[418]\ttrain-mlogloss:2.020336\teval-mlogloss:2.281005\n",
      "[419]\ttrain-mlogloss:2.019922\teval-mlogloss:2.281034\n",
      "[420]\ttrain-mlogloss:2.019564\teval-mlogloss:2.280999\n",
      "[421]\ttrain-mlogloss:2.019118\teval-mlogloss:2.280971\n",
      "[422]\ttrain-mlogloss:2.018672\teval-mlogloss:2.280955\n",
      "[423]\ttrain-mlogloss:2.018306\teval-mlogloss:2.280925\n",
      "[424]\ttrain-mlogloss:2.017977\teval-mlogloss:2.280962\n",
      "[425]\ttrain-mlogloss:2.017610\teval-mlogloss:2.280984\n",
      "[426]\ttrain-mlogloss:2.017254\teval-mlogloss:2.280957\n",
      "[427]\ttrain-mlogloss:2.016887\teval-mlogloss:2.280936\n",
      "[428]\ttrain-mlogloss:2.016524\teval-mlogloss:2.280918\n",
      "[429]\ttrain-mlogloss:2.016204\teval-mlogloss:2.280916\n",
      "[430]\ttrain-mlogloss:2.015906\teval-mlogloss:2.280909\n",
      "[431]\ttrain-mlogloss:2.015536\teval-mlogloss:2.280897\n",
      "[432]\ttrain-mlogloss:2.015033\teval-mlogloss:2.280850\n",
      "[433]\ttrain-mlogloss:2.014651\teval-mlogloss:2.280847\n",
      "[434]\ttrain-mlogloss:2.014282\teval-mlogloss:2.280783\n",
      "[435]\ttrain-mlogloss:2.013936\teval-mlogloss:2.280808\n",
      "[436]\ttrain-mlogloss:2.013583\teval-mlogloss:2.280773\n",
      "[437]\ttrain-mlogloss:2.013157\teval-mlogloss:2.280765\n",
      "[438]\ttrain-mlogloss:2.012823\teval-mlogloss:2.280737\n",
      "[439]\ttrain-mlogloss:2.012392\teval-mlogloss:2.280681\n",
      "[440]\ttrain-mlogloss:2.012010\teval-mlogloss:2.280653\n",
      "[441]\ttrain-mlogloss:2.011689\teval-mlogloss:2.280617\n",
      "[442]\ttrain-mlogloss:2.011337\teval-mlogloss:2.280559\n",
      "[443]\ttrain-mlogloss:2.011003\teval-mlogloss:2.280505\n",
      "[444]\ttrain-mlogloss:2.010620\teval-mlogloss:2.280471\n",
      "[445]\ttrain-mlogloss:2.010320\teval-mlogloss:2.280480\n",
      "[446]\ttrain-mlogloss:2.009934\teval-mlogloss:2.280463\n",
      "[447]\ttrain-mlogloss:2.009556\teval-mlogloss:2.280429\n",
      "[448]\ttrain-mlogloss:2.009233\teval-mlogloss:2.280386\n",
      "[449]\ttrain-mlogloss:2.008967\teval-mlogloss:2.280403\n",
      "[450]\ttrain-mlogloss:2.008656\teval-mlogloss:2.280374\n",
      "[451]\ttrain-mlogloss:2.008338\teval-mlogloss:2.280351\n",
      "[452]\ttrain-mlogloss:2.008012\teval-mlogloss:2.280349\n",
      "[453]\ttrain-mlogloss:2.007581\teval-mlogloss:2.280334\n",
      "[454]\ttrain-mlogloss:2.007303\teval-mlogloss:2.280244\n",
      "[455]\ttrain-mlogloss:2.006998\teval-mlogloss:2.280220\n",
      "[456]\ttrain-mlogloss:2.006638\teval-mlogloss:2.280194\n",
      "[457]\ttrain-mlogloss:2.006293\teval-mlogloss:2.280177\n",
      "[458]\ttrain-mlogloss:2.005805\teval-mlogloss:2.280169\n",
      "[459]\ttrain-mlogloss:2.005447\teval-mlogloss:2.280150\n",
      "[460]\ttrain-mlogloss:2.005075\teval-mlogloss:2.280176\n",
      "[461]\ttrain-mlogloss:2.004719\teval-mlogloss:2.280144\n",
      "[462]\ttrain-mlogloss:2.004327\teval-mlogloss:2.280106\n",
      "[463]\ttrain-mlogloss:2.004049\teval-mlogloss:2.280058\n",
      "[464]\ttrain-mlogloss:2.003654\teval-mlogloss:2.280022\n",
      "[465]\ttrain-mlogloss:2.003324\teval-mlogloss:2.279982\n",
      "[466]\ttrain-mlogloss:2.003010\teval-mlogloss:2.280009\n",
      "[467]\ttrain-mlogloss:2.002710\teval-mlogloss:2.279967\n",
      "[468]\ttrain-mlogloss:2.002370\teval-mlogloss:2.279965\n",
      "[469]\ttrain-mlogloss:2.002013\teval-mlogloss:2.279937\n",
      "[470]\ttrain-mlogloss:2.001703\teval-mlogloss:2.279953\n",
      "[471]\ttrain-mlogloss:2.001347\teval-mlogloss:2.279964\n",
      "[472]\ttrain-mlogloss:2.001030\teval-mlogloss:2.279973\n",
      "[473]\ttrain-mlogloss:2.000647\teval-mlogloss:2.279943\n",
      "[474]\ttrain-mlogloss:2.000288\teval-mlogloss:2.279921\n",
      "[475]\ttrain-mlogloss:1.999918\teval-mlogloss:2.279901\n",
      "[476]\ttrain-mlogloss:1.999495\teval-mlogloss:2.279866\n",
      "[477]\ttrain-mlogloss:1.999150\teval-mlogloss:2.279850\n",
      "[478]\ttrain-mlogloss:1.998692\teval-mlogloss:2.279851\n",
      "[479]\ttrain-mlogloss:1.998409\teval-mlogloss:2.279828\n",
      "[480]\ttrain-mlogloss:1.998095\teval-mlogloss:2.279835\n",
      "[481]\ttrain-mlogloss:1.997687\teval-mlogloss:2.279790\n",
      "[482]\ttrain-mlogloss:1.997434\teval-mlogloss:2.279800\n",
      "[483]\ttrain-mlogloss:1.997081\teval-mlogloss:2.279759\n",
      "[484]\ttrain-mlogloss:1.996742\teval-mlogloss:2.279741\n",
      "[485]\ttrain-mlogloss:1.996457\teval-mlogloss:2.279764\n",
      "[486]\ttrain-mlogloss:1.996100\teval-mlogloss:2.279708\n",
      "[487]\ttrain-mlogloss:1.995811\teval-mlogloss:2.279714\n",
      "[488]\ttrain-mlogloss:1.995429\teval-mlogloss:2.279672\n",
      "[489]\ttrain-mlogloss:1.995041\teval-mlogloss:2.279626\n",
      "[490]\ttrain-mlogloss:1.994742\teval-mlogloss:2.279647\n",
      "[491]\ttrain-mlogloss:1.994446\teval-mlogloss:2.279636\n",
      "[492]\ttrain-mlogloss:1.994169\teval-mlogloss:2.279613\n",
      "[493]\ttrain-mlogloss:1.993837\teval-mlogloss:2.279598\n",
      "[494]\ttrain-mlogloss:1.993491\teval-mlogloss:2.279610\n",
      "[495]\ttrain-mlogloss:1.993221\teval-mlogloss:2.279603\n",
      "[496]\ttrain-mlogloss:1.992831\teval-mlogloss:2.279559\n",
      "[497]\ttrain-mlogloss:1.992470\teval-mlogloss:2.279581\n",
      "[498]\ttrain-mlogloss:1.992064\teval-mlogloss:2.279545\n",
      "[499]\ttrain-mlogloss:1.991694\teval-mlogloss:2.279571\n",
      "[500]\ttrain-mlogloss:1.991399\teval-mlogloss:2.279578\n",
      "[501]\ttrain-mlogloss:1.990980\teval-mlogloss:2.279524\n",
      "[502]\ttrain-mlogloss:1.990628\teval-mlogloss:2.279496\n",
      "[503]\ttrain-mlogloss:1.990281\teval-mlogloss:2.279482\n",
      "[504]\ttrain-mlogloss:1.989909\teval-mlogloss:2.279505\n",
      "[505]\ttrain-mlogloss:1.989600\teval-mlogloss:2.279462\n",
      "[506]\ttrain-mlogloss:1.989285\teval-mlogloss:2.279418\n",
      "[507]\ttrain-mlogloss:1.989013\teval-mlogloss:2.279387\n",
      "[508]\ttrain-mlogloss:1.988633\teval-mlogloss:2.279406\n",
      "[509]\ttrain-mlogloss:1.988291\teval-mlogloss:2.279386\n",
      "[510]\ttrain-mlogloss:1.987991\teval-mlogloss:2.279363\n",
      "[511]\ttrain-mlogloss:1.987723\teval-mlogloss:2.279341\n",
      "[512]\ttrain-mlogloss:1.987359\teval-mlogloss:2.279330\n",
      "[513]\ttrain-mlogloss:1.986982\teval-mlogloss:2.279343\n",
      "[514]\ttrain-mlogloss:1.986742\teval-mlogloss:2.279329\n",
      "[515]\ttrain-mlogloss:1.986348\teval-mlogloss:2.279297\n",
      "[516]\ttrain-mlogloss:1.986013\teval-mlogloss:2.279325\n",
      "[517]\ttrain-mlogloss:1.985615\teval-mlogloss:2.279255\n",
      "[518]\ttrain-mlogloss:1.985272\teval-mlogloss:2.279273\n",
      "[519]\ttrain-mlogloss:1.984957\teval-mlogloss:2.279254\n",
      "[520]\ttrain-mlogloss:1.984699\teval-mlogloss:2.279265\n",
      "[521]\ttrain-mlogloss:1.984451\teval-mlogloss:2.279227\n",
      "[522]\ttrain-mlogloss:1.984169\teval-mlogloss:2.279226\n",
      "[523]\ttrain-mlogloss:1.983853\teval-mlogloss:2.279191\n",
      "[524]\ttrain-mlogloss:1.983555\teval-mlogloss:2.279193\n",
      "[525]\ttrain-mlogloss:1.983236\teval-mlogloss:2.279132\n",
      "[526]\ttrain-mlogloss:1.982958\teval-mlogloss:2.279156\n",
      "[527]\ttrain-mlogloss:1.982689\teval-mlogloss:2.279132\n",
      "[528]\ttrain-mlogloss:1.982428\teval-mlogloss:2.279154\n",
      "[529]\ttrain-mlogloss:1.982102\teval-mlogloss:2.279102\n",
      "[530]\ttrain-mlogloss:1.981783\teval-mlogloss:2.279097\n",
      "[531]\ttrain-mlogloss:1.981485\teval-mlogloss:2.279117\n",
      "[532]\ttrain-mlogloss:1.981168\teval-mlogloss:2.279133\n",
      "[533]\ttrain-mlogloss:1.980881\teval-mlogloss:2.279120\n",
      "[534]\ttrain-mlogloss:1.980577\teval-mlogloss:2.279109\n",
      "[535]\ttrain-mlogloss:1.980279\teval-mlogloss:2.279111\n",
      "[536]\ttrain-mlogloss:1.980014\teval-mlogloss:2.279130\n",
      "[537]\ttrain-mlogloss:1.979667\teval-mlogloss:2.279101\n",
      "[538]\ttrain-mlogloss:1.979333\teval-mlogloss:2.279074\n",
      "[539]\ttrain-mlogloss:1.979008\teval-mlogloss:2.279093\n",
      "[540]\ttrain-mlogloss:1.978733\teval-mlogloss:2.279109\n",
      "[541]\ttrain-mlogloss:1.978400\teval-mlogloss:2.279110\n",
      "[542]\ttrain-mlogloss:1.978008\teval-mlogloss:2.279100\n",
      "[543]\ttrain-mlogloss:1.977648\teval-mlogloss:2.279079\n",
      "[544]\ttrain-mlogloss:1.977365\teval-mlogloss:2.279065\n",
      "[545]\ttrain-mlogloss:1.977000\teval-mlogloss:2.279106\n",
      "[546]\ttrain-mlogloss:1.976642\teval-mlogloss:2.279042\n",
      "[547]\ttrain-mlogloss:1.976258\teval-mlogloss:2.278992\n",
      "[548]\ttrain-mlogloss:1.975935\teval-mlogloss:2.278939\n",
      "[549]\ttrain-mlogloss:1.975624\teval-mlogloss:2.278930\n",
      "[550]\ttrain-mlogloss:1.975323\teval-mlogloss:2.278899\n",
      "[551]\ttrain-mlogloss:1.974939\teval-mlogloss:2.278907\n",
      "[552]\ttrain-mlogloss:1.974614\teval-mlogloss:2.278894\n",
      "[553]\ttrain-mlogloss:1.974245\teval-mlogloss:2.278892\n",
      "[554]\ttrain-mlogloss:1.973888\teval-mlogloss:2.278883\n",
      "[555]\ttrain-mlogloss:1.973625\teval-mlogloss:2.278906\n",
      "[556]\ttrain-mlogloss:1.973313\teval-mlogloss:2.278891\n",
      "[557]\ttrain-mlogloss:1.972973\teval-mlogloss:2.278898\n",
      "[558]\ttrain-mlogloss:1.972636\teval-mlogloss:2.278897\n",
      "[559]\ttrain-mlogloss:1.972316\teval-mlogloss:2.278935\n",
      "[560]\ttrain-mlogloss:1.971967\teval-mlogloss:2.278984\n",
      "[561]\ttrain-mlogloss:1.971598\teval-mlogloss:2.278985\n",
      "[562]\ttrain-mlogloss:1.971345\teval-mlogloss:2.278954\n",
      "[563]\ttrain-mlogloss:1.971115\teval-mlogloss:2.278929\n",
      "[564]\ttrain-mlogloss:1.970866\teval-mlogloss:2.278893\n",
      "[565]\ttrain-mlogloss:1.970566\teval-mlogloss:2.278948\n",
      "[566]\ttrain-mlogloss:1.970187\teval-mlogloss:2.278986\n",
      "[567]\ttrain-mlogloss:1.969856\teval-mlogloss:2.278997\n",
      "[568]\ttrain-mlogloss:1.969540\teval-mlogloss:2.279010\n",
      "[569]\ttrain-mlogloss:1.969258\teval-mlogloss:2.279027\n",
      "[570]\ttrain-mlogloss:1.968937\teval-mlogloss:2.278986\n",
      "[571]\ttrain-mlogloss:1.968666\teval-mlogloss:2.278987\n",
      "[572]\ttrain-mlogloss:1.968312\teval-mlogloss:2.278983\n",
      "[573]\ttrain-mlogloss:1.967992\teval-mlogloss:2.279001\n",
      "[574]\ttrain-mlogloss:1.967734\teval-mlogloss:2.279044\n",
      "[575]\ttrain-mlogloss:1.967474\teval-mlogloss:2.279020\n",
      "[576]\ttrain-mlogloss:1.967192\teval-mlogloss:2.278989\n",
      "[577]\ttrain-mlogloss:1.966827\teval-mlogloss:2.279015\n",
      "[578]\ttrain-mlogloss:1.966504\teval-mlogloss:2.278952\n",
      "[579]\ttrain-mlogloss:1.966203\teval-mlogloss:2.278977\n",
      "[580]\ttrain-mlogloss:1.965813\teval-mlogloss:2.278939\n",
      "[581]\ttrain-mlogloss:1.965520\teval-mlogloss:2.278916\n",
      "[582]\ttrain-mlogloss:1.965185\teval-mlogloss:2.278932\n",
      "[583]\ttrain-mlogloss:1.964936\teval-mlogloss:2.278917\n",
      "[584]\ttrain-mlogloss:1.964604\teval-mlogloss:2.278955\n",
      "[585]\ttrain-mlogloss:1.964318\teval-mlogloss:2.278938\n",
      "[586]\ttrain-mlogloss:1.964056\teval-mlogloss:2.278928\n",
      "[587]\ttrain-mlogloss:1.963777\teval-mlogloss:2.278924\n",
      "[588]\ttrain-mlogloss:1.963499\teval-mlogloss:2.278951\n",
      "[589]\ttrain-mlogloss:1.963163\teval-mlogloss:2.278975\n",
      "[590]\ttrain-mlogloss:1.962877\teval-mlogloss:2.278964\n",
      "[591]\ttrain-mlogloss:1.962578\teval-mlogloss:2.278972\n",
      "[592]\ttrain-mlogloss:1.962278\teval-mlogloss:2.278981\n",
      "[593]\ttrain-mlogloss:1.961985\teval-mlogloss:2.278977\n",
      "[594]\ttrain-mlogloss:1.961636\teval-mlogloss:2.278963\n",
      "[595]\ttrain-mlogloss:1.961280\teval-mlogloss:2.278974\n",
      "[596]\ttrain-mlogloss:1.960956\teval-mlogloss:2.278928\n",
      "[597]\ttrain-mlogloss:1.960715\teval-mlogloss:2.278914\n",
      "[598]\ttrain-mlogloss:1.960407\teval-mlogloss:2.278920\n",
      "[599]\ttrain-mlogloss:1.960022\teval-mlogloss:2.278927\n",
      "[600]\ttrain-mlogloss:1.959716\teval-mlogloss:2.278966\n",
      "[601]\ttrain-mlogloss:1.959481\teval-mlogloss:2.278975\n",
      "[602]\ttrain-mlogloss:1.959167\teval-mlogloss:2.278975\n",
      "[603]\ttrain-mlogloss:1.958912\teval-mlogloss:2.279036\n",
      "[604]\ttrain-mlogloss:1.958598\teval-mlogloss:2.279045\n",
      "[605]\ttrain-mlogloss:1.958287\teval-mlogloss:2.279053\n",
      "[606]\ttrain-mlogloss:1.958042\teval-mlogloss:2.279056\n",
      "[607]\ttrain-mlogloss:1.957729\teval-mlogloss:2.279075\n",
      "[608]\ttrain-mlogloss:1.957390\teval-mlogloss:2.279063\n",
      "[609]\ttrain-mlogloss:1.957035\teval-mlogloss:2.279076\n",
      "[610]\ttrain-mlogloss:1.956720\teval-mlogloss:2.279088\n",
      "[611]\ttrain-mlogloss:1.956444\teval-mlogloss:2.279098\n",
      "[612]\ttrain-mlogloss:1.956178\teval-mlogloss:2.279073\n",
      "[613]\ttrain-mlogloss:1.955824\teval-mlogloss:2.279083\n",
      "[614]\ttrain-mlogloss:1.955510\teval-mlogloss:2.279072\n",
      "[615]\ttrain-mlogloss:1.955248\teval-mlogloss:2.279119\n",
      "[616]\ttrain-mlogloss:1.955052\teval-mlogloss:2.279112\n",
      "[617]\ttrain-mlogloss:1.954796\teval-mlogloss:2.279108\n",
      "[618]\ttrain-mlogloss:1.954486\teval-mlogloss:2.279125\n",
      "[619]\ttrain-mlogloss:1.954226\teval-mlogloss:2.279150\n",
      "[620]\ttrain-mlogloss:1.953962\teval-mlogloss:2.279169\n",
      "[621]\ttrain-mlogloss:1.953651\teval-mlogloss:2.279178\n",
      "[622]\ttrain-mlogloss:1.953361\teval-mlogloss:2.279144\n",
      "[623]\ttrain-mlogloss:1.953099\teval-mlogloss:2.279068\n",
      "[624]\ttrain-mlogloss:1.952780\teval-mlogloss:2.279068\n",
      "[625]\ttrain-mlogloss:1.952484\teval-mlogloss:2.279113\n",
      "[626]\ttrain-mlogloss:1.952258\teval-mlogloss:2.279150\n",
      "[627]\ttrain-mlogloss:1.952033\teval-mlogloss:2.279171\n",
      "[628]\ttrain-mlogloss:1.951679\teval-mlogloss:2.279202\n",
      "[629]\ttrain-mlogloss:1.951411\teval-mlogloss:2.279209\n",
      "[630]\ttrain-mlogloss:1.951163\teval-mlogloss:2.279214\n",
      "[631]\ttrain-mlogloss:1.950839\teval-mlogloss:2.279259\n",
      "[632]\ttrain-mlogloss:1.950623\teval-mlogloss:2.279259\n",
      "[633]\ttrain-mlogloss:1.950321\teval-mlogloss:2.279226\n",
      "[634]\ttrain-mlogloss:1.950033\teval-mlogloss:2.279217\n",
      "[635]\ttrain-mlogloss:1.949777\teval-mlogloss:2.279169\n",
      "[636]\ttrain-mlogloss:1.949463\teval-mlogloss:2.279153\n",
      "[637]\ttrain-mlogloss:1.949165\teval-mlogloss:2.279185\n",
      "[638]\ttrain-mlogloss:1.948937\teval-mlogloss:2.279206\n",
      "[639]\ttrain-mlogloss:1.948637\teval-mlogloss:2.279217\n",
      "[640]\ttrain-mlogloss:1.948417\teval-mlogloss:2.279236\n",
      "[641]\ttrain-mlogloss:1.948144\teval-mlogloss:2.279243\n",
      "[642]\ttrain-mlogloss:1.947810\teval-mlogloss:2.279244\n",
      "[643]\ttrain-mlogloss:1.947606\teval-mlogloss:2.279237\n",
      "[644]\ttrain-mlogloss:1.947257\teval-mlogloss:2.279285\n",
      "[645]\ttrain-mlogloss:1.946990\teval-mlogloss:2.279288\n",
      "[646]\ttrain-mlogloss:1.946656\teval-mlogloss:2.279330\n",
      "[647]\ttrain-mlogloss:1.946380\teval-mlogloss:2.279320\n",
      "[648]\ttrain-mlogloss:1.946039\teval-mlogloss:2.279310\n",
      "[649]\ttrain-mlogloss:1.945756\teval-mlogloss:2.279298\n",
      "[650]\ttrain-mlogloss:1.945482\teval-mlogloss:2.279345\n",
      "[651]\ttrain-mlogloss:1.945172\teval-mlogloss:2.279323\n",
      "[652]\ttrain-mlogloss:1.944801\teval-mlogloss:2.279334\n",
      "[653]\ttrain-mlogloss:1.944505\teval-mlogloss:2.279339\n",
      "[654]\ttrain-mlogloss:1.944235\teval-mlogloss:2.279375\n",
      "Stopping. Best iteration:\n",
      "[554]\ttrain-mlogloss:1.973888\teval-mlogloss:2.278883\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n",
      "Predict test set...\n",
      "Training time: 4.65 minutes\n",
      "2.27889130245\n"
     ]
    }
   ],
   "source": [
    "res1, score1 = run_xgb(train_sp, test_sp, Y, eta=0.07, random_state=100)\n",
    "print score1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost params. ETA: 0.07, MAX_DEPTH: 5, SUBSAMPLE: 0.7, COLSAMPLE_BY_TREE: 0.7\n",
      "('Length train:', 52251)\n",
      "('Length valid:', 22394)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 100 rounds.\n",
      "[0]\ttrain-mlogloss:2.473025\teval-mlogloss:2.475304\n",
      "[1]\ttrain-mlogloss:2.461930\teval-mlogloss:2.466236\n",
      "[2]\ttrain-mlogloss:2.451746\teval-mlogloss:2.458080\n",
      "[3]\ttrain-mlogloss:2.442297\teval-mlogloss:2.450231\n",
      "[4]\ttrain-mlogloss:2.433736\teval-mlogloss:2.443236\n",
      "[5]\ttrain-mlogloss:2.425351\teval-mlogloss:2.436614\n",
      "[6]\ttrain-mlogloss:2.417495\teval-mlogloss:2.430377\n",
      "[7]\ttrain-mlogloss:2.410064\teval-mlogloss:2.424597\n",
      "[8]\ttrain-mlogloss:2.403033\teval-mlogloss:2.419322\n",
      "[9]\ttrain-mlogloss:2.396508\teval-mlogloss:2.414510\n",
      "[10]\ttrain-mlogloss:2.390391\teval-mlogloss:2.409921\n",
      "[11]\ttrain-mlogloss:2.384352\teval-mlogloss:2.405509\n",
      "[12]\ttrain-mlogloss:2.378884\teval-mlogloss:2.401271\n",
      "[13]\ttrain-mlogloss:2.373378\teval-mlogloss:2.397378\n",
      "[14]\ttrain-mlogloss:2.368134\teval-mlogloss:2.393631\n",
      "[15]\ttrain-mlogloss:2.363213\teval-mlogloss:2.390075\n",
      "[16]\ttrain-mlogloss:2.358443\teval-mlogloss:2.386889\n",
      "[17]\ttrain-mlogloss:2.353901\teval-mlogloss:2.383849\n",
      "[18]\ttrain-mlogloss:2.349366\teval-mlogloss:2.380990\n",
      "[19]\ttrain-mlogloss:2.344956\teval-mlogloss:2.378191\n",
      "[20]\ttrain-mlogloss:2.340952\teval-mlogloss:2.375600\n",
      "[21]\ttrain-mlogloss:2.336984\teval-mlogloss:2.372937\n",
      "[22]\ttrain-mlogloss:2.333289\teval-mlogloss:2.370482\n",
      "[23]\ttrain-mlogloss:2.329845\teval-mlogloss:2.368273\n",
      "[24]\ttrain-mlogloss:2.326453\teval-mlogloss:2.366167\n",
      "[25]\ttrain-mlogloss:2.323118\teval-mlogloss:2.364143\n",
      "[26]\ttrain-mlogloss:2.319902\teval-mlogloss:2.362214\n",
      "[27]\ttrain-mlogloss:2.316829\teval-mlogloss:2.360245\n",
      "[28]\ttrain-mlogloss:2.314021\teval-mlogloss:2.358495\n",
      "[29]\ttrain-mlogloss:2.311114\teval-mlogloss:2.356803\n",
      "[30]\ttrain-mlogloss:2.308208\teval-mlogloss:2.355051\n",
      "[31]\ttrain-mlogloss:2.305313\teval-mlogloss:2.353261\n",
      "[32]\ttrain-mlogloss:2.302612\teval-mlogloss:2.351712\n",
      "[33]\ttrain-mlogloss:2.299852\teval-mlogloss:2.350160\n",
      "[34]\ttrain-mlogloss:2.297317\teval-mlogloss:2.348757\n",
      "[35]\ttrain-mlogloss:2.294783\teval-mlogloss:2.347372\n",
      "[36]\ttrain-mlogloss:2.292476\teval-mlogloss:2.346118\n",
      "[37]\ttrain-mlogloss:2.290136\teval-mlogloss:2.344969\n",
      "[38]\ttrain-mlogloss:2.287885\teval-mlogloss:2.343817\n",
      "[39]\ttrain-mlogloss:2.285705\teval-mlogloss:2.342718\n",
      "[40]\ttrain-mlogloss:2.283449\teval-mlogloss:2.341570\n",
      "[41]\ttrain-mlogloss:2.281199\teval-mlogloss:2.340426\n",
      "[42]\ttrain-mlogloss:2.278932\teval-mlogloss:2.339351\n",
      "[43]\ttrain-mlogloss:2.276983\teval-mlogloss:2.338285\n",
      "[44]\ttrain-mlogloss:2.274976\teval-mlogloss:2.337348\n",
      "[45]\ttrain-mlogloss:2.272965\teval-mlogloss:2.336416\n",
      "[46]\ttrain-mlogloss:2.270793\teval-mlogloss:2.335497\n",
      "[47]\ttrain-mlogloss:2.268947\teval-mlogloss:2.334813\n",
      "[48]\ttrain-mlogloss:2.267195\teval-mlogloss:2.333861\n",
      "[49]\ttrain-mlogloss:2.265371\teval-mlogloss:2.333061\n",
      "[50]\ttrain-mlogloss:2.263695\teval-mlogloss:2.332184\n",
      "[51]\ttrain-mlogloss:2.261803\teval-mlogloss:2.331317\n",
      "[52]\ttrain-mlogloss:2.259916\teval-mlogloss:2.330480\n",
      "[53]\ttrain-mlogloss:2.258246\teval-mlogloss:2.329787\n",
      "[54]\ttrain-mlogloss:2.256530\teval-mlogloss:2.329080\n",
      "[55]\ttrain-mlogloss:2.254915\teval-mlogloss:2.328460\n",
      "[56]\ttrain-mlogloss:2.253325\teval-mlogloss:2.327752\n",
      "[57]\ttrain-mlogloss:2.251756\teval-mlogloss:2.327076\n",
      "[58]\ttrain-mlogloss:2.250238\teval-mlogloss:2.326461\n",
      "[59]\ttrain-mlogloss:2.248673\teval-mlogloss:2.325810\n",
      "[60]\ttrain-mlogloss:2.247166\teval-mlogloss:2.325240\n",
      "[61]\ttrain-mlogloss:2.245574\teval-mlogloss:2.324639\n",
      "[62]\ttrain-mlogloss:2.244138\teval-mlogloss:2.324092\n",
      "[63]\ttrain-mlogloss:2.242622\teval-mlogloss:2.323528\n",
      "[64]\ttrain-mlogloss:2.241261\teval-mlogloss:2.322938\n",
      "[65]\ttrain-mlogloss:2.239733\teval-mlogloss:2.322393\n",
      "[66]\ttrain-mlogloss:2.238234\teval-mlogloss:2.321954\n",
      "[67]\ttrain-mlogloss:2.236833\teval-mlogloss:2.321470\n",
      "[68]\ttrain-mlogloss:2.235435\teval-mlogloss:2.320987\n",
      "[69]\ttrain-mlogloss:2.234026\teval-mlogloss:2.320379\n",
      "[70]\ttrain-mlogloss:2.232722\teval-mlogloss:2.319836\n",
      "[71]\ttrain-mlogloss:2.231580\teval-mlogloss:2.319319\n",
      "[72]\ttrain-mlogloss:2.230278\teval-mlogloss:2.318798\n",
      "[73]\ttrain-mlogloss:2.228978\teval-mlogloss:2.318392\n",
      "[74]\ttrain-mlogloss:2.227713\teval-mlogloss:2.317875\n",
      "[75]\ttrain-mlogloss:2.226506\teval-mlogloss:2.317417\n",
      "[76]\ttrain-mlogloss:2.225369\teval-mlogloss:2.317019\n",
      "[77]\ttrain-mlogloss:2.224191\teval-mlogloss:2.316714\n",
      "[78]\ttrain-mlogloss:2.222990\teval-mlogloss:2.316240\n",
      "[79]\ttrain-mlogloss:2.221804\teval-mlogloss:2.315829\n",
      "[80]\ttrain-mlogloss:2.220774\teval-mlogloss:2.315401\n",
      "[81]\ttrain-mlogloss:2.219690\teval-mlogloss:2.315084\n",
      "[82]\ttrain-mlogloss:2.218585\teval-mlogloss:2.314728\n",
      "[83]\ttrain-mlogloss:2.217484\teval-mlogloss:2.314332\n",
      "[84]\ttrain-mlogloss:2.216423\teval-mlogloss:2.314026\n",
      "[85]\ttrain-mlogloss:2.215267\teval-mlogloss:2.313598\n",
      "[86]\ttrain-mlogloss:2.214067\teval-mlogloss:2.313210\n",
      "[87]\ttrain-mlogloss:2.213008\teval-mlogloss:2.312929\n",
      "[88]\ttrain-mlogloss:2.211988\teval-mlogloss:2.312635\n",
      "[89]\ttrain-mlogloss:2.210883\teval-mlogloss:2.312184\n",
      "[90]\ttrain-mlogloss:2.209903\teval-mlogloss:2.311872\n",
      "[91]\ttrain-mlogloss:2.208836\teval-mlogloss:2.311652\n",
      "[92]\ttrain-mlogloss:2.207556\teval-mlogloss:2.311220\n",
      "[93]\ttrain-mlogloss:2.206443\teval-mlogloss:2.311012\n",
      "[94]\ttrain-mlogloss:2.205300\teval-mlogloss:2.310686\n",
      "[95]\ttrain-mlogloss:2.204247\teval-mlogloss:2.310423\n",
      "[96]\ttrain-mlogloss:2.203309\teval-mlogloss:2.310186\n",
      "[97]\ttrain-mlogloss:2.202304\teval-mlogloss:2.309977\n",
      "[98]\ttrain-mlogloss:2.201336\teval-mlogloss:2.309730\n",
      "[99]\ttrain-mlogloss:2.200254\teval-mlogloss:2.309424\n",
      "[100]\ttrain-mlogloss:2.199238\teval-mlogloss:2.309118\n",
      "[101]\ttrain-mlogloss:2.198277\teval-mlogloss:2.308917\n",
      "[102]\ttrain-mlogloss:2.197260\teval-mlogloss:2.308705\n",
      "[103]\ttrain-mlogloss:2.196217\teval-mlogloss:2.308465\n",
      "[104]\ttrain-mlogloss:2.195292\teval-mlogloss:2.308195\n",
      "[105]\ttrain-mlogloss:2.194382\teval-mlogloss:2.308015\n",
      "[106]\ttrain-mlogloss:2.193446\teval-mlogloss:2.307741\n",
      "[107]\ttrain-mlogloss:2.192404\teval-mlogloss:2.307454\n",
      "[108]\ttrain-mlogloss:2.191564\teval-mlogloss:2.307163\n",
      "[109]\ttrain-mlogloss:2.190661\teval-mlogloss:2.306918\n",
      "[110]\ttrain-mlogloss:2.189845\teval-mlogloss:2.306686\n",
      "[111]\ttrain-mlogloss:2.188937\teval-mlogloss:2.306439\n",
      "[112]\ttrain-mlogloss:2.188037\teval-mlogloss:2.306122\n",
      "[113]\ttrain-mlogloss:2.187072\teval-mlogloss:2.305859\n",
      "[114]\ttrain-mlogloss:2.186121\teval-mlogloss:2.305645\n",
      "[115]\ttrain-mlogloss:2.185317\teval-mlogloss:2.305438\n",
      "[116]\ttrain-mlogloss:2.184448\teval-mlogloss:2.305302\n",
      "[117]\ttrain-mlogloss:2.183582\teval-mlogloss:2.305045\n",
      "[118]\ttrain-mlogloss:2.182673\teval-mlogloss:2.304834\n",
      "[119]\ttrain-mlogloss:2.181647\teval-mlogloss:2.304533\n",
      "[120]\ttrain-mlogloss:2.180742\teval-mlogloss:2.304396\n",
      "[121]\ttrain-mlogloss:2.179974\teval-mlogloss:2.304134\n",
      "[122]\ttrain-mlogloss:2.179162\teval-mlogloss:2.303946\n",
      "[123]\ttrain-mlogloss:2.178298\teval-mlogloss:2.303721\n",
      "[124]\ttrain-mlogloss:2.177468\teval-mlogloss:2.303655\n",
      "[125]\ttrain-mlogloss:2.176713\teval-mlogloss:2.303462\n",
      "[126]\ttrain-mlogloss:2.175874\teval-mlogloss:2.303285\n",
      "[127]\ttrain-mlogloss:2.174913\teval-mlogloss:2.303085\n",
      "[128]\ttrain-mlogloss:2.173977\teval-mlogloss:2.302750\n",
      "[129]\ttrain-mlogloss:2.173215\teval-mlogloss:2.302576\n",
      "[130]\ttrain-mlogloss:2.172342\teval-mlogloss:2.302290\n",
      "[131]\ttrain-mlogloss:2.171467\teval-mlogloss:2.302082\n",
      "[132]\ttrain-mlogloss:2.170599\teval-mlogloss:2.301928\n",
      "[133]\ttrain-mlogloss:2.169888\teval-mlogloss:2.301785\n",
      "[134]\ttrain-mlogloss:2.168846\teval-mlogloss:2.301514\n",
      "[135]\ttrain-mlogloss:2.167979\teval-mlogloss:2.301376\n",
      "[136]\ttrain-mlogloss:2.167213\teval-mlogloss:2.301148\n",
      "[137]\ttrain-mlogloss:2.166340\teval-mlogloss:2.301028\n",
      "[138]\ttrain-mlogloss:2.165557\teval-mlogloss:2.300858\n",
      "[139]\ttrain-mlogloss:2.164792\teval-mlogloss:2.300632\n",
      "[140]\ttrain-mlogloss:2.164025\teval-mlogloss:2.300560\n",
      "[141]\ttrain-mlogloss:2.163136\teval-mlogloss:2.300339\n",
      "[142]\ttrain-mlogloss:2.162384\teval-mlogloss:2.300215\n",
      "[143]\ttrain-mlogloss:2.161585\teval-mlogloss:2.300087\n",
      "[144]\ttrain-mlogloss:2.160984\teval-mlogloss:2.299958\n",
      "[145]\ttrain-mlogloss:2.160286\teval-mlogloss:2.299814\n",
      "[146]\ttrain-mlogloss:2.159472\teval-mlogloss:2.299665\n",
      "[147]\ttrain-mlogloss:2.158801\teval-mlogloss:2.299582\n",
      "[148]\ttrain-mlogloss:2.157984\teval-mlogloss:2.299426\n",
      "[149]\ttrain-mlogloss:2.157251\teval-mlogloss:2.299305\n",
      "[150]\ttrain-mlogloss:2.156468\teval-mlogloss:2.299139\n",
      "[151]\ttrain-mlogloss:2.155803\teval-mlogloss:2.299010\n",
      "[152]\ttrain-mlogloss:2.155097\teval-mlogloss:2.298847\n",
      "[153]\ttrain-mlogloss:2.154429\teval-mlogloss:2.298662\n",
      "[154]\ttrain-mlogloss:2.153688\teval-mlogloss:2.298562\n",
      "[155]\ttrain-mlogloss:2.152773\teval-mlogloss:2.298448\n",
      "[156]\ttrain-mlogloss:2.152062\teval-mlogloss:2.298393\n",
      "[157]\ttrain-mlogloss:2.151360\teval-mlogloss:2.298306\n",
      "[158]\ttrain-mlogloss:2.150600\teval-mlogloss:2.298064\n",
      "[159]\ttrain-mlogloss:2.149919\teval-mlogloss:2.297933\n",
      "[160]\ttrain-mlogloss:2.149215\teval-mlogloss:2.297871\n",
      "[161]\ttrain-mlogloss:2.148391\teval-mlogloss:2.297750\n",
      "[162]\ttrain-mlogloss:2.147761\teval-mlogloss:2.297654\n",
      "[163]\ttrain-mlogloss:2.147155\teval-mlogloss:2.297539\n",
      "[164]\ttrain-mlogloss:2.146413\teval-mlogloss:2.297428\n",
      "[165]\ttrain-mlogloss:2.145709\teval-mlogloss:2.297312\n",
      "[166]\ttrain-mlogloss:2.144955\teval-mlogloss:2.297118\n",
      "[167]\ttrain-mlogloss:2.144391\teval-mlogloss:2.296985\n",
      "[168]\ttrain-mlogloss:2.143512\teval-mlogloss:2.296825\n",
      "[169]\ttrain-mlogloss:2.142717\teval-mlogloss:2.296728\n",
      "[170]\ttrain-mlogloss:2.142044\teval-mlogloss:2.296650\n",
      "[171]\ttrain-mlogloss:2.141274\teval-mlogloss:2.296462\n",
      "[172]\ttrain-mlogloss:2.140629\teval-mlogloss:2.296352\n",
      "[173]\ttrain-mlogloss:2.139931\teval-mlogloss:2.296243\n",
      "[174]\ttrain-mlogloss:2.139378\teval-mlogloss:2.296152\n",
      "[175]\ttrain-mlogloss:2.138630\teval-mlogloss:2.296056\n",
      "[176]\ttrain-mlogloss:2.138011\teval-mlogloss:2.295888\n",
      "[177]\ttrain-mlogloss:2.137423\teval-mlogloss:2.295784\n",
      "[178]\ttrain-mlogloss:2.136870\teval-mlogloss:2.295683\n",
      "[179]\ttrain-mlogloss:2.136183\teval-mlogloss:2.295578\n",
      "[180]\ttrain-mlogloss:2.135527\teval-mlogloss:2.295403\n",
      "[181]\ttrain-mlogloss:2.134775\teval-mlogloss:2.295343\n",
      "[182]\ttrain-mlogloss:2.134107\teval-mlogloss:2.295239\n",
      "[183]\ttrain-mlogloss:2.133412\teval-mlogloss:2.295188\n",
      "[184]\ttrain-mlogloss:2.132797\teval-mlogloss:2.295070\n",
      "[185]\ttrain-mlogloss:2.132025\teval-mlogloss:2.294935\n",
      "[186]\ttrain-mlogloss:2.131272\teval-mlogloss:2.294886\n",
      "[187]\ttrain-mlogloss:2.130671\teval-mlogloss:2.294809\n",
      "[188]\ttrain-mlogloss:2.130010\teval-mlogloss:2.294692\n",
      "[189]\ttrain-mlogloss:2.129379\teval-mlogloss:2.294643\n",
      "[190]\ttrain-mlogloss:2.128835\teval-mlogloss:2.294532\n",
      "[191]\ttrain-mlogloss:2.128242\teval-mlogloss:2.294429\n",
      "[192]\ttrain-mlogloss:2.127605\teval-mlogloss:2.294322\n",
      "[193]\ttrain-mlogloss:2.127100\teval-mlogloss:2.294243\n",
      "[194]\ttrain-mlogloss:2.126458\teval-mlogloss:2.294116\n",
      "[195]\ttrain-mlogloss:2.125790\teval-mlogloss:2.294027\n",
      "[196]\ttrain-mlogloss:2.125240\teval-mlogloss:2.293946\n",
      "[197]\ttrain-mlogloss:2.124652\teval-mlogloss:2.293888\n",
      "[198]\ttrain-mlogloss:2.124071\teval-mlogloss:2.293795\n",
      "[199]\ttrain-mlogloss:2.123475\teval-mlogloss:2.293725\n",
      "[200]\ttrain-mlogloss:2.122953\teval-mlogloss:2.293630\n",
      "[201]\ttrain-mlogloss:2.122358\teval-mlogloss:2.293529\n",
      "[202]\ttrain-mlogloss:2.121670\teval-mlogloss:2.293455\n",
      "[203]\ttrain-mlogloss:2.121135\teval-mlogloss:2.293340\n",
      "[204]\ttrain-mlogloss:2.120525\teval-mlogloss:2.293252\n",
      "[205]\ttrain-mlogloss:2.119889\teval-mlogloss:2.293164\n",
      "[206]\ttrain-mlogloss:2.119341\teval-mlogloss:2.293043\n",
      "[207]\ttrain-mlogloss:2.118776\teval-mlogloss:2.292935\n",
      "[208]\ttrain-mlogloss:2.118229\teval-mlogloss:2.292841\n",
      "[209]\ttrain-mlogloss:2.117537\teval-mlogloss:2.292749\n",
      "[210]\ttrain-mlogloss:2.116955\teval-mlogloss:2.292625\n",
      "[211]\ttrain-mlogloss:2.116439\teval-mlogloss:2.292529\n",
      "[212]\ttrain-mlogloss:2.115787\teval-mlogloss:2.292407\n",
      "[213]\ttrain-mlogloss:2.115189\teval-mlogloss:2.292296\n",
      "[214]\ttrain-mlogloss:2.114601\teval-mlogloss:2.292170\n",
      "[215]\ttrain-mlogloss:2.114057\teval-mlogloss:2.292095\n",
      "[216]\ttrain-mlogloss:2.113462\teval-mlogloss:2.291975\n",
      "[217]\ttrain-mlogloss:2.112938\teval-mlogloss:2.291858\n",
      "[218]\ttrain-mlogloss:2.112406\teval-mlogloss:2.291797\n",
      "[219]\ttrain-mlogloss:2.111871\teval-mlogloss:2.291754\n",
      "[220]\ttrain-mlogloss:2.111304\teval-mlogloss:2.291672\n",
      "[221]\ttrain-mlogloss:2.110786\teval-mlogloss:2.291552\n",
      "[222]\ttrain-mlogloss:2.110238\teval-mlogloss:2.291528\n",
      "[223]\ttrain-mlogloss:2.109692\teval-mlogloss:2.291498\n",
      "[224]\ttrain-mlogloss:2.109084\teval-mlogloss:2.291343\n",
      "[225]\ttrain-mlogloss:2.108525\teval-mlogloss:2.291241\n",
      "[226]\ttrain-mlogloss:2.107914\teval-mlogloss:2.291216\n",
      "[227]\ttrain-mlogloss:2.107312\teval-mlogloss:2.291157\n",
      "[228]\ttrain-mlogloss:2.106774\teval-mlogloss:2.291085\n",
      "[229]\ttrain-mlogloss:2.106216\teval-mlogloss:2.291002\n",
      "[230]\ttrain-mlogloss:2.105701\teval-mlogloss:2.290963\n",
      "[231]\ttrain-mlogloss:2.105151\teval-mlogloss:2.290840\n",
      "[232]\ttrain-mlogloss:2.104603\teval-mlogloss:2.290733\n",
      "[233]\ttrain-mlogloss:2.104024\teval-mlogloss:2.290668\n",
      "[234]\ttrain-mlogloss:2.103367\teval-mlogloss:2.290588\n",
      "[235]\ttrain-mlogloss:2.102843\teval-mlogloss:2.290544\n",
      "[236]\ttrain-mlogloss:2.102280\teval-mlogloss:2.290419\n",
      "[237]\ttrain-mlogloss:2.101768\teval-mlogloss:2.290366\n",
      "[238]\ttrain-mlogloss:2.101232\teval-mlogloss:2.290272\n",
      "[239]\ttrain-mlogloss:2.100646\teval-mlogloss:2.290232\n",
      "[240]\ttrain-mlogloss:2.100081\teval-mlogloss:2.290133\n",
      "[241]\ttrain-mlogloss:2.099473\teval-mlogloss:2.290098\n",
      "[242]\ttrain-mlogloss:2.098889\teval-mlogloss:2.289980\n",
      "[243]\ttrain-mlogloss:2.098395\teval-mlogloss:2.289917\n",
      "[244]\ttrain-mlogloss:2.097807\teval-mlogloss:2.289865\n",
      "[245]\ttrain-mlogloss:2.097293\teval-mlogloss:2.289771\n",
      "[246]\ttrain-mlogloss:2.096765\teval-mlogloss:2.289697\n",
      "[247]\ttrain-mlogloss:2.096250\teval-mlogloss:2.289641\n",
      "[248]\ttrain-mlogloss:2.095738\teval-mlogloss:2.289577\n",
      "[249]\ttrain-mlogloss:2.095280\teval-mlogloss:2.289549\n",
      "[250]\ttrain-mlogloss:2.094830\teval-mlogloss:2.289428\n",
      "[251]\ttrain-mlogloss:2.094392\teval-mlogloss:2.289326\n",
      "[252]\ttrain-mlogloss:2.093904\teval-mlogloss:2.289260\n",
      "[253]\ttrain-mlogloss:2.093290\teval-mlogloss:2.289251\n",
      "[254]\ttrain-mlogloss:2.092796\teval-mlogloss:2.289282\n",
      "[255]\ttrain-mlogloss:2.092346\teval-mlogloss:2.289230\n",
      "[256]\ttrain-mlogloss:2.091794\teval-mlogloss:2.289116\n",
      "[257]\ttrain-mlogloss:2.091322\teval-mlogloss:2.289073\n",
      "[258]\ttrain-mlogloss:2.090769\teval-mlogloss:2.289016\n",
      "[259]\ttrain-mlogloss:2.090317\teval-mlogloss:2.288997\n",
      "[260]\ttrain-mlogloss:2.089782\teval-mlogloss:2.288977\n",
      "[261]\ttrain-mlogloss:2.089239\teval-mlogloss:2.288961\n",
      "[262]\ttrain-mlogloss:2.088672\teval-mlogloss:2.288873\n",
      "[263]\ttrain-mlogloss:2.088151\teval-mlogloss:2.288808\n",
      "[264]\ttrain-mlogloss:2.087626\teval-mlogloss:2.288756\n",
      "[265]\ttrain-mlogloss:2.087133\teval-mlogloss:2.288713\n",
      "[266]\ttrain-mlogloss:2.086678\teval-mlogloss:2.288646\n",
      "[267]\ttrain-mlogloss:2.086215\teval-mlogloss:2.288595\n",
      "[268]\ttrain-mlogloss:2.085686\teval-mlogloss:2.288512\n",
      "[269]\ttrain-mlogloss:2.085181\teval-mlogloss:2.288439\n",
      "[270]\ttrain-mlogloss:2.084684\teval-mlogloss:2.288390\n",
      "[271]\ttrain-mlogloss:2.084135\teval-mlogloss:2.288356\n",
      "[272]\ttrain-mlogloss:2.083640\teval-mlogloss:2.288311\n",
      "[273]\ttrain-mlogloss:2.083112\teval-mlogloss:2.288245\n",
      "[274]\ttrain-mlogloss:2.082669\teval-mlogloss:2.288199\n",
      "[275]\ttrain-mlogloss:2.082247\teval-mlogloss:2.288116\n",
      "[276]\ttrain-mlogloss:2.081797\teval-mlogloss:2.288095\n",
      "[277]\ttrain-mlogloss:2.081280\teval-mlogloss:2.288119\n",
      "[278]\ttrain-mlogloss:2.080745\teval-mlogloss:2.288026\n",
      "[279]\ttrain-mlogloss:2.080246\teval-mlogloss:2.287950\n",
      "[280]\ttrain-mlogloss:2.079759\teval-mlogloss:2.287868\n",
      "[281]\ttrain-mlogloss:2.079278\teval-mlogloss:2.287831\n",
      "[282]\ttrain-mlogloss:2.078816\teval-mlogloss:2.287769\n",
      "[283]\ttrain-mlogloss:2.078439\teval-mlogloss:2.287738\n",
      "[284]\ttrain-mlogloss:2.077893\teval-mlogloss:2.287558\n",
      "[285]\ttrain-mlogloss:2.077426\teval-mlogloss:2.287472\n",
      "[286]\ttrain-mlogloss:2.076884\teval-mlogloss:2.287455\n",
      "[287]\ttrain-mlogloss:2.076337\teval-mlogloss:2.287442\n",
      "[288]\ttrain-mlogloss:2.075836\teval-mlogloss:2.287366\n",
      "[289]\ttrain-mlogloss:2.075432\teval-mlogloss:2.287293\n",
      "[290]\ttrain-mlogloss:2.075004\teval-mlogloss:2.287197\n",
      "[291]\ttrain-mlogloss:2.074487\teval-mlogloss:2.287187\n",
      "[292]\ttrain-mlogloss:2.074035\teval-mlogloss:2.287119\n",
      "[293]\ttrain-mlogloss:2.073477\teval-mlogloss:2.287069\n",
      "[294]\ttrain-mlogloss:2.072942\teval-mlogloss:2.286966\n",
      "[295]\ttrain-mlogloss:2.072454\teval-mlogloss:2.286934\n",
      "[296]\ttrain-mlogloss:2.071901\teval-mlogloss:2.286922\n",
      "[297]\ttrain-mlogloss:2.071387\teval-mlogloss:2.286905\n",
      "[298]\ttrain-mlogloss:2.070958\teval-mlogloss:2.286797\n",
      "[299]\ttrain-mlogloss:2.070491\teval-mlogloss:2.286783\n",
      "[300]\ttrain-mlogloss:2.070083\teval-mlogloss:2.286716\n",
      "[301]\ttrain-mlogloss:2.069716\teval-mlogloss:2.286672\n",
      "[302]\ttrain-mlogloss:2.069236\teval-mlogloss:2.286555\n",
      "[303]\ttrain-mlogloss:2.068781\teval-mlogloss:2.286494\n",
      "[304]\ttrain-mlogloss:2.068398\teval-mlogloss:2.286460\n",
      "[305]\ttrain-mlogloss:2.067915\teval-mlogloss:2.286434\n",
      "[306]\ttrain-mlogloss:2.067515\teval-mlogloss:2.286404\n",
      "[307]\ttrain-mlogloss:2.067026\teval-mlogloss:2.286329\n",
      "[308]\ttrain-mlogloss:2.066605\teval-mlogloss:2.286320\n",
      "[309]\ttrain-mlogloss:2.066175\teval-mlogloss:2.286273\n",
      "[310]\ttrain-mlogloss:2.065755\teval-mlogloss:2.286209\n",
      "[311]\ttrain-mlogloss:2.065322\teval-mlogloss:2.286213\n",
      "[312]\ttrain-mlogloss:2.064877\teval-mlogloss:2.286135\n",
      "[313]\ttrain-mlogloss:2.064444\teval-mlogloss:2.286090\n",
      "[314]\ttrain-mlogloss:2.064017\teval-mlogloss:2.286099\n",
      "[315]\ttrain-mlogloss:2.063588\teval-mlogloss:2.286067\n",
      "[316]\ttrain-mlogloss:2.063104\teval-mlogloss:2.286011\n",
      "[317]\ttrain-mlogloss:2.062703\teval-mlogloss:2.285950\n",
      "[318]\ttrain-mlogloss:2.062223\teval-mlogloss:2.285858\n",
      "[319]\ttrain-mlogloss:2.061723\teval-mlogloss:2.285850\n",
      "[320]\ttrain-mlogloss:2.061315\teval-mlogloss:2.285748\n",
      "[321]\ttrain-mlogloss:2.060779\teval-mlogloss:2.285647\n",
      "[322]\ttrain-mlogloss:2.060368\teval-mlogloss:2.285622\n",
      "[323]\ttrain-mlogloss:2.059983\teval-mlogloss:2.285583\n",
      "[324]\ttrain-mlogloss:2.059567\teval-mlogloss:2.285532\n",
      "[325]\ttrain-mlogloss:2.059138\teval-mlogloss:2.285437\n",
      "[326]\ttrain-mlogloss:2.058684\teval-mlogloss:2.285491\n",
      "[327]\ttrain-mlogloss:2.058209\teval-mlogloss:2.285496\n",
      "[328]\ttrain-mlogloss:2.057725\teval-mlogloss:2.285431\n",
      "[329]\ttrain-mlogloss:2.057328\teval-mlogloss:2.285443\n",
      "[330]\ttrain-mlogloss:2.056865\teval-mlogloss:2.285433\n",
      "[331]\ttrain-mlogloss:2.056376\teval-mlogloss:2.285412\n",
      "[332]\ttrain-mlogloss:2.055888\teval-mlogloss:2.285366\n",
      "[333]\ttrain-mlogloss:2.055475\teval-mlogloss:2.285386\n",
      "[334]\ttrain-mlogloss:2.055049\teval-mlogloss:2.285358\n",
      "[335]\ttrain-mlogloss:2.054681\teval-mlogloss:2.285408\n",
      "[336]\ttrain-mlogloss:2.054264\teval-mlogloss:2.285428\n",
      "[337]\ttrain-mlogloss:2.053764\teval-mlogloss:2.285378\n",
      "[338]\ttrain-mlogloss:2.053390\teval-mlogloss:2.285327\n",
      "[339]\ttrain-mlogloss:2.052983\teval-mlogloss:2.285296\n",
      "[340]\ttrain-mlogloss:2.052461\teval-mlogloss:2.285309\n",
      "[341]\ttrain-mlogloss:2.051963\teval-mlogloss:2.285319\n",
      "[342]\ttrain-mlogloss:2.051505\teval-mlogloss:2.285301\n",
      "[343]\ttrain-mlogloss:2.051040\teval-mlogloss:2.285281\n",
      "[344]\ttrain-mlogloss:2.050565\teval-mlogloss:2.285195\n",
      "[345]\ttrain-mlogloss:2.050239\teval-mlogloss:2.285236\n",
      "[346]\ttrain-mlogloss:2.049816\teval-mlogloss:2.285199\n",
      "[347]\ttrain-mlogloss:2.049435\teval-mlogloss:2.285171\n",
      "[348]\ttrain-mlogloss:2.049029\teval-mlogloss:2.285119\n",
      "[349]\ttrain-mlogloss:2.048664\teval-mlogloss:2.285127\n",
      "[350]\ttrain-mlogloss:2.048303\teval-mlogloss:2.285068\n",
      "[351]\ttrain-mlogloss:2.047950\teval-mlogloss:2.285063\n",
      "[352]\ttrain-mlogloss:2.047608\teval-mlogloss:2.285107\n",
      "[353]\ttrain-mlogloss:2.047192\teval-mlogloss:2.285070\n",
      "[354]\ttrain-mlogloss:2.046803\teval-mlogloss:2.285047\n",
      "[355]\ttrain-mlogloss:2.046319\teval-mlogloss:2.284930\n",
      "[356]\ttrain-mlogloss:2.045823\teval-mlogloss:2.284948\n",
      "[357]\ttrain-mlogloss:2.045401\teval-mlogloss:2.284937\n",
      "[358]\ttrain-mlogloss:2.045083\teval-mlogloss:2.284950\n",
      "[359]\ttrain-mlogloss:2.044669\teval-mlogloss:2.284912\n",
      "[360]\ttrain-mlogloss:2.044214\teval-mlogloss:2.284822\n",
      "[361]\ttrain-mlogloss:2.043799\teval-mlogloss:2.284748\n",
      "[362]\ttrain-mlogloss:2.043462\teval-mlogloss:2.284768\n",
      "[363]\ttrain-mlogloss:2.043063\teval-mlogloss:2.284784\n",
      "[364]\ttrain-mlogloss:2.042708\teval-mlogloss:2.284773\n",
      "[365]\ttrain-mlogloss:2.042372\teval-mlogloss:2.284736\n",
      "[366]\ttrain-mlogloss:2.041984\teval-mlogloss:2.284755\n",
      "[367]\ttrain-mlogloss:2.041611\teval-mlogloss:2.284717\n",
      "[368]\ttrain-mlogloss:2.041239\teval-mlogloss:2.284656\n",
      "[369]\ttrain-mlogloss:2.040788\teval-mlogloss:2.284647\n",
      "[370]\ttrain-mlogloss:2.040332\teval-mlogloss:2.284621\n",
      "[371]\ttrain-mlogloss:2.039912\teval-mlogloss:2.284534\n",
      "[372]\ttrain-mlogloss:2.039577\teval-mlogloss:2.284507\n",
      "[373]\ttrain-mlogloss:2.039166\teval-mlogloss:2.284464\n",
      "[374]\ttrain-mlogloss:2.038782\teval-mlogloss:2.284460\n",
      "[375]\ttrain-mlogloss:2.038375\teval-mlogloss:2.284433\n",
      "[376]\ttrain-mlogloss:2.037974\teval-mlogloss:2.284445\n",
      "[377]\ttrain-mlogloss:2.037457\teval-mlogloss:2.284466\n",
      "[378]\ttrain-mlogloss:2.037040\teval-mlogloss:2.284410\n",
      "[379]\ttrain-mlogloss:2.036660\teval-mlogloss:2.284413\n",
      "[380]\ttrain-mlogloss:2.036287\teval-mlogloss:2.284407\n",
      "[381]\ttrain-mlogloss:2.035943\teval-mlogloss:2.284422\n",
      "[382]\ttrain-mlogloss:2.035582\teval-mlogloss:2.284383\n",
      "[383]\ttrain-mlogloss:2.035182\teval-mlogloss:2.284335\n",
      "[384]\ttrain-mlogloss:2.034876\teval-mlogloss:2.284330\n",
      "[385]\ttrain-mlogloss:2.034516\teval-mlogloss:2.284252\n",
      "[386]\ttrain-mlogloss:2.034105\teval-mlogloss:2.284207\n",
      "[387]\ttrain-mlogloss:2.033659\teval-mlogloss:2.284199\n",
      "[388]\ttrain-mlogloss:2.033337\teval-mlogloss:2.284230\n",
      "[389]\ttrain-mlogloss:2.032973\teval-mlogloss:2.284222\n",
      "[390]\ttrain-mlogloss:2.032607\teval-mlogloss:2.284223\n",
      "[391]\ttrain-mlogloss:2.032213\teval-mlogloss:2.284192\n",
      "[392]\ttrain-mlogloss:2.031812\teval-mlogloss:2.284164\n",
      "[393]\ttrain-mlogloss:2.031381\teval-mlogloss:2.284177\n",
      "[394]\ttrain-mlogloss:2.030984\teval-mlogloss:2.284175\n",
      "[395]\ttrain-mlogloss:2.030462\teval-mlogloss:2.284111\n",
      "[396]\ttrain-mlogloss:2.030144\teval-mlogloss:2.284106\n",
      "[397]\ttrain-mlogloss:2.029770\teval-mlogloss:2.284102\n",
      "[398]\ttrain-mlogloss:2.029418\teval-mlogloss:2.284037\n",
      "[399]\ttrain-mlogloss:2.029002\teval-mlogloss:2.284034\n",
      "[400]\ttrain-mlogloss:2.028575\teval-mlogloss:2.283909\n",
      "[401]\ttrain-mlogloss:2.028265\teval-mlogloss:2.283850\n",
      "[402]\ttrain-mlogloss:2.027812\teval-mlogloss:2.283834\n",
      "[403]\ttrain-mlogloss:2.027438\teval-mlogloss:2.283808\n",
      "[404]\ttrain-mlogloss:2.027044\teval-mlogloss:2.283745\n",
      "[405]\ttrain-mlogloss:2.026693\teval-mlogloss:2.283708\n",
      "[406]\ttrain-mlogloss:2.026316\teval-mlogloss:2.283646\n",
      "[407]\ttrain-mlogloss:2.025814\teval-mlogloss:2.283576\n",
      "[408]\ttrain-mlogloss:2.025375\teval-mlogloss:2.283585\n",
      "[409]\ttrain-mlogloss:2.025016\teval-mlogloss:2.283570\n",
      "[410]\ttrain-mlogloss:2.024703\teval-mlogloss:2.283547\n",
      "[411]\ttrain-mlogloss:2.024363\teval-mlogloss:2.283494\n",
      "[412]\ttrain-mlogloss:2.023943\teval-mlogloss:2.283484\n",
      "[413]\ttrain-mlogloss:2.023572\teval-mlogloss:2.283401\n",
      "[414]\ttrain-mlogloss:2.023139\teval-mlogloss:2.283407\n",
      "[415]\ttrain-mlogloss:2.022730\teval-mlogloss:2.283420\n",
      "[416]\ttrain-mlogloss:2.022470\teval-mlogloss:2.283357\n",
      "[417]\ttrain-mlogloss:2.022066\teval-mlogloss:2.283329\n",
      "[418]\ttrain-mlogloss:2.021732\teval-mlogloss:2.283327\n",
      "[419]\ttrain-mlogloss:2.021358\teval-mlogloss:2.283260\n",
      "[420]\ttrain-mlogloss:2.021056\teval-mlogloss:2.283268\n",
      "[421]\ttrain-mlogloss:2.020692\teval-mlogloss:2.283248\n",
      "[422]\ttrain-mlogloss:2.020336\teval-mlogloss:2.283186\n",
      "[423]\ttrain-mlogloss:2.020000\teval-mlogloss:2.283204\n",
      "[424]\ttrain-mlogloss:2.019629\teval-mlogloss:2.283179\n",
      "[425]\ttrain-mlogloss:2.019274\teval-mlogloss:2.283137\n",
      "[426]\ttrain-mlogloss:2.018857\teval-mlogloss:2.283139\n",
      "[427]\ttrain-mlogloss:2.018497\teval-mlogloss:2.283175\n",
      "[428]\ttrain-mlogloss:2.018189\teval-mlogloss:2.283177\n",
      "[429]\ttrain-mlogloss:2.017770\teval-mlogloss:2.283165\n",
      "[430]\ttrain-mlogloss:2.017430\teval-mlogloss:2.283171\n",
      "[431]\ttrain-mlogloss:2.017023\teval-mlogloss:2.283163\n",
      "[432]\ttrain-mlogloss:2.016690\teval-mlogloss:2.283159\n",
      "[433]\ttrain-mlogloss:2.016329\teval-mlogloss:2.283186\n",
      "[434]\ttrain-mlogloss:2.016013\teval-mlogloss:2.283151\n",
      "[435]\ttrain-mlogloss:2.015630\teval-mlogloss:2.283136\n",
      "[436]\ttrain-mlogloss:2.015162\teval-mlogloss:2.283086\n",
      "[437]\ttrain-mlogloss:2.014825\teval-mlogloss:2.283051\n",
      "[438]\ttrain-mlogloss:2.014463\teval-mlogloss:2.283041\n",
      "[439]\ttrain-mlogloss:2.014054\teval-mlogloss:2.282983\n",
      "[440]\ttrain-mlogloss:2.013666\teval-mlogloss:2.282952\n",
      "[441]\ttrain-mlogloss:2.013309\teval-mlogloss:2.282959\n",
      "[442]\ttrain-mlogloss:2.012976\teval-mlogloss:2.282950\n",
      "[443]\ttrain-mlogloss:2.012543\teval-mlogloss:2.282989\n",
      "[444]\ttrain-mlogloss:2.012140\teval-mlogloss:2.283008\n",
      "[445]\ttrain-mlogloss:2.011811\teval-mlogloss:2.283018\n",
      "[446]\ttrain-mlogloss:2.011531\teval-mlogloss:2.283050\n",
      "[447]\ttrain-mlogloss:2.011211\teval-mlogloss:2.283010\n",
      "[448]\ttrain-mlogloss:2.010863\teval-mlogloss:2.283069\n",
      "[449]\ttrain-mlogloss:2.010454\teval-mlogloss:2.283092\n",
      "[450]\ttrain-mlogloss:2.010017\teval-mlogloss:2.283110\n",
      "[451]\ttrain-mlogloss:2.009632\teval-mlogloss:2.283068\n",
      "[452]\ttrain-mlogloss:2.009263\teval-mlogloss:2.283073\n",
      "[453]\ttrain-mlogloss:2.008925\teval-mlogloss:2.283062\n",
      "[454]\ttrain-mlogloss:2.008598\teval-mlogloss:2.283090\n",
      "[455]\ttrain-mlogloss:2.008292\teval-mlogloss:2.283062\n",
      "[456]\ttrain-mlogloss:2.007919\teval-mlogloss:2.283050\n",
      "[457]\ttrain-mlogloss:2.007554\teval-mlogloss:2.282996\n",
      "[458]\ttrain-mlogloss:2.007219\teval-mlogloss:2.282967\n",
      "[459]\ttrain-mlogloss:2.006922\teval-mlogloss:2.282938\n",
      "[460]\ttrain-mlogloss:2.006531\teval-mlogloss:2.283008\n",
      "[461]\ttrain-mlogloss:2.006088\teval-mlogloss:2.282987\n",
      "[462]\ttrain-mlogloss:2.005685\teval-mlogloss:2.282947\n",
      "[463]\ttrain-mlogloss:2.005289\teval-mlogloss:2.282987\n",
      "[464]\ttrain-mlogloss:2.004890\teval-mlogloss:2.283002\n",
      "[465]\ttrain-mlogloss:2.004595\teval-mlogloss:2.282971\n",
      "[466]\ttrain-mlogloss:2.004282\teval-mlogloss:2.282963\n",
      "[467]\ttrain-mlogloss:2.003873\teval-mlogloss:2.282936\n",
      "[468]\ttrain-mlogloss:2.003545\teval-mlogloss:2.282907\n",
      "[469]\ttrain-mlogloss:2.003237\teval-mlogloss:2.282916\n",
      "[470]\ttrain-mlogloss:2.002931\teval-mlogloss:2.282917\n",
      "[471]\ttrain-mlogloss:2.002576\teval-mlogloss:2.282886\n",
      "[472]\ttrain-mlogloss:2.002188\teval-mlogloss:2.282859\n",
      "[473]\ttrain-mlogloss:2.001833\teval-mlogloss:2.282839\n",
      "[474]\ttrain-mlogloss:2.001507\teval-mlogloss:2.282823\n",
      "[475]\ttrain-mlogloss:2.001153\teval-mlogloss:2.282811\n",
      "[476]\ttrain-mlogloss:2.000690\teval-mlogloss:2.282812\n",
      "[477]\ttrain-mlogloss:2.000421\teval-mlogloss:2.282826\n",
      "[478]\ttrain-mlogloss:2.000066\teval-mlogloss:2.282739\n",
      "[479]\ttrain-mlogloss:1.999750\teval-mlogloss:2.282742\n",
      "[480]\ttrain-mlogloss:1.999416\teval-mlogloss:2.282748\n",
      "[481]\ttrain-mlogloss:1.999050\teval-mlogloss:2.282787\n",
      "[482]\ttrain-mlogloss:1.998694\teval-mlogloss:2.282777\n",
      "[483]\ttrain-mlogloss:1.998393\teval-mlogloss:2.282735\n",
      "[484]\ttrain-mlogloss:1.998017\teval-mlogloss:2.282672\n",
      "[485]\ttrain-mlogloss:1.997600\teval-mlogloss:2.282707\n",
      "[486]\ttrain-mlogloss:1.997326\teval-mlogloss:2.282687\n",
      "[487]\ttrain-mlogloss:1.997061\teval-mlogloss:2.282727\n",
      "[488]\ttrain-mlogloss:1.996767\teval-mlogloss:2.282714\n",
      "[489]\ttrain-mlogloss:1.996384\teval-mlogloss:2.282726\n",
      "[490]\ttrain-mlogloss:1.996092\teval-mlogloss:2.282802\n",
      "[491]\ttrain-mlogloss:1.995761\teval-mlogloss:2.282789\n",
      "[492]\ttrain-mlogloss:1.995478\teval-mlogloss:2.282808\n",
      "[493]\ttrain-mlogloss:1.995072\teval-mlogloss:2.282831\n",
      "[494]\ttrain-mlogloss:1.994805\teval-mlogloss:2.282836\n",
      "[495]\ttrain-mlogloss:1.994407\teval-mlogloss:2.282830\n",
      "[496]\ttrain-mlogloss:1.994069\teval-mlogloss:2.282847\n",
      "[497]\ttrain-mlogloss:1.993715\teval-mlogloss:2.282797\n",
      "[498]\ttrain-mlogloss:1.993359\teval-mlogloss:2.282760\n",
      "[499]\ttrain-mlogloss:1.993050\teval-mlogloss:2.282817\n",
      "[500]\ttrain-mlogloss:1.992804\teval-mlogloss:2.282756\n",
      "[501]\ttrain-mlogloss:1.992463\teval-mlogloss:2.282808\n",
      "[502]\ttrain-mlogloss:1.992155\teval-mlogloss:2.282806\n",
      "[503]\ttrain-mlogloss:1.991802\teval-mlogloss:2.282797\n",
      "[504]\ttrain-mlogloss:1.991439\teval-mlogloss:2.282782\n",
      "[505]\ttrain-mlogloss:1.991022\teval-mlogloss:2.282796\n",
      "[506]\ttrain-mlogloss:1.990750\teval-mlogloss:2.282762\n",
      "[507]\ttrain-mlogloss:1.990387\teval-mlogloss:2.282794\n",
      "[508]\ttrain-mlogloss:1.990035\teval-mlogloss:2.282810\n",
      "[509]\ttrain-mlogloss:1.989720\teval-mlogloss:2.282849\n",
      "[510]\ttrain-mlogloss:1.989388\teval-mlogloss:2.282826\n",
      "[511]\ttrain-mlogloss:1.989020\teval-mlogloss:2.282781\n",
      "[512]\ttrain-mlogloss:1.988656\teval-mlogloss:2.282755\n",
      "[513]\ttrain-mlogloss:1.988308\teval-mlogloss:2.282759\n",
      "[514]\ttrain-mlogloss:1.987916\teval-mlogloss:2.282723\n",
      "[515]\ttrain-mlogloss:1.987549\teval-mlogloss:2.282752\n",
      "[516]\ttrain-mlogloss:1.987239\teval-mlogloss:2.282796\n",
      "[517]\ttrain-mlogloss:1.986954\teval-mlogloss:2.282813\n",
      "[518]\ttrain-mlogloss:1.986552\teval-mlogloss:2.282877\n",
      "[519]\ttrain-mlogloss:1.986208\teval-mlogloss:2.282869\n",
      "[520]\ttrain-mlogloss:1.985860\teval-mlogloss:2.282867\n",
      "[521]\ttrain-mlogloss:1.985471\teval-mlogloss:2.282817\n",
      "[522]\ttrain-mlogloss:1.985203\teval-mlogloss:2.282764\n",
      "[523]\ttrain-mlogloss:1.984923\teval-mlogloss:2.282801\n",
      "[524]\ttrain-mlogloss:1.984566\teval-mlogloss:2.282773\n",
      "[525]\ttrain-mlogloss:1.984225\teval-mlogloss:2.282767\n",
      "[526]\ttrain-mlogloss:1.983947\teval-mlogloss:2.282767\n",
      "[527]\ttrain-mlogloss:1.983561\teval-mlogloss:2.282825\n",
      "[528]\ttrain-mlogloss:1.983230\teval-mlogloss:2.282838\n",
      "[529]\ttrain-mlogloss:1.982918\teval-mlogloss:2.282818\n",
      "[530]\ttrain-mlogloss:1.982565\teval-mlogloss:2.282828\n",
      "[531]\ttrain-mlogloss:1.982198\teval-mlogloss:2.282833\n",
      "[532]\ttrain-mlogloss:1.981850\teval-mlogloss:2.282832\n",
      "[533]\ttrain-mlogloss:1.981521\teval-mlogloss:2.282792\n",
      "[534]\ttrain-mlogloss:1.981192\teval-mlogloss:2.282816\n",
      "[535]\ttrain-mlogloss:1.980806\teval-mlogloss:2.282813\n",
      "[536]\ttrain-mlogloss:1.980512\teval-mlogloss:2.282850\n",
      "[537]\ttrain-mlogloss:1.980246\teval-mlogloss:2.282848\n",
      "[538]\ttrain-mlogloss:1.979978\teval-mlogloss:2.282829\n",
      "[539]\ttrain-mlogloss:1.979670\teval-mlogloss:2.282837\n",
      "[540]\ttrain-mlogloss:1.979314\teval-mlogloss:2.282841\n",
      "[541]\ttrain-mlogloss:1.978999\teval-mlogloss:2.282842\n",
      "[542]\ttrain-mlogloss:1.978647\teval-mlogloss:2.282836\n",
      "[543]\ttrain-mlogloss:1.978355\teval-mlogloss:2.282849\n",
      "[544]\ttrain-mlogloss:1.978021\teval-mlogloss:2.282857\n",
      "[545]\ttrain-mlogloss:1.977705\teval-mlogloss:2.282862\n",
      "[546]\ttrain-mlogloss:1.977364\teval-mlogloss:2.282879\n",
      "[547]\ttrain-mlogloss:1.977098\teval-mlogloss:2.282899\n",
      "[548]\ttrain-mlogloss:1.976748\teval-mlogloss:2.282884\n",
      "[549]\ttrain-mlogloss:1.976402\teval-mlogloss:2.282835\n",
      "[550]\ttrain-mlogloss:1.976050\teval-mlogloss:2.282828\n",
      "[551]\ttrain-mlogloss:1.975723\teval-mlogloss:2.282853\n",
      "[552]\ttrain-mlogloss:1.975460\teval-mlogloss:2.282816\n",
      "[553]\ttrain-mlogloss:1.975076\teval-mlogloss:2.282838\n",
      "[554]\ttrain-mlogloss:1.974737\teval-mlogloss:2.282790\n",
      "[555]\ttrain-mlogloss:1.974390\teval-mlogloss:2.282828\n",
      "[556]\ttrain-mlogloss:1.974053\teval-mlogloss:2.282816\n",
      "[557]\ttrain-mlogloss:1.973776\teval-mlogloss:2.282804\n",
      "[558]\ttrain-mlogloss:1.973464\teval-mlogloss:2.282801\n",
      "[559]\ttrain-mlogloss:1.973132\teval-mlogloss:2.282798\n",
      "[560]\ttrain-mlogloss:1.972855\teval-mlogloss:2.282777\n",
      "[561]\ttrain-mlogloss:1.972471\teval-mlogloss:2.282779\n",
      "[562]\ttrain-mlogloss:1.972165\teval-mlogloss:2.282761\n",
      "[563]\ttrain-mlogloss:1.971857\teval-mlogloss:2.282753\n",
      "[564]\ttrain-mlogloss:1.971568\teval-mlogloss:2.282782\n",
      "[565]\ttrain-mlogloss:1.971283\teval-mlogloss:2.282790\n",
      "[566]\ttrain-mlogloss:1.970934\teval-mlogloss:2.282810\n",
      "[567]\ttrain-mlogloss:1.970642\teval-mlogloss:2.282796\n",
      "[568]\ttrain-mlogloss:1.970339\teval-mlogloss:2.282828\n",
      "[569]\ttrain-mlogloss:1.970007\teval-mlogloss:2.282846\n",
      "[570]\ttrain-mlogloss:1.969744\teval-mlogloss:2.282843\n",
      "[571]\ttrain-mlogloss:1.969417\teval-mlogloss:2.282881\n",
      "[572]\ttrain-mlogloss:1.969096\teval-mlogloss:2.282881\n",
      "[573]\ttrain-mlogloss:1.968704\teval-mlogloss:2.282946\n",
      "[574]\ttrain-mlogloss:1.968414\teval-mlogloss:2.282934\n",
      "[575]\ttrain-mlogloss:1.968016\teval-mlogloss:2.282935\n",
      "[576]\ttrain-mlogloss:1.967671\teval-mlogloss:2.282962\n",
      "[577]\ttrain-mlogloss:1.967319\teval-mlogloss:2.282921\n",
      "[578]\ttrain-mlogloss:1.967039\teval-mlogloss:2.282962\n",
      "[579]\ttrain-mlogloss:1.966735\teval-mlogloss:2.282934\n",
      "[580]\ttrain-mlogloss:1.966472\teval-mlogloss:2.282944\n",
      "[581]\ttrain-mlogloss:1.966134\teval-mlogloss:2.282954\n",
      "[582]\ttrain-mlogloss:1.965800\teval-mlogloss:2.282901\n",
      "[583]\ttrain-mlogloss:1.965447\teval-mlogloss:2.282889\n",
      "[584]\ttrain-mlogloss:1.965041\teval-mlogloss:2.282930\n",
      "Stopping. Best iteration:\n",
      "[484]\ttrain-mlogloss:1.998017\teval-mlogloss:2.282672\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n",
      "Predict test set...\n",
      "Training time: 4.15 minutes\n",
      "2.28273604687\n"
     ]
    }
   ],
   "source": [
    "res2, score2 = run_xgb(train_sp, test_sp, Y, eta=0.07, random_state=142)\n",
    "print score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       F23-    F24-26   F27-28    F29-32    F33-42      F43+      M22-  \\\n",
      "0  0.002184  0.007761  0.00743  0.013168  0.025861  0.027927  0.006436   \n",
      "\n",
      "    M23-26    M27-28    M29-31  M32-38      M39+            device_id  \n",
      "0  0.08699  0.091995  0.179706  0.1959  0.354641  1002079943728939269  \n"
     ]
    }
   ],
   "source": [
    "result_0 = pd.DataFrame(res, columns=lable_group.classes_)\n",
    "result_0[\"device_id\"] = device_id\n",
    "print(result_0.head(1))\n",
    "result_0 = result_0.set_index(\"device_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       F23-    F24-26   F27-28    F29-32    F33-42      F43+      M22-  \\\n",
      "0  0.003058  0.005439  0.01118  0.015913  0.031806  0.042212  0.008021   \n",
      "\n",
      "     M23-26    M27-28    M29-31    M32-38      M39+            device_id  \n",
      "0  0.053217  0.095841  0.161457  0.228428  0.343429  1002079943728939269  \n"
     ]
    }
   ],
   "source": [
    "result_01 = pd.DataFrame(res1, columns=lable_group.classes_)\n",
    "result_01[\"device_id\"] = device_id\n",
    "print(result_01.head(1))\n",
    "result_01 = result_01.set_index(\"device_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       F23-    F24-26    F27-28    F29-32    F33-42      F43+      M22-  \\\n",
      "0  0.003096  0.006087  0.008176  0.018682  0.056807  0.025827  0.005977   \n",
      "\n",
      "     M23-26    M27-28    M29-31    M32-38      M39+            device_id  \n",
      "0  0.035516  0.081324  0.144081  0.133044  0.481383  1002079943728939269  \n"
     ]
    }
   ],
   "source": [
    "result_02 = pd.DataFrame(res2, columns=lable_group.classes_)\n",
    "result_02[\"device_id\"] = device_id\n",
    "print(result_02.head(1))\n",
    "result_02 = result_02.set_index(\"device_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = result_55 * 0.8 + ((result_0 + result_01 + result_02)/3)*0.2 # 2.4009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# result = result_55 # 2.450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.to_csv('new_data.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################\n",
    "#  Build Model\n",
    "##################\n",
    "\n",
    "\n",
    "#act = keras.layers.advanced_activations.PReLU(init='zero', weights=None)\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(150, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(50, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "\n",
    "model = baseline_model()\n",
    "\n",
    "fit= model.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69984,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "scores_val = model.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val)))\n",
    "\n",
    "\n",
    "#print(\"# Final prediction\")\n",
    "#scores = model.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "#result = pd.DataFrame(scores , columns=lable_group.classes_)\n",
    "#result[\"device_id\"] = device_id\n",
    "#print(result.head(1))\n",
    "#result = result.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result.to_csv('keras_' + str(log_loss(y_val, scores_val)) + '.csv', index=True, index_label='device_id')\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def second_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(155, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(50, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_2 = second_model()\n",
    "\n",
    "fit_2 = model_2.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69784,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "scores_val_2 = model_2.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def third_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(145, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(50, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_3 = third_model()\n",
    "\n",
    "fit_3 = model_3.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69884,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "scores_val_3 = model_3.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forth_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(147, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(50, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_4 = forth_model()\n",
    "\n",
    "fit_4 = model_4.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69984,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "scores_val_4 = model_4.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val_4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fifth_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(152, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.42))\n",
    "    model.add(Dense(52, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.18))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_5 = fifth_model()\n",
    "\n",
    "fit_5 = model_5.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69884,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "scores_val_5 = model_5.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val_5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sixth_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(153, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.42))\n",
    "    model.add(Dense(51, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_6 = fifth_model()\n",
    "\n",
    "fit_6 = model_6.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69884,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "scores_val_6 = model_6.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val_5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seventh_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(144, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(50, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_7 = seventh_model()\n",
    "\n",
    "fit_7 = model_7.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69884,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "scores_val_7 = model_7.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val_7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eight_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(145, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(51, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_8 = eight_model()\n",
    "\n",
    "fit_8 = model_8.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69784,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "scores_val_8 = model_8.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val_8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''def ninth_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(156, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(51, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_9 = ninth_model()\n",
    "\n",
    "fit_9 = model_9.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69884,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "scores_val_9 = model_9.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val_9)))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_3 = model_3.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_3 = pd.DataFrame(scores_3 , columns=lable_group.classes_)\n",
    "result_3[\"device_id\"] = device_id\n",
    "print(result_3.head(1))\n",
    "result_3 = result_3.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result_3.to_csv('keras_' + str(log_loss(y_val, scores_val_3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_2 = model_2.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_2 = pd.DataFrame(scores_2 , columns=lable_group.classes_)\n",
    "result_2[\"device_id\"] = device_id\n",
    "print(result_2.head(1))\n",
    "result_2 = result_2.set_index(\"device_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores = model.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result = pd.DataFrame(scores , columns=lable_group.classes_)\n",
    "result[\"device_id\"] = device_id\n",
    "print(result.head(1))\n",
    "result = result.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('keras_' + str(log_loss(y_val, scores_val)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_4 = model_4.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_4 = pd.DataFrame(scores_4 , columns=lable_group.classes_)\n",
    "result_4[\"device_id\"] = device_id\n",
    "print(result_4.head(1))\n",
    "result_4 = result_4.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result_3.to_csv('keras_' + str(log_loss(y_val, scores_val_3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_5 = model_5.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_5 = pd.DataFrame(scores_5 , columns=lable_group.classes_)\n",
    "result_5[\"device_id\"] = device_id\n",
    "print(result_5.head(1))\n",
    "result_5 = result_5.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result_3.to_csv('keras_' + str(log_loss(y_val, scores_val_3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_6 = model_6.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_6 = pd.DataFrame(scores_6 , columns=lable_group.classes_)\n",
    "result_6[\"device_id\"] = device_id\n",
    "print(result_6.head(1))\n",
    "result_6 = result_6.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result_3.to_csv('keras_' + str(log_loss(y_val, scores_val_3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_7 = model_7.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_7 = pd.DataFrame(scores_7 , columns=lable_group.classes_)\n",
    "result_7[\"device_id\"] = device_id\n",
    "print(result_7.head(1))\n",
    "result_7 = result_7.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result_3.to_csv('keras_' + str(log_loss(y_val, scores_val_3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_8 = model_8.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_8 = pd.DataFrame(scores_8 , columns=lable_group.classes_)\n",
    "result_8[\"device_id\"] = device_id\n",
    "print(result_8.head(1))\n",
    "result_8 = result_8.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result_3.to_csv('keras_' + str(log_loss(y_val, scores_val_3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(\"# Final prediction\")\n",
    "#scores_9 = model_9.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "#result_9 = pd.DataFrame(scores_9 , columns=lable_group.classes_)\n",
    "#result_9[\"device_id\"] = device_id\n",
    "#print(result_9.head(1))\n",
    "#result_9 = result_9.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result_3.to_csv('keras_' + str(log_loss(y_val, scores_val_3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tot = result + result_2 + result_3 + result_4 + result_5 + result_6\n",
    "#tot = result_3 + result_5 + result_6 \n",
    "#tot = result + result_2 + result_3 + result_4 + result_5 + result_6 + result_7\n",
    "#tot = result + result_2 + result_3 + result_4 + result_5 + result_6 + result_7 + result_8 + result_9\n",
    "tot = result + result_2 + result_3 + result_5 + result_6 + result_7 + result_8 + result_0 + result_01 + result_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tot = tot/6\n",
    "#tot = tot/3\n",
    "#tot = tot/7\n",
    "#tot = tot/8\n",
    "tot = tot/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tot[tot<0.001]=0.001 # 1,2,3,4,5,5 models + this -> 2.23982\n",
    "tot[tot>0.999]=0.999 # 1,2,3,4,5,5 models + this -> 2.23982"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tot.to_csv('keras_' + str(log_loss(y_val, (scores_val + scores_val_2 + scores_val_3 + scores_val_4 + scores_val_5 + scores_val_6)/6)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot.to_csv('keras_' + str(log_loss(y_val, (scores_val + scores_val_2 + scores_val_3 + scores_val_5 + scores_val_6 + scores_val_7 + scores_val_8)/1)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tot.to_csv('keras_' + str(log_loss(y_val, (scores_val + scores_val_2 + scores_val_3 + scores_val_4 + scores_val_5 + scores_val_6 + scores_val_7 + scores_val_8 + scores_val_9)/9)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
