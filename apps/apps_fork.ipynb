{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize libraries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Bag of apps categories\n",
    "# Bag of labels categories\n",
    "# Include phone brand and model device\n",
    "\n",
    "print(\"Initialize libraries\")\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics as skmetrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import ensemble\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import gc\n",
    "from scipy import sparse\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, chi2, SelectKBest\n",
    "from sklearn import ensemble\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------- Write functions ----------------------------------------\n",
    "\n",
    "def rstr(df): return df.dtypes, df.head(3) ,df.apply(lambda x: [x.unique()]), df.apply(lambda x: [len(x.unique())]),df.shape\n",
    "\n",
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    #chenglong code for fiting from generator (https://www.kaggle.com/c/talkingdata-mobile-user-demographics/forums/t/22567/neural-network-for-sparse-matrices)\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "\n",
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X[batch_index, :].toarray()\n",
    "        counter += 1\n",
    "        yield X_batch\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ----- PART 1 ----- ###\n",
      "# Read app events\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32473067 entries, 0 to 32473066\n",
      "Data columns (total 4 columns):\n",
      "event_id        int64\n",
      "app_id          int64\n",
      "is_installed    int64\n",
      "is_active       int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 991.0 MB\n",
      "# Read Events\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1488096 entries, 1 to 3252947\n",
      "Data columns (total 2 columns):\n",
      "device_id    1488096 non-null object\n",
      "app_id       1488096 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 34.1+ MB\n",
      "#Part1 formed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#------------------------------------------------ Read data from source files ------------------------------------\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "datadir = ''\n",
    "\n",
    "print(\"### ----- PART 1 ----- ###\")\n",
    "\n",
    "# Data - Events data\n",
    "# Bag of apps\n",
    "print(\"# Read app events\")\n",
    "app_events = pd.read_csv(os.path.join(datadir,'app_events.csv'), dtype={'device_id' : np.str})\n",
    "app_events.head(5)\n",
    "app_events.info()\n",
    "#print(rstr(app_events))\n",
    "\n",
    "# remove duplicates(app_id)\n",
    "app_events= app_events.groupby(\"event_id\")[\"app_id\"].apply(\n",
    "    lambda x: \" \".join(set(\"app_id:\" + str(s) for s in x)))\n",
    "app_events.head(5)\n",
    "\n",
    "print(\"# Read Events\")\n",
    "events = pd.read_csv(os.path.join(datadir,'events.csv'), dtype={'device_id': np.str})\n",
    "events.head(5)\n",
    "events[\"app_id\"] = events[\"event_id\"].map(app_events)\n",
    "events = events.dropna()\n",
    "del app_events\n",
    "\n",
    "events = events[[\"device_id\", \"app_id\"]]\n",
    "events.info()\n",
    "# 1Gb reduced to 34 Mb\n",
    "\n",
    "# remove duplicates(app_id)\n",
    "events.loc[:,\"device_id\"].value_counts(ascending=True)\n",
    "\n",
    "events = events.groupby(\"device_id\")[\"app_id\"].apply(\n",
    "    lambda x: \" \".join(set(str(\" \".join(str(s) for s in x)).split(\" \"))))\n",
    "events = events.reset_index(name=\"app_id\")\n",
    "\n",
    "# expand to multiple rows\n",
    "events = pd.concat([pd.Series(row['device_id'], row['app_id'].split(' '))\n",
    "                    for _, row in events.iterrows()]).reset_index()\n",
    "events.columns = ['app_id', 'device_id']\n",
    "events.head(5)\n",
    "f3 = events[[\"device_id\", \"app_id\"]]    # app_id\n",
    "\n",
    "print(\"#Part1 formed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ----- PART 2 ----- ###\n",
      "# Read App labels\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 459943 entries, 0 to 459942\n",
      "Data columns (total 2 columns):\n",
      "app_id      459943 non-null int64\n",
      "label_id    459943 non-null int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 7.0 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 930 entries, 0 to 929\n",
      "Data columns (total 2 columns):\n",
      "label_id    930 non-null int64\n",
      "category    927 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 14.6+ KB\n",
      "# App labels done\n",
      "## Handling events data for merging with app lables\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 457777 entries, 0 to 457776\n",
      "Data columns (total 2 columns):\n",
      "app_id      457777 non-null object\n",
      "category    457777 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 7.0+ MB\n",
      "## Merge\n",
      "#Expand to multiple rows\n",
      "# App labels done\n",
      "# App category part formed\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#   App labels\n",
    "##################\n",
    "\n",
    "\n",
    "print(\"### ----- PART 2 ----- ###\")\n",
    "\n",
    "print(\"# Read App labels\")\n",
    "app_labels = pd.read_csv(os.path.join(datadir,'app_labels.csv'))\n",
    "label_cat = pd.read_csv(os.path.join(datadir,'label_categories.csv'))\n",
    "app_labels.info()\n",
    "label_cat.info()\n",
    "label_cat=label_cat[['label_id','category']]\n",
    "\n",
    "app_labels=app_labels.merge(label_cat,on='label_id',how='left')\n",
    "app_labels.head(3)\n",
    "events.head(3)\n",
    "#app_labels = app_labels.loc[app_labels.smaller_cat != \"unknown_unknown\"]\n",
    "\n",
    "#app_labels = app_labels.groupby(\"app_id\")[\"category\"].apply(\n",
    "#    lambda x: \";\".join(set(\"app_cat:\" + str(s) for s in x)))\n",
    "app_labels = app_labels.groupby([\"app_id\",\"category\"]).agg('size').reset_index()\n",
    "app_labels = app_labels[['app_id','category']]\n",
    "print(\"# App labels done\")\n",
    "\n",
    "\n",
    "# Remove \"app_id:\" from column\n",
    "print(\"## Handling events data for merging with app lables\")\n",
    "events['app_id'] = events['app_id'].map(lambda x : x.lstrip('app_id:'))\n",
    "events['app_id'] = events['app_id'].astype(str)\n",
    "app_labels['app_id'] = app_labels['app_id'].astype(str)\n",
    "app_labels.info()\n",
    "\n",
    "print(\"## Merge\")\n",
    "\n",
    "events= pd.merge(events, app_labels, on = 'app_id',how='left').astype(str)\n",
    "#events['smaller_cat'].unique()\n",
    "\n",
    "# expand to multiple rows\n",
    "print(\"#Expand to multiple rows\")\n",
    "#events= pd.concat([pd.Series(row['device_id'], row['category'].split(';'))\n",
    "#                    for _, row in events.iterrows()]).reset_index()\n",
    "#events.columns = ['app_cat', 'device_id']\n",
    "#events.head(5)\n",
    "#print(events.info())\n",
    "\n",
    "events= events.groupby([\"device_id\",\"category\"]).agg('size').reset_index()\n",
    "events= events[['device_id','category']]\n",
    "events.head(10)\n",
    "print(\"# App labels done\")\n",
    "\n",
    "f5 = events[[\"device_id\", \"category\"]]    # app_id\n",
    "# Can % total share be included as well?\n",
    "print(\"# App category part formed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ----- PART 3 ----- ###\n",
      "# Read Phone Brand\n",
      "# Generate Train and Test\n",
      "### ----- PART 4 ----- ###\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#   Phone Brand\n",
    "##################\n",
    "print(\"### ----- PART 3 ----- ###\")\n",
    "\n",
    "print(\"# Read Phone Brand\")\n",
    "pbd = pd.read_csv(os.path.join(datadir,'phone_brand_device_model.csv'),\n",
    "                  dtype={'device_id': np.str})\n",
    "pbd.drop_duplicates('device_id', keep='first', inplace=True)\n",
    "\n",
    "##################\n",
    "#  Train and Test\n",
    "##################\n",
    "print(\"# Generate Train and Test\")\n",
    "\n",
    "train = pd.read_csv(os.path.join(datadir,'gender_age_train.csv'),\n",
    "                    dtype={'device_id': np.str})\n",
    "train.drop([\"age\", \"gender\"], axis=1, inplace=True)\n",
    "\n",
    "test = pd.read_csv(os.path.join(datadir,'gender_age_test.csv'),\n",
    "                   dtype={'device_id': np.str})\n",
    "test[\"group\"] = np.nan\n",
    "\n",
    "\n",
    "split_len = len(train)\n",
    "\n",
    "# Group Labels\n",
    "Y = train[\"group\"]\n",
    "lable_group = LabelEncoder()\n",
    "Y = lable_group.fit_transform(Y)\n",
    "device_id = test[\"device_id\"]\n",
    "\n",
    "# Concat\n",
    "Df = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "\n",
    "print(\"### ----- PART 4 ----- ###\")\n",
    "\n",
    "Df = pd.merge(Df, pbd, how=\"left\", on=\"device_id\")\n",
    "Df[\"phone_brand\"] = Df[\"phone_brand\"].apply(lambda x: \"phone_brand:\" + str(x))\n",
    "Df[\"device_model\"] = Df[\"device_model\"].apply(\n",
    "    lambda x: \"device_model:\" + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Concat all features\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6677856 entries, 0 to 6677855\n",
      "Data columns (total 2 columns):\n",
      "device_id    object\n",
      "feature      object\n",
      "dtypes: object(2)\n",
      "memory usage: 101.9+ MB\n",
      "# User-Item-Feature\n",
      "# Sparse matrix done\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "#  Concat Feature\n",
    "###################\n",
    "\n",
    "print(\"# Concat all features\")\n",
    "\n",
    "f1 = Df[[\"device_id\", \"phone_brand\"]]   # phone_brand\n",
    "f2 = Df[[\"device_id\", \"device_model\"]]  # device_model\n",
    "\n",
    "events = None\n",
    "Df = None\n",
    "\n",
    "f1.columns.values[1] = \"feature\"\n",
    "f2.columns.values[1] = \"feature\"\n",
    "f5.columns.values[1] = \"feature\"\n",
    "f3.columns.values[1] = \"feature\"\n",
    "\n",
    "FLS = pd.concat((f1, f2, f3, f5), axis=0, ignore_index=True)\n",
    "\n",
    "FLS.info()\n",
    "\n",
    "###################\n",
    "# User-Item Feature\n",
    "###################\n",
    "print(\"# User-Item-Feature\")\n",
    "\n",
    "device_ids = FLS[\"device_id\"].unique()\n",
    "feature_cs = FLS[\"feature\"].unique()\n",
    "\n",
    "data = np.ones(len(FLS))\n",
    "len(data)\n",
    "\n",
    "dec = LabelEncoder().fit(FLS[\"device_id\"])\n",
    "row = dec.transform(FLS[\"device_id\"])\n",
    "col = LabelEncoder().fit_transform(FLS[\"feature\"])\n",
    "sparse_matrix = sparse.csr_matrix(\n",
    "    (data, (row, col)), shape=(len(device_ids), len(feature_cs)))\n",
    "sparse_matrix.shape\n",
    "sys.getsizeof(sparse_matrix)\n",
    "\n",
    "sparse_matrix = sparse_matrix[:, sparse_matrix.getnnz(0) > 0]\n",
    "print(\"# Sparse matrix done\")\n",
    "\n",
    "del FLS\n",
    "del data\n",
    "f1 = [1]\n",
    "f5 = [1]\n",
    "f2 = [1]\n",
    "f3 = [1]\n",
    "\n",
    "events = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Split data\n",
      "# Feature Selection\n",
      "('# Num of Features: ', 21425)\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#      Data\n",
    "##################\n",
    "\n",
    "print(\"# Split data\")\n",
    "train_row = dec.transform(train[\"device_id\"])\n",
    "train_sp = sparse_matrix[train_row, :]\n",
    "\n",
    "test_row = dec.transform(test[\"device_id\"])\n",
    "test_sp = sparse_matrix[test_row, :]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_sp, Y, train_size=0.999, random_state=10)\n",
    "\n",
    "##################\n",
    "#   Feature Sel\n",
    "##################\n",
    "print(\"# Feature Selection\")\n",
    "#selector = SelectPercentile(f_classif, percentile=53)\n",
    "\n",
    "#selector.fit(X_train, y_train)\n",
    "#X_train.shape\n",
    "#X_train = selector.transform(X_train)\n",
    "#X_train.shape\n",
    "#X_val = selector.transform(X_val)\n",
    "#X_val.shape\n",
    "\n",
    "# Selection using chi-square\n",
    "# selector = SelectKBest(chi2, k=11155).fit(X_train, y_train)\n",
    "# X_train.shape\n",
    "# X_train = selector.transform(X_train)\n",
    "# X_train.shape\n",
    "# X_val = selector.transform(X_val)\n",
    "# X_val.shape\n",
    "\n",
    "print(\"# Num of Features: \", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python2.7/site-packages/keras/engine/training.py:1463: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26s - loss: 2.4395 - acc: 0.1309 - val_loss: 2.4228 - val_acc: 0.0933\n",
      "Epoch 2/18\n",
      "26s - loss: 2.3798 - acc: 0.1538 - val_loss: 2.3867 - val_acc: 0.0800\n",
      "Epoch 3/18\n",
      "26s - loss: 2.3435 - acc: 0.1661 - val_loss: 2.3622 - val_acc: 0.0933\n",
      "Epoch 4/18\n",
      "26s - loss: 2.3225 - acc: 0.1762 - val_loss: 2.3400 - val_acc: 0.0667\n",
      "Epoch 5/18\n",
      "26s - loss: 2.3071 - acc: 0.1800 - val_loss: 2.3202 - val_acc: 0.0667\n",
      "Epoch 6/18\n",
      "26s - loss: 2.2933 - acc: 0.1867 - val_loss: 2.3088 - val_acc: 0.0933\n",
      "Epoch 7/18\n",
      "26s - loss: 2.2849 - acc: 0.1875 - val_loss: 2.3112 - val_acc: 0.1067\n",
      "Epoch 8/18\n",
      "26s - loss: 2.2761 - acc: 0.1932 - val_loss: 2.3037 - val_acc: 0.0800\n",
      "Epoch 9/18\n",
      "26s - loss: 2.2668 - acc: 0.1983 - val_loss: 2.2981 - val_acc: 0.1067\n",
      "Epoch 10/18\n",
      "26s - loss: 2.2624 - acc: 0.1979 - val_loss: 2.3052 - val_acc: 0.1067\n",
      "Epoch 11/18\n",
      "26s - loss: 2.2537 - acc: 0.2029 - val_loss: 2.2916 - val_acc: 0.0800\n",
      "Epoch 12/18\n",
      "26s - loss: 2.2489 - acc: 0.2033 - val_loss: 2.2939 - val_acc: 0.1067\n",
      "Epoch 13/18\n",
      "26s - loss: 2.2459 - acc: 0.2041 - val_loss: 2.2933 - val_acc: 0.1200\n",
      "Epoch 14/18\n",
      "26s - loss: 2.2349 - acc: 0.2082 - val_loss: 2.3024 - val_acc: 0.1467\n",
      "Epoch 15/18\n",
      "26s - loss: 2.2301 - acc: 0.2103 - val_loss: 2.2812 - val_acc: 0.1600\n",
      "Epoch 16/18\n",
      "26s - loss: 2.2257 - acc: 0.2133 - val_loss: 2.2826 - val_acc: 0.1600\n",
      "Epoch 17/18\n",
      "26s - loss: 2.2192 - acc: 0.2141 - val_loss: 2.2788 - val_acc: 0.1200\n",
      "Epoch 18/18\n",
      "26s - loss: 2.2119 - acc: 0.2163 - val_loss: 2.2943 - val_acc: 0.1600\n",
      "logloss val 2.29432598909\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:24: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#  Build Model\n",
    "##################\n",
    "\n",
    "\n",
    "#act = keras.layers.advanced_activations.PReLU(init='zero', weights=None)\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(150, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(50, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "\n",
    "model = baseline_model()\n",
    "\n",
    "fit= model.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69984,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "scores_val = model.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val)))\n",
    "\n",
    "\n",
    "#print(\"# Final prediction\")\n",
    "#scores = model.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "#result = pd.DataFrame(scores , columns=lable_group.classes_)\n",
    "#result[\"device_id\"] = device_id\n",
    "#print(result.head(1))\n",
    "#result = result.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result.to_csv('keras_' + str(log_loss(y_val, scores_val)) + '.csv', index=True, index_label='device_id')\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "27s - loss: 2.4271 - acc: 0.1384 - val_loss: 2.4214 - val_acc: 0.0667\n",
      "Epoch 2/18\n",
      "27s - loss: 2.3521 - acc: 0.1631 - val_loss: 2.3485 - val_acc: 0.0667\n",
      "Epoch 3/18\n",
      "27s - loss: 2.3149 - acc: 0.1762 - val_loss: 2.3236 - val_acc: 0.0800\n",
      "Epoch 4/18\n",
      "27s - loss: 2.2955 - acc: 0.1844 - val_loss: 2.3007 - val_acc: 0.0800\n",
      "Epoch 5/18\n",
      "27s - loss: 2.2847 - acc: 0.1900 - val_loss: 2.2972 - val_acc: 0.1200\n",
      "Epoch 6/18\n",
      "27s - loss: 2.2703 - acc: 0.1950 - val_loss: 2.2851 - val_acc: 0.0800\n",
      "Epoch 7/18\n",
      "27s - loss: 2.2622 - acc: 0.1978 - val_loss: 2.2844 - val_acc: 0.1200\n",
      "Epoch 8/18\n",
      "27s - loss: 2.2578 - acc: 0.2014 - val_loss: 2.2887 - val_acc: 0.0933\n",
      "Epoch 9/18\n",
      "27s - loss: 2.2467 - acc: 0.2059 - val_loss: 2.2736 - val_acc: 0.1600\n",
      "Epoch 10/18\n",
      "27s - loss: 2.2403 - acc: 0.2064 - val_loss: 2.2706 - val_acc: 0.1867\n",
      "Epoch 11/18\n",
      "27s - loss: 2.2354 - acc: 0.2097 - val_loss: 2.2759 - val_acc: 0.1467\n",
      "Epoch 12/18\n",
      "27s - loss: 2.2294 - acc: 0.2116 - val_loss: 2.2725 - val_acc: 0.1600\n",
      "Epoch 13/18\n",
      "27s - loss: 2.2200 - acc: 0.2135 - val_loss: 2.2774 - val_acc: 0.1600\n",
      "Epoch 14/18\n",
      "27s - loss: 2.2152 - acc: 0.2148 - val_loss: 2.2770 - val_acc: 0.1467\n",
      "Epoch 15/18\n",
      "27s - loss: 2.2080 - acc: 0.2186 - val_loss: 2.2731 - val_acc: 0.1600\n",
      "Epoch 16/18\n",
      "27s - loss: 2.2013 - acc: 0.2226 - val_loss: 2.2650 - val_acc: 0.1867\n",
      "Epoch 17/18\n",
      "27s - loss: 2.1951 - acc: 0.2230 - val_loss: 2.2654 - val_acc: 0.1733\n",
      "Epoch 18/18\n",
      "27s - loss: 2.1872 - acc: 0.2261 - val_loss: 2.2669 - val_acc: 0.1867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:24: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "def second_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(155, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(50, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_2 = second_model()\n",
    "\n",
    "fit_2 = model_2.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69784,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "scores_val_2 = model_2.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "24s - loss: 2.4297 - acc: 0.1383 - val_loss: 2.4198 - val_acc: 0.0667\n",
      "Epoch 2/18\n",
      "24s - loss: 2.3601 - acc: 0.1620 - val_loss: 2.3775 - val_acc: 0.0800\n",
      "Epoch 3/18\n",
      "24s - loss: 2.3169 - acc: 0.1778 - val_loss: 2.3454 - val_acc: 0.0933\n",
      "Epoch 4/18\n",
      "24s - loss: 2.2998 - acc: 0.1833 - val_loss: 2.3260 - val_acc: 0.1333\n",
      "Epoch 5/18\n",
      "24s - loss: 2.2871 - acc: 0.1882 - val_loss: 2.3225 - val_acc: 0.0933\n",
      "Epoch 6/18\n",
      "24s - loss: 2.2727 - acc: 0.1940 - val_loss: 2.3194 - val_acc: 0.1067\n",
      "Epoch 7/18\n",
      "24s - loss: 2.2680 - acc: 0.1965 - val_loss: 2.2938 - val_acc: 0.1200\n",
      "Epoch 8/18\n",
      "18s - loss: 2.2561 - acc: 0.2016 - val_loss: 2.3034 - val_acc: 0.1467\n",
      "Epoch 9/18\n",
      "18s - loss: 2.2532 - acc: 0.2013 - val_loss: 2.2960 - val_acc: 0.1600\n",
      "Epoch 10/18\n",
      "18s - loss: 2.2388 - acc: 0.2057 - val_loss: 2.2899 - val_acc: 0.1733\n",
      "Epoch 11/18\n",
      "18s - loss: 2.2383 - acc: 0.2083 - val_loss: 2.2917 - val_acc: 0.1333\n",
      "Epoch 12/18\n",
      "18s - loss: 2.2297 - acc: 0.2127 - val_loss: 2.3073 - val_acc: 0.0933\n",
      "Epoch 13/18\n",
      "18s - loss: 2.2245 - acc: 0.2141 - val_loss: 2.2908 - val_acc: 0.1200\n",
      "Epoch 14/18\n",
      "18s - loss: 2.2154 - acc: 0.2151 - val_loss: 2.2857 - val_acc: 0.1600\n",
      "Epoch 15/18\n",
      "18s - loss: 2.2129 - acc: 0.2176 - val_loss: 2.2831 - val_acc: 0.1600\n",
      "Epoch 16/18\n",
      "18s - loss: 2.2036 - acc: 0.2218 - val_loss: 2.2822 - val_acc: 0.1867\n",
      "Epoch 17/18\n",
      "18s - loss: 2.2000 - acc: 0.2229 - val_loss: 2.2881 - val_acc: 0.1467\n",
      "Epoch 18/18\n",
      "18s - loss: 2.1920 - acc: 0.2256 - val_loss: 2.2899 - val_acc: 0.1600\n",
      "logloss val 2.28994542122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:24: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "def third_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(145, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(50, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_3 = third_model()\n",
    "\n",
    "fit_3 = model_3.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69884,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "scores_val_3 = model_3.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "26s - loss: 2.4287 - acc: 0.1397 - val_loss: 2.4274 - val_acc: 0.0667\n",
      "Epoch 2/18\n",
      "26s - loss: 2.3508 - acc: 0.1660 - val_loss: 2.3647 - val_acc: 0.0933\n",
      "Epoch 3/18\n",
      "26s - loss: 2.3132 - acc: 0.1782 - val_loss: 2.3385 - val_acc: 0.0800\n",
      "Epoch 4/18\n",
      "26s - loss: 2.2929 - acc: 0.1853 - val_loss: 2.3238 - val_acc: 0.0933\n",
      "Epoch 5/18\n",
      "26s - loss: 2.2809 - acc: 0.1910 - val_loss: 2.3257 - val_acc: 0.0667\n",
      "Epoch 6/18\n",
      "26s - loss: 2.2726 - acc: 0.1952 - val_loss: 2.3175 - val_acc: 0.0667\n",
      "Epoch 7/18\n",
      "26s - loss: 2.2639 - acc: 0.1956 - val_loss: 2.3141 - val_acc: 0.0933\n",
      "Epoch 8/18\n",
      "26s - loss: 2.2529 - acc: 0.2006 - val_loss: 2.2924 - val_acc: 0.1467\n",
      "Epoch 9/18\n",
      "26s - loss: 2.2485 - acc: 0.2025 - val_loss: 2.3019 - val_acc: 0.1067\n",
      "Epoch 10/18\n",
      "26s - loss: 2.2395 - acc: 0.2082 - val_loss: 2.2989 - val_acc: 0.1333\n",
      "Epoch 11/18\n",
      "26s - loss: 2.2357 - acc: 0.2069 - val_loss: 2.3004 - val_acc: 0.1067\n",
      "Epoch 12/18\n",
      "26s - loss: 2.2295 - acc: 0.2107 - val_loss: 2.3055 - val_acc: 0.1067\n",
      "Epoch 13/18\n",
      "26s - loss: 2.2199 - acc: 0.2153 - val_loss: 2.3150 - val_acc: 0.0933\n",
      "Epoch 14/18\n",
      "26s - loss: 2.2121 - acc: 0.2174 - val_loss: 2.3074 - val_acc: 0.0933\n",
      "Epoch 15/18\n",
      "26s - loss: 2.2104 - acc: 0.2179 - val_loss: 2.2943 - val_acc: 0.1600\n",
      "Epoch 16/18\n",
      "26s - loss: 2.1995 - acc: 0.2234 - val_loss: 2.3101 - val_acc: 0.0933\n",
      "Epoch 17/18\n",
      "26s - loss: 2.1949 - acc: 0.2246 - val_loss: 2.3069 - val_acc: 0.1067\n",
      "Epoch 18/18\n",
      "26s - loss: 2.1874 - acc: 0.2261 - val_loss: 2.3071 - val_acc: 0.1200\n",
      "logloss val 2.30709473689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:24: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "def forth_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(147, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(50, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_4 = forth_model()\n",
    "\n",
    "fit_4 = model_4.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69984,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "scores_val_4 = model_4.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val_4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "26s - loss: 2.4315 - acc: 0.1362 - val_loss: 2.4236 - val_acc: 0.0667\n",
      "Epoch 2/18\n",
      "26s - loss: 2.3562 - acc: 0.1631 - val_loss: 2.3699 - val_acc: 0.0800\n",
      "Epoch 3/18\n",
      "26s - loss: 2.3170 - acc: 0.1770 - val_loss: 2.3345 - val_acc: 0.0800\n",
      "Epoch 4/18\n",
      "26s - loss: 2.2994 - acc: 0.1855 - val_loss: 2.3408 - val_acc: 0.0400\n",
      "Epoch 5/18\n",
      "26s - loss: 2.2847 - acc: 0.1891 - val_loss: 2.3274 - val_acc: 0.0533\n",
      "Epoch 6/18\n",
      "26s - loss: 2.2760 - acc: 0.1934 - val_loss: 2.3113 - val_acc: 0.0533\n",
      "Epoch 7/18\n",
      "26s - loss: 2.2626 - acc: 0.1983 - val_loss: 2.3081 - val_acc: 0.0533\n",
      "Epoch 8/18\n",
      "26s - loss: 2.2589 - acc: 0.2002 - val_loss: 2.3138 - val_acc: 0.0800\n",
      "Epoch 9/18\n",
      "26s - loss: 2.2493 - acc: 0.2047 - val_loss: 2.3080 - val_acc: 0.1333\n",
      "Epoch 10/18\n",
      "26s - loss: 2.2442 - acc: 0.2072 - val_loss: 2.3105 - val_acc: 0.1333\n",
      "Epoch 11/18\n",
      "26s - loss: 2.2388 - acc: 0.2088 - val_loss: 2.3041 - val_acc: 0.1600\n",
      "Epoch 12/18\n",
      "26s - loss: 2.2287 - acc: 0.2088 - val_loss: 2.2971 - val_acc: 0.1867\n",
      "Epoch 13/18\n",
      "26s - loss: 2.2263 - acc: 0.2139 - val_loss: 2.2938 - val_acc: 0.1733\n",
      "Epoch 14/18\n",
      "26s - loss: 2.2177 - acc: 0.2146 - val_loss: 2.2988 - val_acc: 0.1600\n",
      "Epoch 15/18\n",
      "26s - loss: 2.2099 - acc: 0.2200 - val_loss: 2.3013 - val_acc: 0.1467\n",
      "Epoch 16/18\n",
      "26s - loss: 2.2044 - acc: 0.2211 - val_loss: 2.2968 - val_acc: 0.1467\n",
      "Epoch 17/18\n",
      "26s - loss: 2.1982 - acc: 0.2239 - val_loss: 2.3035 - val_acc: 0.1467\n",
      "Epoch 18/18\n",
      "26s - loss: 2.1918 - acc: 0.2274 - val_loss: 2.2947 - val_acc: 0.1733\n",
      "logloss val 2.29472557624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:24: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "def fifth_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(152, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.42))\n",
    "    model.add(Dense(52, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.18))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_5 = fifth_model()\n",
    "\n",
    "fit_5 = model_5.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69884,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "scores_val_5 = model_5.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val_5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "26s - loss: 2.4257 - acc: 0.1368 - val_loss: 2.4218 - val_acc: 0.0933\n",
      "Epoch 2/18\n",
      "26s - loss: 2.3514 - acc: 0.1656 - val_loss: 2.3680 - val_acc: 0.0667\n",
      "Epoch 3/18\n",
      "26s - loss: 2.3176 - acc: 0.1748 - val_loss: 2.3355 - val_acc: 0.0800\n",
      "Epoch 4/18\n",
      "26s - loss: 2.2992 - acc: 0.1812 - val_loss: 2.3074 - val_acc: 0.0933\n",
      "Epoch 5/18\n",
      "26s - loss: 2.2815 - acc: 0.1887 - val_loss: 2.2989 - val_acc: 0.1067\n",
      "Epoch 6/18\n",
      "26s - loss: 2.2747 - acc: 0.1927 - val_loss: 2.2978 - val_acc: 0.1200\n",
      "Epoch 7/18\n",
      "26s - loss: 2.2652 - acc: 0.1967 - val_loss: 2.2945 - val_acc: 0.1067\n",
      "Epoch 8/18\n",
      "26s - loss: 2.2563 - acc: 0.2010 - val_loss: 2.2986 - val_acc: 0.1200\n",
      "Epoch 9/18\n",
      "26s - loss: 2.2542 - acc: 0.2016 - val_loss: 2.3029 - val_acc: 0.1333\n",
      "Epoch 10/18\n",
      "26s - loss: 2.2394 - acc: 0.2073 - val_loss: 2.2894 - val_acc: 0.1600\n",
      "Epoch 11/18\n",
      "26s - loss: 2.2365 - acc: 0.2082 - val_loss: 2.2921 - val_acc: 0.1600\n",
      "Epoch 12/18\n",
      "26s - loss: 2.2315 - acc: 0.2100 - val_loss: 2.2965 - val_acc: 0.1333\n",
      "Epoch 13/18\n",
      "26s - loss: 2.2223 - acc: 0.2133 - val_loss: 2.2905 - val_acc: 0.1867\n",
      "Epoch 14/18\n",
      "26s - loss: 2.2190 - acc: 0.2139 - val_loss: 2.2855 - val_acc: 0.1867\n",
      "Epoch 15/18\n",
      "26s - loss: 2.2109 - acc: 0.2187 - val_loss: 2.2934 - val_acc: 0.1467\n",
      "Epoch 16/18\n",
      "26s - loss: 2.2031 - acc: 0.2231 - val_loss: 2.2778 - val_acc: 0.1867\n",
      "Epoch 17/18\n",
      "26s - loss: 2.1985 - acc: 0.2230 - val_loss: 2.2906 - val_acc: 0.1600\n",
      "Epoch 18/18\n",
      "26s - loss: 2.1881 - acc: 0.2257 - val_loss: 2.2718 - val_acc: 0.2000\n",
      "logloss val 2.29472557624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:24: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "def sixth_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(153, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.42))\n",
    "    model.add(Dense(51, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_6 = fifth_model()\n",
    "\n",
    "fit_6 = model_6.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69884,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "scores_val_6 = model_6.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val_5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "25s - loss: 2.4311 - acc: 0.1344 - val_loss: 2.4119 - val_acc: 0.0667\n",
      "Epoch 2/18\n",
      "25s - loss: 2.3571 - acc: 0.1616 - val_loss: 2.3568 - val_acc: 0.1200\n",
      "Epoch 3/18\n",
      "25s - loss: 2.3182 - acc: 0.1759 - val_loss: 2.3274 - val_acc: 0.1067\n",
      "Epoch 4/18\n",
      "25s - loss: 2.2968 - acc: 0.1836 - val_loss: 2.3094 - val_acc: 0.0933\n",
      "Epoch 5/18\n",
      "25s - loss: 2.2837 - acc: 0.1882 - val_loss: 2.3049 - val_acc: 0.0933\n",
      "Epoch 6/18\n",
      "25s - loss: 2.2736 - acc: 0.1935 - val_loss: 2.2914 - val_acc: 0.1467\n",
      "Epoch 7/18\n",
      "25s - loss: 2.2618 - acc: 0.1966 - val_loss: 2.3033 - val_acc: 0.1200\n",
      "Epoch 8/18\n",
      "25s - loss: 2.2570 - acc: 0.1996 - val_loss: 2.2930 - val_acc: 0.1467\n",
      "Epoch 9/18\n",
      "25s - loss: 2.2499 - acc: 0.2025 - val_loss: 2.2907 - val_acc: 0.1467\n",
      "Epoch 10/18\n",
      "25s - loss: 2.2406 - acc: 0.2081 - val_loss: 2.2766 - val_acc: 0.1467\n",
      "Epoch 11/18\n",
      "25s - loss: 2.2338 - acc: 0.2103 - val_loss: 2.2826 - val_acc: 0.1333\n",
      "Epoch 12/18\n",
      "25s - loss: 2.2319 - acc: 0.2089 - val_loss: 2.2897 - val_acc: 0.1600\n",
      "Epoch 13/18\n",
      "25s - loss: 2.2229 - acc: 0.2135 - val_loss: 2.2687 - val_acc: 0.1733\n",
      "Epoch 14/18\n",
      "25s - loss: 2.2140 - acc: 0.2174 - val_loss: 2.2842 - val_acc: 0.1733\n",
      "Epoch 15/18\n",
      "25s - loss: 2.2101 - acc: 0.2182 - val_loss: 2.2813 - val_acc: 0.1467\n",
      "Epoch 16/18\n",
      "25s - loss: 2.2054 - acc: 0.2210 - val_loss: 2.2795 - val_acc: 0.1600\n",
      "Epoch 17/18\n",
      "25s - loss: 2.1978 - acc: 0.2231 - val_loss: 2.2656 - val_acc: 0.1867\n",
      "Epoch 18/18\n",
      "25s - loss: 2.1898 - acc: 0.2267 - val_loss: 2.2698 - val_acc: 0.1733\n",
      "logloss val 2.26979568958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:24: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "def seventh_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(144, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(50, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_7 = seventh_model()\n",
    "\n",
    "fit_7 = model_7.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69884,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "scores_val_7 = model_7.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val_7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "25s - loss: 2.4328 - acc: 0.1346 - val_loss: 2.4146 - val_acc: 0.0667\n",
      "Epoch 2/18\n",
      "25s - loss: 2.3592 - acc: 0.1630 - val_loss: 2.3653 - val_acc: 0.0933\n",
      "Epoch 3/18\n",
      "25s - loss: 2.3188 - acc: 0.1750 - val_loss: 2.3337 - val_acc: 0.0800\n",
      "Epoch 4/18\n",
      "25s - loss: 2.2979 - acc: 0.1845 - val_loss: 2.3126 - val_acc: 0.0933\n",
      "Epoch 5/18\n",
      "19s - loss: 2.2868 - acc: 0.1868 - val_loss: 2.3064 - val_acc: 0.0933\n",
      "Epoch 6/18\n",
      "18s - loss: 2.2734 - acc: 0.1931 - val_loss: 2.3121 - val_acc: 0.1067\n",
      "Epoch 7/18\n",
      "18s - loss: 2.2635 - acc: 0.1978 - val_loss: 2.3029 - val_acc: 0.1333\n",
      "Epoch 8/18\n",
      "18s - loss: 2.2584 - acc: 0.1981 - val_loss: 2.3103 - val_acc: 0.1067\n",
      "Epoch 9/18\n",
      "18s - loss: 2.2501 - acc: 0.2032 - val_loss: 2.3038 - val_acc: 0.1200\n",
      "Epoch 10/18\n",
      "18s - loss: 2.2422 - acc: 0.2045 - val_loss: 2.2970 - val_acc: 0.1467\n",
      "Epoch 11/18\n",
      "18s - loss: 2.2385 - acc: 0.2096 - val_loss: 2.3014 - val_acc: 0.1333\n",
      "Epoch 12/18\n",
      "18s - loss: 2.2289 - acc: 0.2108 - val_loss: 2.2996 - val_acc: 0.1600\n",
      "Epoch 13/18\n",
      "18s - loss: 2.2221 - acc: 0.2125 - val_loss: 2.2934 - val_acc: 0.1733\n",
      "Epoch 14/18\n",
      "18s - loss: 2.2141 - acc: 0.2168 - val_loss: 2.2918 - val_acc: 0.1733\n",
      "Epoch 15/18\n",
      "18s - loss: 2.2083 - acc: 0.2217 - val_loss: 2.3010 - val_acc: 0.1733\n",
      "Epoch 16/18\n",
      "18s - loss: 2.2049 - acc: 0.2189 - val_loss: 2.2936 - val_acc: 0.1733\n",
      "Epoch 17/18\n",
      "18s - loss: 2.1951 - acc: 0.2241 - val_loss: 2.3005 - val_acc: 0.2133\n",
      "Epoch 18/18\n",
      "18s - loss: 2.1886 - acc: 0.2264 - val_loss: 2.2963 - val_acc: 0.2000\n",
      "logloss val 2.29634685993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:24: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "def eight_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(145, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(51, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_8 = eight_model()\n",
    "\n",
    "fit_8 = model_8.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69784,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "scores_val_8 = model_8.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val_8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "19s - loss: 2.4330 - acc: 0.1360 - val_loss: 2.4163 - val_acc: 0.0400\n",
      "Epoch 2/18\n",
      "19s - loss: 2.3586 - acc: 0.1622 - val_loss: 2.3704 - val_acc: 0.0800\n",
      "Epoch 3/18\n",
      "19s - loss: 2.3165 - acc: 0.1749 - val_loss: 2.3180 - val_acc: 0.1067\n",
      "Epoch 4/18\n",
      "19s - loss: 2.2984 - acc: 0.1828 - val_loss: 2.3054 - val_acc: 0.1200\n",
      "Epoch 5/18\n",
      "20s - loss: 2.2864 - acc: 0.1907 - val_loss: 2.3051 - val_acc: 0.1067\n",
      "Epoch 6/18\n",
      "20s - loss: 2.2721 - acc: 0.1927 - val_loss: 2.2936 - val_acc: 0.1067\n",
      "Epoch 7/18\n",
      "20s - loss: 2.2653 - acc: 0.1982 - val_loss: 2.3034 - val_acc: 0.1333\n",
      "Epoch 8/18\n",
      "20s - loss: 2.2557 - acc: 0.2013 - val_loss: 2.2867 - val_acc: 0.1333\n",
      "Epoch 9/18\n",
      "20s - loss: 2.2508 - acc: 0.2024 - val_loss: 2.3017 - val_acc: 0.1200\n",
      "Epoch 10/18\n",
      "20s - loss: 2.2422 - acc: 0.2075 - val_loss: 2.2989 - val_acc: 0.1467\n",
      "Epoch 11/18\n",
      "20s - loss: 2.2360 - acc: 0.2077 - val_loss: 2.2885 - val_acc: 0.1467\n",
      "Epoch 12/18\n",
      "20s - loss: 2.2290 - acc: 0.2134 - val_loss: 2.2892 - val_acc: 0.1200\n",
      "Epoch 13/18\n",
      "20s - loss: 2.2226 - acc: 0.2138 - val_loss: 2.2795 - val_acc: 0.1600\n",
      "Epoch 14/18\n",
      "20s - loss: 2.2153 - acc: 0.2186 - val_loss: 2.2859 - val_acc: 0.1733\n",
      "Epoch 15/18\n",
      "20s - loss: 2.2092 - acc: 0.2172 - val_loss: 2.2941 - val_acc: 0.1600\n",
      "Epoch 16/18\n",
      "20s - loss: 2.2029 - acc: 0.2215 - val_loss: 2.2879 - val_acc: 0.1467\n",
      "Epoch 17/18\n",
      "20s - loss: 2.1958 - acc: 0.2242 - val_loss: 2.2802 - val_acc: 0.2133\n",
      "Epoch 18/18\n",
      "20s - loss: 2.1869 - acc: 0.2267 - val_loss: 2.2771 - val_acc: 0.1600\n",
      "logloss val 2.27709468762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:24: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "def ninth_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(156, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(51, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_9 = ninth_model()\n",
    "\n",
    "fit_9 = model_9.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69884,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "scores_val_9 = model_9.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val_9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Final prediction\n",
      "       F23-    F24-26    F27-28    F29-32    F33-42      F43+      M22-  \\\n",
      "0  0.000138  0.000508  0.000948  0.003894  0.013173  0.020745  0.002387   \n",
      "\n",
      "     M23-26    M27-28    M29-31    M32-38      M39+            device_id  \n",
      "0  0.033445  0.044551  0.133506  0.369774  0.376932  1002079943728939269  \n"
     ]
    }
   ],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_3 = model_3.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_3 = pd.DataFrame(scores_3 , columns=lable_group.classes_)\n",
    "result_3[\"device_id\"] = device_id\n",
    "print(result_3.head(1))\n",
    "result_3 = result_3.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result_3.to_csv('keras_' + str(log_loss(y_val, scores_val_3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Final prediction\n",
      "       F23-    F24-26    F27-28    F29-32    F33-42      F43+      M22-  \\\n",
      "0  0.000484  0.001221  0.002257  0.007976  0.034026  0.057715  0.003035   \n",
      "\n",
      "     M23-26    M27-28    M29-31    M32-38      M39+            device_id  \n",
      "0  0.015861  0.031093  0.083018  0.265599  0.497715  1002079943728939269  \n"
     ]
    }
   ],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_2 = model_2.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_2 = pd.DataFrame(scores_2 , columns=lable_group.classes_)\n",
    "result_2[\"device_id\"] = device_id\n",
    "print(result_2.head(1))\n",
    "result_2 = result_2.set_index(\"device_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Final prediction\n",
      "       F23-   F24-26    F27-28    F29-32    F33-42     F43+      M22-  \\\n",
      "0  0.000035  0.00015  0.000347  0.002641  0.017033  0.02544  0.000846   \n",
      "\n",
      "     M23-26    M27-28   M29-31   M32-38     M39+            device_id  \n",
      "0  0.013455  0.022123  0.09382  0.31628  0.50783  1002079943728939269  \n"
     ]
    }
   ],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores = model.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result = pd.DataFrame(scores , columns=lable_group.classes_)\n",
    "result[\"device_id\"] = device_id\n",
    "print(result.head(1))\n",
    "result = result.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('keras_' + str(log_loss(y_val, scores_val)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Final prediction\n",
      "       F23-    F24-26   F27-28    F29-32    F33-42      F43+      M22-  \\\n",
      "0  0.000174  0.000609  0.00185  0.005094  0.024427  0.062238  0.002163   \n",
      "\n",
      "     M23-26    M27-28   M29-31   M32-38      M39+            device_id  \n",
      "0  0.018281  0.035111  0.09415  0.28475  0.471152  1002079943728939269  \n"
     ]
    }
   ],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_4 = model_4.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_4 = pd.DataFrame(scores_4 , columns=lable_group.classes_)\n",
    "result_4[\"device_id\"] = device_id\n",
    "print(result_4.head(1))\n",
    "result_4 = result_4.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result_3.to_csv('keras_' + str(log_loss(y_val, scores_val_3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Final prediction\n",
      "       F23-    F24-26    F27-28    F29-32    F33-42      F43+      M22-  \\\n",
      "0  0.000274  0.000913  0.002163  0.006315  0.028079  0.050359  0.003889   \n",
      "\n",
      "     M23-26    M27-28    M29-31    M32-38      M39+            device_id  \n",
      "0  0.029594  0.039524  0.115268  0.260335  0.463286  1002079943728939269  \n"
     ]
    }
   ],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_5 = model_5.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_5 = pd.DataFrame(scores_5 , columns=lable_group.classes_)\n",
    "result_5[\"device_id\"] = device_id\n",
    "print(result_5.head(1))\n",
    "result_5 = result_5.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result_3.to_csv('keras_' + str(log_loss(y_val, scores_val_3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Final prediction\n",
      "      F23-    F24-26   F27-28    F29-32    F33-42     F43+      M22-  \\\n",
      "0  0.00015  0.000854  0.00177  0.007022  0.029162  0.06107  0.001486   \n",
      "\n",
      "     M23-26    M27-28    M29-31    M32-38      M39+            device_id  \n",
      "0  0.016423  0.031027  0.093113  0.247202  0.510721  1002079943728939269  \n"
     ]
    }
   ],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_6 = model_6.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_6 = pd.DataFrame(scores_6 , columns=lable_group.classes_)\n",
    "result_6[\"device_id\"] = device_id\n",
    "print(result_6.head(1))\n",
    "result_6 = result_6.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result_3.to_csv('keras_' + str(log_loss(y_val, scores_val_3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Final prediction\n",
      "       F23-    F24-26    F27-28    F29-32    F33-42      F43+      M22-  \\\n",
      "0  0.000073  0.000406  0.001144  0.003903  0.021798  0.041399  0.000882   \n",
      "\n",
      "     M23-26    M27-28    M29-31    M32-38      M39+            device_id  \n",
      "0  0.017247  0.029377  0.091626  0.240454  0.551691  1002079943728939269  \n"
     ]
    }
   ],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_7 = model_7.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_7 = pd.DataFrame(scores_7 , columns=lable_group.classes_)\n",
    "result_7[\"device_id\"] = device_id\n",
    "print(result_7.head(1))\n",
    "result_7 = result_7.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result_3.to_csv('keras_' + str(log_loss(y_val, scores_val_3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Final prediction\n",
      "       F23-    F24-26    F27-28    F29-32    F33-42      F43+      M22-  \\\n",
      "0  0.000344  0.001679  0.003131  0.008121  0.027526  0.054043  0.004425   \n",
      "\n",
      "     M23-26    M27-28   M29-31   M32-38      M39+            device_id  \n",
      "0  0.035263  0.057595  0.12862  0.26707  0.412184  1002079943728939269  \n"
     ]
    }
   ],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_8 = model_8.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_8 = pd.DataFrame(scores_8 , columns=lable_group.classes_)\n",
    "result_8[\"device_id\"] = device_id\n",
    "print(result_8.head(1))\n",
    "result_8 = result_8.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result_3.to_csv('keras_' + str(log_loss(y_val, scores_val_3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Final prediction\n",
      "       F23-    F24-26    F27-28    F29-32    F33-42      F43+      M22-  \\\n",
      "0  0.000141  0.000402  0.000939  0.004165  0.025532  0.049513  0.001885   \n",
      "\n",
      "     M23-26    M27-28    M29-31    M32-38      M39+            device_id  \n",
      "0  0.013404  0.024059  0.069278  0.205052  0.605631  1002079943728939269  \n"
     ]
    }
   ],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_9 = model_9.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_9 = pd.DataFrame(scores_9 , columns=lable_group.classes_)\n",
    "result_9[\"device_id\"] = device_id\n",
    "print(result_9.head(1))\n",
    "result_9 = result_9.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result_3.to_csv('keras_' + str(log_loss(y_val, scores_val_3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tot = result + result_2 + result_3 + result_4 + result_5 + result_6\n",
    "#tot = result_3 + result_5 + result_6 \n",
    "#tot = result + result_2 + result_3 + result_4 + result_5 + result_6 + result_7\n",
    "#tot = result + result_2 + result_3 + result_4 + result_5 + result_6 + result_7 + result_8 + result_9\n",
    "tot = result + result_2 + result_3 + result_5 + result_6 + result_7 + result_8 + result_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tot = tot/6\n",
    "#tot = tot/3\n",
    "#tot = tot/7\n",
    "tot = tot/8\n",
    "#tot = tot/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tot[tot<0.001]=0.001 # 1,2,3,4,5,5 models + this -> 2.23982\n",
    "tot[tot>0.999]=0.999 # 1,2,3,4,5,5 models + this -> 2.23982"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tot.to_csv('keras_' + str(log_loss(y_val, (scores_val + scores_val_2 + scores_val_3 + scores_val_4 + scores_val_5 + scores_val_6)/6)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot.to_csv('keras_' + str(log_loss(y_val, (scores_val_6 + scores_val_3 + scores_val_5)/3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot.to_csv('keras_' + str(log_loss(y_val, (scores_val + scores_val_2 + scores_val_3 + scores_val_5 + scores_val_6 + scores_val_7 + scores_val_8 + scores_val_9)/8)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tot.to_csv('keras_' + str(log_loss(y_val, (scores_val + scores_val_2 + scores_val_3 + scores_val_4 + scores_val_5 + scores_val_6 + scores_val_7 + scores_val_8 + scores_val_9)/9)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
