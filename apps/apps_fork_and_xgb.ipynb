{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize libraries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Bag of apps categories\n",
    "# Bag of labels categories\n",
    "# Include phone brand and model device\n",
    "\n",
    "print(\"Initialize libraries\")\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics as skmetrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import ensemble\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import gc\n",
    "from scipy import sparse\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, chi2, SelectKBest\n",
    "from sklearn import ensemble\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------- Write functions ----------------------------------------\n",
    "\n",
    "def rstr(df): return df.dtypes, df.head(3) ,df.apply(lambda x: [x.unique()]), df.apply(lambda x: [len(x.unique())]),df.shape\n",
    "\n",
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    #chenglong code for fiting from generator (https://www.kaggle.com/c/talkingdata-mobile-user-demographics/forums/t/22567/neural-network-for-sparse-matrices)\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "\n",
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X[batch_index, :].toarray()\n",
    "        counter += 1\n",
    "        yield X_batch\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ----- PART 1 ----- ###\n",
      "# Read app events\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32473067 entries, 0 to 32473066\n",
      "Data columns (total 4 columns):\n",
      "event_id        int64\n",
      "app_id          int64\n",
      "is_installed    int64\n",
      "is_active       int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 991.0 MB\n",
      "# Read Events\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1488096 entries, 1 to 3252947\n",
      "Data columns (total 2 columns):\n",
      "device_id    1488096 non-null object\n",
      "app_id       1488096 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 34.1+ MB\n",
      "#Part1 formed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#------------------------------------------------ Read data from source files ------------------------------------\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "datadir = ''\n",
    "\n",
    "print(\"### ----- PART 1 ----- ###\")\n",
    "\n",
    "# Data - Events data\n",
    "# Bag of apps\n",
    "print(\"# Read app events\")\n",
    "app_events = pd.read_csv(os.path.join(datadir,'app_events.csv'), dtype={'device_id' : np.str})\n",
    "app_events.head(5)\n",
    "app_events.info()\n",
    "#print(rstr(app_events))\n",
    "\n",
    "# remove duplicates(app_id)\n",
    "app_events= app_events.groupby(\"event_id\")[\"app_id\"].apply(\n",
    "    lambda x: \" \".join(set(\"app_id:\" + str(s) for s in x)))\n",
    "app_events.head(5)\n",
    "\n",
    "print(\"# Read Events\")\n",
    "events = pd.read_csv(os.path.join(datadir,'events.csv'), dtype={'device_id': np.str})\n",
    "events.head(5)\n",
    "events[\"app_id\"] = events[\"event_id\"].map(app_events)\n",
    "events = events.dropna()\n",
    "del app_events\n",
    "\n",
    "events = events[[\"device_id\", \"app_id\"]]\n",
    "events.info()\n",
    "# 1Gb reduced to 34 Mb\n",
    "\n",
    "# remove duplicates(app_id)\n",
    "events.loc[:,\"device_id\"].value_counts(ascending=True)\n",
    "\n",
    "events = events.groupby(\"device_id\")[\"app_id\"].apply(\n",
    "    lambda x: \" \".join(set(str(\" \".join(str(s) for s in x)).split(\" \"))))\n",
    "events = events.reset_index(name=\"app_id\")\n",
    "\n",
    "# expand to multiple rows\n",
    "events = pd.concat([pd.Series(row['device_id'], row['app_id'].split(' '))\n",
    "                    for _, row in events.iterrows()]).reset_index()\n",
    "events.columns = ['app_id', 'device_id']\n",
    "events.head(5)\n",
    "f3 = events[[\"device_id\", \"app_id\"]]    # app_id\n",
    "\n",
    "print(\"#Part1 formed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ----- PART 2 ----- ###\n",
      "# Read App labels\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 459943 entries, 0 to 459942\n",
      "Data columns (total 2 columns):\n",
      "app_id      459943 non-null int64\n",
      "label_id    459943 non-null int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 7.0 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 930 entries, 0 to 929\n",
      "Data columns (total 2 columns):\n",
      "label_id    930 non-null int64\n",
      "category    927 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 14.6+ KB\n",
      "# App labels done\n",
      "## Handling events data for merging with app lables\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 457777 entries, 0 to 457776\n",
      "Data columns (total 2 columns):\n",
      "app_id      457777 non-null object\n",
      "category    457777 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 7.0+ MB\n",
      "## Merge\n",
      "#Expand to multiple rows\n",
      "# App labels done\n",
      "# App category part formed\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#   App labels\n",
    "##################\n",
    "\n",
    "\n",
    "print(\"### ----- PART 2 ----- ###\")\n",
    "\n",
    "print(\"# Read App labels\")\n",
    "app_labels = pd.read_csv(os.path.join(datadir,'app_labels.csv'))\n",
    "label_cat = pd.read_csv(os.path.join(datadir,'label_categories.csv'))\n",
    "app_labels.info()\n",
    "label_cat.info()\n",
    "label_cat=label_cat[['label_id','category']]\n",
    "\n",
    "app_labels=app_labels.merge(label_cat,on='label_id',how='left')\n",
    "app_labels.head(3)\n",
    "events.head(3)\n",
    "#app_labels = app_labels.loc[app_labels.smaller_cat != \"unknown_unknown\"]\n",
    "\n",
    "#app_labels = app_labels.groupby(\"app_id\")[\"category\"].apply(\n",
    "#    lambda x: \";\".join(set(\"app_cat:\" + str(s) for s in x)))\n",
    "app_labels = app_labels.groupby([\"app_id\",\"category\"]).agg('size').reset_index()\n",
    "app_labels = app_labels[['app_id','category']]\n",
    "print(\"# App labels done\")\n",
    "\n",
    "\n",
    "# Remove \"app_id:\" from column\n",
    "print(\"## Handling events data for merging with app lables\")\n",
    "events['app_id'] = events['app_id'].map(lambda x : x.lstrip('app_id:'))\n",
    "events['app_id'] = events['app_id'].astype(str)\n",
    "app_labels['app_id'] = app_labels['app_id'].astype(str)\n",
    "app_labels.info()\n",
    "\n",
    "print(\"## Merge\")\n",
    "\n",
    "events= pd.merge(events, app_labels, on = 'app_id',how='left').astype(str)\n",
    "#events['smaller_cat'].unique()\n",
    "\n",
    "# expand to multiple rows\n",
    "print(\"#Expand to multiple rows\")\n",
    "#events= pd.concat([pd.Series(row['device_id'], row['category'].split(';'))\n",
    "#                    for _, row in events.iterrows()]).reset_index()\n",
    "#events.columns = ['app_cat', 'device_id']\n",
    "#events.head(5)\n",
    "#print(events.info())\n",
    "\n",
    "events= events.groupby([\"device_id\",\"category\"]).agg('size').reset_index()\n",
    "events= events[['device_id','category']]\n",
    "events.head(10)\n",
    "print(\"# App labels done\")\n",
    "\n",
    "f5 = events[[\"device_id\", \"category\"]]    # app_id\n",
    "# Can % total share be included as well?\n",
    "print(\"# App category part formed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ----- PART 3 ----- ###\n",
      "# Read Phone Brand\n",
      "# Generate Train and Test\n",
      "### ----- PART 4 ----- ###\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#   Phone Brand\n",
    "##################\n",
    "print(\"### ----- PART 3 ----- ###\")\n",
    "\n",
    "print(\"# Read Phone Brand\")\n",
    "pbd = pd.read_csv(os.path.join(datadir,'phone_brand_device_model.csv'),\n",
    "                  dtype={'device_id': np.str})\n",
    "pbd.drop_duplicates('device_id', keep='first', inplace=True)\n",
    "\n",
    "##################\n",
    "#  Train and Test\n",
    "##################\n",
    "print(\"# Generate Train and Test\")\n",
    "\n",
    "train = pd.read_csv(os.path.join(datadir,'gender_age_train.csv'),\n",
    "                    dtype={'device_id': np.str})\n",
    "train.drop([\"age\", \"gender\"], axis=1, inplace=True)\n",
    "\n",
    "test = pd.read_csv(os.path.join(datadir,'gender_age_test.csv'),\n",
    "                   dtype={'device_id': np.str})\n",
    "test[\"group\"] = np.nan\n",
    "\n",
    "\n",
    "split_len = len(train)\n",
    "\n",
    "# Group Labels\n",
    "Y = train[\"group\"]\n",
    "lable_group = LabelEncoder()\n",
    "Y = lable_group.fit_transform(Y)\n",
    "device_id = test[\"device_id\"]\n",
    "\n",
    "# Concat\n",
    "Df = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "\n",
    "print(\"### ----- PART 4 ----- ###\")\n",
    "\n",
    "Df = pd.merge(Df, pbd, how=\"left\", on=\"device_id\")\n",
    "Df[\"phone_brand\"] = Df[\"phone_brand\"].apply(lambda x: \"phone_brand:\" + str(x))\n",
    "Df[\"device_model\"] = Df[\"device_model\"].apply(\n",
    "    lambda x: \"device_model:\" + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Concat all features\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6677856 entries, 0 to 6677855\n",
      "Data columns (total 2 columns):\n",
      "device_id    object\n",
      "feature      object\n",
      "dtypes: object(2)\n",
      "memory usage: 101.9+ MB\n",
      "# User-Item-Feature\n",
      "# Sparse matrix done\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "#  Concat Feature\n",
    "###################\n",
    "\n",
    "print(\"# Concat all features\")\n",
    "\n",
    "f1 = Df[[\"device_id\", \"phone_brand\"]]   # phone_brand\n",
    "f2 = Df[[\"device_id\", \"device_model\"]]  # device_model\n",
    "\n",
    "events = None\n",
    "Df = None\n",
    "\n",
    "f1.columns.values[1] = \"feature\"\n",
    "f2.columns.values[1] = \"feature\"\n",
    "f5.columns.values[1] = \"feature\"\n",
    "f3.columns.values[1] = \"feature\"\n",
    "\n",
    "FLS = pd.concat((f1, f2, f3, f5), axis=0, ignore_index=True)\n",
    "\n",
    "FLS.info()\n",
    "\n",
    "###################\n",
    "# User-Item Feature\n",
    "###################\n",
    "print(\"# User-Item-Feature\")\n",
    "\n",
    "device_ids = FLS[\"device_id\"].unique()\n",
    "feature_cs = FLS[\"feature\"].unique()\n",
    "\n",
    "data = np.ones(len(FLS))\n",
    "len(data)\n",
    "\n",
    "dec = LabelEncoder().fit(FLS[\"device_id\"])\n",
    "row = dec.transform(FLS[\"device_id\"])\n",
    "col = LabelEncoder().fit_transform(FLS[\"feature\"])\n",
    "sparse_matrix = sparse.csr_matrix(\n",
    "    (data, (row, col)), shape=(len(device_ids), len(feature_cs)))\n",
    "sparse_matrix.shape\n",
    "sys.getsizeof(sparse_matrix)\n",
    "\n",
    "sparse_matrix = sparse_matrix[:, sparse_matrix.getnnz(0) > 0]\n",
    "print(\"# Sparse matrix done\")\n",
    "\n",
    "del FLS\n",
    "del data\n",
    "f1 = [1]\n",
    "f5 = [1]\n",
    "f2 = [1]\n",
    "f3 = [1]\n",
    "\n",
    "events = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Split data\n",
      "# Feature Selection\n",
      "('# Num of Features: ', 21425)\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#      Data\n",
    "##################\n",
    "\n",
    "print(\"# Split data\")\n",
    "train_row = dec.transform(train[\"device_id\"])\n",
    "train_sp = sparse_matrix[train_row, :]\n",
    "\n",
    "test_row = dec.transform(test[\"device_id\"])\n",
    "test_sp = sparse_matrix[test_row, :]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_sp, Y, train_size=0.999, random_state=10)\n",
    "\n",
    "##################\n",
    "#   Feature Sel\n",
    "##################\n",
    "print(\"# Feature Selection\")\n",
    "#selector = SelectPercentile(f_classif, percentile=53)\n",
    "\n",
    "#selector.fit(X_train, y_train)\n",
    "#X_train.shape\n",
    "#X_train = selector.transform(X_train)\n",
    "#X_train.shape\n",
    "#X_val = selector.transform(X_val)\n",
    "#X_val.shape\n",
    "\n",
    "# Selection using chi-square\n",
    "# selector = SelectKBest(chi2, k=11155).fit(X_train, y_train)\n",
    "# X_train.shape\n",
    "# X_train = selector.transform(X_train)\n",
    "# X_train.shape\n",
    "# X_val = selector.transform(X_val)\n",
    "# X_val.shape\n",
    "\n",
    "print(\"# Num of Features: \", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import time\n",
    "\n",
    "def run_xgb(train, test, target, eta=0.5, random_state=0):\n",
    "    #eta = 0.1\n",
    "    max_depth = 5\n",
    "    subsample = 0.7\n",
    "    colsample_bytree = 0.7\n",
    "    start_time = time.time()\n",
    "\n",
    "    print('XGBoost params. ETA: {}, MAX_DEPTH: {}, SUBSAMPLE: {}, COLSAMPLE_BY_TREE: {}'.format(eta, max_depth, subsample, colsample_bytree))\n",
    "    params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"num_class\": 12,\n",
    "        \"booster\" : \"gbtree\",\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"eta\": eta,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"subsample\": subsample,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"silent\": 1,\n",
    "        \"seed\": random_state,\n",
    "    }\n",
    "    num_boost_round = 2000\n",
    "    early_stopping_rounds = 100\n",
    "    test_size = 0.3\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(train, target, test_size=test_size, random_state=random_state)\n",
    "    # TODO change split \n",
    "    print('Length train:', X_train.shape[0])\n",
    "    print('Length valid:', X_valid.shape[0])\n",
    "    #y_train = X_train[target]\n",
    "    #y_valid = X_valid[target]\n",
    "    # TODO delete upper code\n",
    "    dtrain = xgb.DMatrix(X_train, y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, y_valid)\n",
    "    # sparse matrix ???\n",
    "\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "    gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n",
    "\n",
    "    print(\"Validating...\")\n",
    "    check = gbm.predict(xgb.DMatrix(X_valid), ntree_limit=gbm.best_iteration)\n",
    "    score = log_loss(y_valid.tolist(), check)\n",
    "\n",
    "    print(\"Predict test set...\")\n",
    "    test_prediction = gbm.predict(xgb.DMatrix(test), ntree_limit=gbm.best_iteration)\n",
    "\n",
    "    print('Training time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "    return test_prediction.tolist(), score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "last = pd.read_csv('keras_2.26110777855.csv')\n",
    "last['device_id'] = last.device_id.astype(str)\n",
    "last = last.set_index(\"device_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python2.7/site-packages/keras/engine/training.py:1463: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17s - loss: 2.4295 - acc: 0.1338 - val_loss: 2.4061 - val_acc: 0.0667\n",
      "Epoch 2/18\n",
      "17s - loss: 2.3543 - acc: 0.1638 - val_loss: 2.3685 - val_acc: 0.0667\n",
      "Epoch 3/18\n",
      "17s - loss: 2.3158 - acc: 0.1770 - val_loss: 2.3241 - val_acc: 0.1067\n",
      "Epoch 4/18\n",
      "17s - loss: 2.2987 - acc: 0.1824 - val_loss: 2.3146 - val_acc: 0.1067\n",
      "Epoch 5/18\n",
      "17s - loss: 2.2817 - acc: 0.1881 - val_loss: 2.2957 - val_acc: 0.0667\n",
      "Epoch 6/18\n",
      "17s - loss: 2.2738 - acc: 0.1926 - val_loss: 2.3014 - val_acc: 0.0667\n",
      "Epoch 7/18\n",
      "17s - loss: 2.2638 - acc: 0.1965 - val_loss: 2.2922 - val_acc: 0.0933\n",
      "Epoch 8/18\n",
      "17s - loss: 2.2590 - acc: 0.2012 - val_loss: 2.2901 - val_acc: 0.1067\n",
      "Epoch 9/18\n",
      "17s - loss: 2.2503 - acc: 0.2052 - val_loss: 2.2771 - val_acc: 0.1200\n",
      "Epoch 10/18\n",
      "17s - loss: 2.2382 - acc: 0.2064 - val_loss: 2.2794 - val_acc: 0.0800\n",
      "Epoch 11/18\n",
      "17s - loss: 2.2383 - acc: 0.2075 - val_loss: 2.2852 - val_acc: 0.1333\n",
      "Epoch 12/18\n",
      "17s - loss: 2.2294 - acc: 0.2084 - val_loss: 2.2758 - val_acc: 0.1333\n",
      "Epoch 13/18\n",
      "17s - loss: 2.2256 - acc: 0.2140 - val_loss: 2.2771 - val_acc: 0.1333\n",
      "Epoch 14/18\n",
      "17s - loss: 2.2158 - acc: 0.2163 - val_loss: 2.2809 - val_acc: 0.1600\n",
      "Epoch 15/18\n",
      "18s - loss: 2.2109 - acc: 0.2171 - val_loss: 2.2753 - val_acc: 0.1333\n",
      "Epoch 16/18\n",
      "23s - loss: 2.2051 - acc: 0.2190 - val_loss: 2.2883 - val_acc: 0.1200\n",
      "Epoch 17/18\n",
      "24s - loss: 2.2001 - acc: 0.2226 - val_loss: 2.2822 - val_acc: 0.1067\n",
      "Epoch 18/18\n",
      "23s - loss: 2.1908 - acc: 0.2251 - val_loss: 2.2733 - val_acc: 0.1333\n",
      "logloss val 2.27333394686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:24: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "def fifth_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(152, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.42))\n",
    "    model.add(Dense(52, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.18))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_5 = fifth_model()\n",
    "\n",
    "fit_5 = model_5.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69884,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "scores_val_5 = model_5.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val_5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Final prediction\n",
      "       F23-    F24-26    F27-28    F29-32    F33-42      F43+      M22-  \\\n",
      "0  0.000035  0.000198  0.000707  0.003975  0.021523  0.033455  0.000436   \n",
      "\n",
      "     M23-26    M27-28    M29-31    M32-38      M39+            device_id  \n",
      "0  0.011628  0.028034  0.105885  0.270119  0.524005  1002079943728939269  \n"
     ]
    }
   ],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_55 = model_5.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_55 = pd.DataFrame(scores_55 , columns=lable_group.classes_)\n",
    "result_55[\"device_id\"] = device_id\n",
    "print(result_55.head(1))\n",
    "result_55 = result_55.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result_3.to_csv('keras_' + str(log_loss(y_val, scores_val_3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# дальше разбавляем сетками, 10% пятой дало небольшой рост\n",
    "# xgboost не дает прироста\n",
    "# нужно не меньше 5-10 позиция для серебра"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot = 0.9*last + 0.1*result_55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot[tot<0.001]=0.001 # \n",
    "tot[tot>0.999]=0.999 # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot.to_csv('keras_test' + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res, score = run_xgb(train_sp, test_sp, Y, eta=0.07, random_state=0)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res1, score1 = run_xgb(train_sp, test_sp, Y, eta=0.07, random_state=100)\n",
    "print score1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res2, score2 = run_xgb(train_sp, test_sp, Y, eta=0.07, random_state=142)\n",
    "print score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_0 = pd.DataFrame(res, columns=lable_group.classes_)\n",
    "result_0[\"device_id\"] = device_id\n",
    "print(result_0.head(1))\n",
    "result_0 = result_0.set_index(\"device_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_01 = pd.DataFrame(res1, columns=lable_group.classes_)\n",
    "result_01[\"device_id\"] = device_id\n",
    "print(result_01.head(1))\n",
    "result_01 = result_01.set_index(\"device_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_02 = pd.DataFrame(res2, columns=lable_group.classes_)\n",
    "result_02[\"device_id\"] = device_id\n",
    "print(result_02.head(1))\n",
    "result_02 = result_02.set_index(\"device_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#result.to_csv('xgb.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################\n",
    "#  Build Model\n",
    "##################\n",
    "\n",
    "\n",
    "#act = keras.layers.advanced_activations.PReLU(init='zero', weights=None)\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(150, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(50, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "\n",
    "model = baseline_model()\n",
    "\n",
    "fit= model.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69984,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "scores_val = model.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val)))\n",
    "\n",
    "\n",
    "#print(\"# Final prediction\")\n",
    "#scores = model.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "#result = pd.DataFrame(scores , columns=lable_group.classes_)\n",
    "#result[\"device_id\"] = device_id\n",
    "#print(result.head(1))\n",
    "#result = result.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result.to_csv('keras_' + str(log_loss(y_val, scores_val)) + '.csv', index=True, index_label='device_id')\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def second_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(155, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(50, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_2 = second_model()\n",
    "\n",
    "fit_2 = model_2.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69784,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "scores_val_2 = model_2.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def third_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(145, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(50, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_3 = third_model()\n",
    "\n",
    "fit_3 = model_3.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69884,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "scores_val_3 = model_3.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forth_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(147, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(50, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_4 = forth_model()\n",
    "\n",
    "fit_4 = model_4.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69984,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "scores_val_4 = model_4.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val_4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fifth_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(152, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.42))\n",
    "    model.add(Dense(52, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.18))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_5 = fifth_model()\n",
    "\n",
    "fit_5 = model_5.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69884,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "scores_val_5 = model_5.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val_5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sixth_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(153, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.42))\n",
    "    model.add(Dense(51, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_6 = fifth_model()\n",
    "\n",
    "fit_6 = model_6.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69884,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "scores_val_6 = model_6.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val_5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seventh_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(144, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(50, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_7 = seventh_model()\n",
    "\n",
    "fit_7 = model_7.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69884,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "scores_val_7 = model_7.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val_7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eight_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(145, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(51, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_8 = eight_model()\n",
    "\n",
    "fit_8 = model_8.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69784,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "scores_val_8 = model_8.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val_8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''def ninth_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(156, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(51, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model_9 = ninth_model()\n",
    "\n",
    "fit_9 = model_9.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=18,\n",
    "                         samples_per_epoch=69884,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "scores_val_9 = model_9.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val_9)))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_3 = model_3.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_3 = pd.DataFrame(scores_3 , columns=lable_group.classes_)\n",
    "result_3[\"device_id\"] = device_id\n",
    "print(result_3.head(1))\n",
    "result_3 = result_3.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result_3.to_csv('keras_' + str(log_loss(y_val, scores_val_3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_2 = model_2.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_2 = pd.DataFrame(scores_2 , columns=lable_group.classes_)\n",
    "result_2[\"device_id\"] = device_id\n",
    "print(result_2.head(1))\n",
    "result_2 = result_2.set_index(\"device_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores = model.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result = pd.DataFrame(scores , columns=lable_group.classes_)\n",
    "result[\"device_id\"] = device_id\n",
    "print(result.head(1))\n",
    "result = result.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('keras_' + str(log_loss(y_val, scores_val)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_4 = model_4.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_4 = pd.DataFrame(scores_4 , columns=lable_group.classes_)\n",
    "result_4[\"device_id\"] = device_id\n",
    "print(result_4.head(1))\n",
    "result_4 = result_4.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result_3.to_csv('keras_' + str(log_loss(y_val, scores_val_3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_5 = model_5.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_5 = pd.DataFrame(scores_5 , columns=lable_group.classes_)\n",
    "result_5[\"device_id\"] = device_id\n",
    "print(result_5.head(1))\n",
    "result_5 = result_5.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result_3.to_csv('keras_' + str(log_loss(y_val, scores_val_3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_6 = model_6.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_6 = pd.DataFrame(scores_6 , columns=lable_group.classes_)\n",
    "result_6[\"device_id\"] = device_id\n",
    "print(result_6.head(1))\n",
    "result_6 = result_6.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result_3.to_csv('keras_' + str(log_loss(y_val, scores_val_3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_7 = model_7.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_7 = pd.DataFrame(scores_7 , columns=lable_group.classes_)\n",
    "result_7[\"device_id\"] = device_id\n",
    "print(result_7.head(1))\n",
    "result_7 = result_7.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result_3.to_csv('keras_' + str(log_loss(y_val, scores_val_3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"# Final prediction\")\n",
    "scores_8 = model_8.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result_8 = pd.DataFrame(scores_8 , columns=lable_group.classes_)\n",
    "result_8[\"device_id\"] = device_id\n",
    "print(result_8.head(1))\n",
    "result_8 = result_8.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result_3.to_csv('keras_' + str(log_loss(y_val, scores_val_3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(\"# Final prediction\")\n",
    "#scores_9 = model_9.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "#result_9 = pd.DataFrame(scores_9 , columns=lable_group.classes_)\n",
    "#result_9[\"device_id\"] = device_id\n",
    "#print(result_9.head(1))\n",
    "#result_9 = result_9.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "#result_3.to_csv('keras_' + str(log_loss(y_val, scores_val_3)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tot = result + result_2 + result_3 + result_4 + result_5 + result_6\n",
    "#tot = result_3 + result_5 + result_6 \n",
    "#tot = result + result_2 + result_3 + result_4 + result_5 + result_6 + result_7\n",
    "#tot = result + result_2 + result_3 + result_4 + result_5 + result_6 + result_7 + result_8 + result_9\n",
    "tot = result + result_2 + result_3 + result_5 + result_6 + result_7 + result_8 + result_0 + result_01 + result_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tot = tot/6\n",
    "#tot = tot/3\n",
    "#tot = tot/7\n",
    "#tot = tot/8\n",
    "tot = tot/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tot[tot<0.001]=0.001 # 1,2,3,4,5,5 models + this -> 2.23982\n",
    "tot[tot>0.999]=0.999 # 1,2,3,4,5,5 models + this -> 2.23982"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tot.to_csv('keras_' + str(log_loss(y_val, (scores_val + scores_val_2 + scores_val_3 + scores_val_4 + scores_val_5 + scores_val_6)/6)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot.to_csv('keras_' + str(log_loss(y_val, (scores_val + scores_val_2 + scores_val_3 + scores_val_5 + scores_val_6 + scores_val_7 + scores_val_8)/1)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tot.to_csv('keras_' + str(log_loss(y_val, (scores_val + scores_val_2 + scores_val_3 + scores_val_4 + scores_val_5 + scores_val_6 + scores_val_7 + scores_val_8 + scores_val_9)/9)) + '.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
