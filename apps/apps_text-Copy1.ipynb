{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import zipfile\n",
    "import time\n",
    "import shutil\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gender_train = pd.read_csv('gender_age_train.csv')\n",
    "gender_test = pd.read_csv('gender_age_test.csv')\n",
    "phone_brand = pd.read_csv('phone_brand_device_model.csv')\n",
    "app_events = pd.read_csv('app_events.csv', dtype={'app_id': np.str})\n",
    "app_labels = pd.read_csv('app_labels.csv',dtype={'app_id': np.str})\n",
    "labels = pd.read_csv('label_categories.csv')\n",
    "events = pd.read_csv('events_new.csv', index_col=0)\n",
    "app_text = pd.read_csv('app_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#app_cat = pd.merge(app_labels,labels,on='label_id')\n",
    "#app_cat_un = app_cat.groupby('app_id', as_index=False).agg({'category' : lambda x: ' '.join(x)})\n",
    "#app_cat_un.shape\n",
    "#app_ev = pd.merge(app_events, app_cat_un, on='app_id')\n",
    "#app_ev.shape\n",
    "#app_text = app_ev.groupby('event_id', as_index = False).agg({'category' : lambda x: ' '.join(x), 'app_id' : lambda x: ' '.join(x)})\n",
    "#app_text.shape\n",
    "#app_text.to_csv('app_text.csv',index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data - events and app_text\n",
    "data = pd.merge(events, app_text, on='event_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop columns from app_ev (?????)\n",
    "# where is app_ev (?)\n",
    "app_ev = app_events.drop(['is_installed', 'is_active'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# app_cnt - count of events for any app\n",
    "app_cnt = app_ev.groupby('app_id', as_index=False)['event_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data - merge of data and app_events\n",
    "data = pd.merge(data, app_events, on='event_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>device_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>long_lat</th>\n",
       "      <th>is_morning</th>\n",
       "      <th>is_night</th>\n",
       "      <th>is_day</th>\n",
       "      <th>is_evening</th>\n",
       "      <th>category</th>\n",
       "      <th>app_id_x</th>\n",
       "      <th>app_id_y</th>\n",
       "      <th>is_installed</th>\n",
       "      <th>is_active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>-6401643145415154744</td>\n",
       "      <td>2016-05-01 00:54:12</td>\n",
       "      <td>103.65</td>\n",
       "      <td>30.97</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>103.65_30.97</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Property Industry 1.0 Industry tag Relatives 1...</td>\n",
       "      <td>5927333115845830913 -5720078949152207372 -1633...</td>\n",
       "      <td>5927333115845830913</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-6401643145415154744</td>\n",
       "      <td>2016-05-01 00:54:12</td>\n",
       "      <td>103.65</td>\n",
       "      <td>30.97</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>103.65_30.97</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Property Industry 1.0 Industry tag Relatives 1...</td>\n",
       "      <td>5927333115845830913 -5720078949152207372 -1633...</td>\n",
       "      <td>-5720078949152207372</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id            device_id            timestamp  longitude  latitude  \\\n",
       "0         2 -6401643145415154744  2016-05-01 00:54:12     103.65     30.97   \n",
       "1         2 -6401643145415154744  2016-05-01 00:54:12     103.65     30.97   \n",
       "\n",
       "   hour  weekday      long_lat is_morning is_night is_day is_evening  \\\n",
       "0     0        6  103.65_30.97      False     True  False      False   \n",
       "1     0        6  103.65_30.97      False     True  False      False   \n",
       "\n",
       "                                            category  \\\n",
       "0  Property Industry 1.0 Industry tag Relatives 1...   \n",
       "1  Property Industry 1.0 Industry tag Relatives 1...   \n",
       "\n",
       "                                            app_id_x              app_id_y  \\\n",
       "0  5927333115845830913 -5720078949152207372 -1633...   5927333115845830913   \n",
       "1  5927333115845830913 -5720078949152207372 -1633...  -5720078949152207372   \n",
       "\n",
       "   is_installed  is_active  \n",
       "0             1          1  \n",
       "1             1          0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO drop duplicates from data (!!!)\n",
    "# duplicates by event_id, device and timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset=['device_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# kick duplicates from phone_brand\n",
    "phone_brand.drop_duplicates(subset='device_id',keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# change bool values to int type\n",
    "#data.is_morning = data.is_morning.astype(int)\n",
    "#data.is_night = data.is_night.astype(int)\n",
    "#data.is_day = data.is_day.astype(int)\n",
    "#data.is_evening = data.is_evening.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train and test data - merge of gender... and data\n",
    "train_data = pd.merge(gender_train, data, on='device_id', how='left')\n",
    "test_data = pd.merge(gender_test, data, on='device_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train and test data - merge of ..._data and phone_brand\n",
    "train_data = pd.merge(train_data, phone_brand, on='device_id')\n",
    "test_data = pd.merge(test_data, phone_brand, on='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fill na values with no_data\n",
    "train_data = train_data.fillna('no_data')\n",
    "test_data = test_data.fillna('no_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# app_events_sum - grouped by event app_events\n",
    "#app_events_sum = app_events.groupby(['event_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count of apps, count of installed, count of active\n",
    "#app_cnt = app_events.groupby(['event_id'], as_index=False)['app_id'].count()\n",
    "#installed_sum = app_events.groupby(['event_id'], as_index=False)['is_installed'].sum()\n",
    "#active_sum = app_events.groupby(['event_id'], as_index=False)['is_active'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summary of previous counts\n",
    "#app_data_med = pd.merge(app_cnt, installed_sum, on=['event_id'])\n",
    "#app_data = pd.merge(app_data_med, active_sum, on=['event_id'])\n",
    "#app_data['active_share'] = app_data.is_active/app_data.app_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop gender and age from train_data\n",
    "#train_data = train_data.drop(['gender','age'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group = train_data['group']\n",
    "train_data = train_data[['device_id', 'category', 'app_id_x', 'phone_brand', 'device_model']]\n",
    "test_data = test_data[['device_id', 'category', 'app_id_x', 'phone_brand', 'device_model']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.category = train_data.category.astype(str)\n",
    "train_data.app_id_x = train_data.app_id_x.astype(str)\n",
    "train_data.phone_brand = train_data.phone_brand.astype(str)\n",
    "train_data.device_model = train_data.device_model.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data['text'] = train_data.category + ' ' + train_data.app_id_x + ' ' + \\\n",
    "train_data.phone_brand + ' ' + train_data.device_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['text'] = test_data.category + ' ' + test_data.app_id_x + ' ' + \\\n",
    "test_data.phone_brand + ' ' + test_data.device_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74645, 6)\n",
      "(112071, 6)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total = pd.concat([train_data, test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(186716, 6)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2, max_df=0.9, stop_words='english')\n",
    "X = vectorizer.fit_transform(total.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74645, 12985)\n",
      "(112071, 12985)\n"
     ]
    }
   ],
   "source": [
    "print(X[:train_data.shape[0]].shape)\n",
    "print(X[train_data.shape[0]:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# count devices in events\n",
    "hour_cnt = train_data.groupby(['device_id'], as_index=False)['hour'].count().rename(columns={'hour':'device_cnt'})\n",
    "# hour activities\n",
    "hour_avg = train_data.groupby(['device_id'], as_index=False)['hour'].mean().rename(columns={'hour':'hour_avg'})\n",
    "hour_med = train_data.groupby(['device_id'], as_index=False)['hour'].median().rename(columns={'hour':'hour_med'})\n",
    "hour_min = train_data.groupby(['device_id'], as_index=False)['hour'].min().rename(columns={'hour':'hour_min'})\n",
    "hour_max = train_data.groupby(['device_id'], as_index=False)['hour'].max().rename(columns={'hour':'hour_max'})\n",
    "# weekday activities\n",
    "weekday_avg = train_data.groupby(['device_id'], as_index=False)['weekday'].mean().rename(columns={'weekday':'weekday_avg'})\n",
    "weekday_med = train_data.groupby(['device_id'], as_index=False)['weekday'].median().rename(columns={'weekday':'weekday_med'})\n",
    "weekday_min = train_data.groupby(['device_id'], as_index=False)['weekday'].min().rename(columns={'weekday':'weekday_min'})\n",
    "weekday_max = train_data.groupby(['device_id'], as_index=False)['weekday'].max().rename(columns={'weekday':'weekday_max'})\n",
    "# part of day activities\n",
    "morning_sum = train_data.groupby(['device_id'], as_index=False)['is_morning'].sum().rename(columns={'is_morning':'morning_sum'})\n",
    "morning_avg = train_data.groupby(['device_id'], as_index=False)['is_morning'].mean().rename(columns={'is_morning':'morning_avg'})\n",
    "day_sum = train_data.groupby(['device_id'], as_index=False)['is_day'].sum().rename(columns={'is_morning':'day_sum'})\n",
    "day_avg = train_data.groupby(['device_id'], as_index=False)['is_day'].mean().rename(columns={'is_morning':'day_avg'})\n",
    "night_sum = train_data.groupby(['device_id'], as_index=False)['is_night'].sum().rename(columns={'is_night':'night_sum'})\n",
    "night_avg = train_data.groupby(['device_id'], as_index=False)['is_night'].mean().rename(columns={'is_night':'night_avg'})\n",
    "evening_sum = train_data.groupby(['device_id'], as_index=False)['is_evening'].sum().rename(columns={'is_evening':'evening_sum'})\n",
    "evening_avg = train_data.groupby(['device_id'], as_index=False)['is_evening'].mean().rename(columns={'is_evening':'evening_avg'})\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# count devices in events\n",
    "hour_cnt_test = test_data.groupby(['device_id'], as_index=False)['hour'].count().rename(columns={'hour':'device_cnt'})\n",
    "# hour activities\n",
    "hour_avg_test = test_data.groupby(['device_id'], as_index=False)['hour'].mean().rename(columns={'hour':'hour_avg'})\n",
    "hour_med_test = test_data.groupby(['device_id'], as_index=False)['hour'].median().rename(columns={'hour':'hour_med'})\n",
    "hour_min_test = test_data.groupby(['device_id'], as_index=False)['hour'].min().rename(columns={'hour':'hour_min'})\n",
    "hour_max_test = test_data.groupby(['device_id'], as_index=False)['hour'].max().rename(columns={'hour':'hour_max'})\n",
    "# weekday activities\n",
    "weekday_avg_test = test_data.groupby(['device_id'], as_index=False)['weekday'].mean().rename(columns={'weekday':'weekday_avg'})\n",
    "weekday_med_test = test_data.groupby(['device_id'], as_index=False)['weekday'].median().rename(columns={'weekday':'weekday_med'})\n",
    "weekday_min_test = test_data.groupby(['device_id'], as_index=False)['weekday'].min().rename(columns={'weekday':'weekday_min'})\n",
    "weekday_max_test = test_data.groupby(['device_id'], as_index=False)['weekday'].max().rename(columns={'weekday':'weekday_max'})\n",
    "# part of day activities\n",
    "morning_sum_test = test_data.groupby(['device_id'], as_index=False)['is_morning'].sum().rename(columns={'is_morning':'morning_sum'})\n",
    "morning_avg_test = test_data.groupby(['device_id'], as_index=False)['is_morning'].mean().rename(columns={'is_morning':'morning_avg'})\n",
    "day_sum_test = test_data.groupby(['device_id'], as_index=False)['is_day'].sum().rename(columns={'is_morning':'day_sum'})\n",
    "day_avg_test = test_data.groupby(['device_id'], as_index=False)['is_day'].mean().rename(columns={'is_morning':'day_avg'})\n",
    "night_sum_test = test_data.groupby(['device_id'], as_index=False)['is_night'].sum().rename(columns={'is_night':'night_sum'})\n",
    "night_avg_test = test_data.groupby(['device_id'], as_index=False)['is_night'].mean().rename(columns={'is_night':'night_avg'})\n",
    "evening_sum_test = test_data.groupby(['device_id'], as_index=False)['is_evening'].sum().rename(columns={'is_evening':'evening_sum'})\n",
    "evening_avg_test = test_data.groupby(['device_id'], as_index=False)['is_evening'].mean().rename(columns={'is_evening':'evening_avg'})\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "result_train = pd.merge(hour_cnt, hour_avg, on='device_id')\n",
    "result_train = pd.merge(result_train, hour_med, on='device_id')\n",
    "result_train = pd.merge(result_train, hour_min, on='device_id')\n",
    "result_train = pd.merge(result_train, hour_max, on='device_id')\n",
    "result_train = pd.merge(result_train, weekday_avg, on='device_id')\n",
    "result_train = pd.merge(result_train, weekday_med, on='device_id')\n",
    "result_train = pd.merge(result_train, weekday_min, on='device_id')\n",
    "result_train = pd.merge(result_train, weekday_max, on='device_id')\n",
    "result_train = pd.merge(result_train, morning_sum, on='device_id')\n",
    "result_train = pd.merge(result_train, morning_avg, on='device_id')\n",
    "result_train = pd.merge(result_train, day_sum, on='device_id')\n",
    "result_train = pd.merge(result_train, day_avg, on='device_id')\n",
    "result_train = pd.merge(result_train, evening_sum, on='device_id')\n",
    "result_train = pd.merge(result_train, evening_avg, on='device_id')\n",
    "result_train = pd.merge(result_train, night_sum, on='device_id')\n",
    "result_train = pd.merge(result_train, night_avg, on='device_id')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "result_test = pd.merge(hour_cnt_test, hour_avg_test, on='device_id')\n",
    "result_test = pd.merge(result_test, hour_med_test, on='device_id')\n",
    "result_test = pd.merge(result_test, hour_min_test, on='device_id')\n",
    "result_test = pd.merge(result_test, hour_max_test, on='device_id')\n",
    "result_test = pd.merge(result_test, weekday_avg_test, on='device_id')\n",
    "result_test = pd.merge(result_test, weekday_med_test, on='device_id')\n",
    "result_test = pd.merge(result_test, weekday_min_test, on='device_id')\n",
    "result_test = pd.merge(result_test, weekday_max_test, on='device_id')\n",
    "result_test = pd.merge(result_test, morning_sum_test, on='device_id')\n",
    "result_test = pd.merge(result_test, morning_avg_test, on='device_id')\n",
    "result_test = pd.merge(result_test, day_sum_test, on='device_id')\n",
    "result_test = pd.merge(result_test, day_avg_test, on='device_id')\n",
    "result_test = pd.merge(result_test, evening_sum_test, on='device_id')\n",
    "result_test = pd.merge(result_test, evening_avg_test, on='device_id')\n",
    "result_test = pd.merge(result_test, night_sum_test, on='device_id')\n",
    "result_test = pd.merge(result_test, night_avg_test, on='device_id')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#result_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#result_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_column(table, f):\n",
    "    labels = sorted(table[f].unique())\n",
    "    mappings = dict()\n",
    "    for i in range(len(labels)):\n",
    "        mappings[labels[i]] = i\n",
    "    table = table.replace({f: mappings})\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#phone_brand['phone+brand'] = phone_brand.phone_brand + phone_brand.device_model\n",
    "#phone_brand = phone_brand.drop(['phone_brand','device_model'],axis=1)\n",
    "#phone_brand = pd.get_dummies(phone_brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#result_test = pd.merge(result_test, phone_brand, on='device_id')\n",
    "#result_train = pd.merge(result_train, phone_brand, on='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#result_train = pd.merge(result_train, gender_train, on='device_id')\n",
    "#result_train = result_train.drop(['gender','age'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#result_train = map_column(result_train, 'group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.seed(2016)\n",
    "\n",
    "def run_xgb(train, test, features, target, eta=0.1, random_state=0):\n",
    "    #eta = 0.1\n",
    "    max_depth = 3\n",
    "    subsample = 0.7\n",
    "    colsample_bytree = 0.7\n",
    "    start_time = time.time()\n",
    "\n",
    "    print('XGBoost params. ETA: {}, MAX_DEPTH: {}, SUBSAMPLE: {}, COLSAMPLE_BY_TREE: {}'.format(eta, max_depth, subsample, colsample_bytree))\n",
    "    params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"num_class\": 12,\n",
    "        \"booster\" : \"gbtree\",\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"eta\": eta,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"subsample\": subsample,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"silent\": 1,\n",
    "        \"seed\": random_state,\n",
    "    }\n",
    "    num_boost_round = 2000\n",
    "    early_stopping_rounds = 100\n",
    "    test_size = 0.3\n",
    "\n",
    "    X_train, X_valid = train_test_split(train, test_size=test_size, random_state=random_state)\n",
    "    # TODO change split \n",
    "    print('Length train:', len(X_train.index))\n",
    "    print('Length valid:', len(X_valid.index))\n",
    "    y_train = X_train[target]\n",
    "    y_valid = X_valid[target]\n",
    "    # TODO delete upper code\n",
    "    dtrain = xgb.DMatrix(X_train[features], y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid[features], y_valid)\n",
    "    # sparse matrix ???\n",
    "\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "    gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n",
    "\n",
    "    print(\"Validating...\")\n",
    "    check = gbm.predict(xgb.DMatrix(X_valid[features]), ntree_limit=gbm.best_iteration)\n",
    "    score = log_loss(y_valid.tolist(), check)\n",
    "\n",
    "    print(\"Predict test set...\")\n",
    "    test_prediction = gbm.predict(xgb.DMatrix(test[features]), ntree_limit=gbm.best_iteration)\n",
    "\n",
    "    print('Training time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "    return test_prediction.tolist(), score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_submission(score, test, prediction):\n",
    "    # Make Submission\n",
    "    now = datetime.datetime.now()\n",
    "    sub_file = 'submission_' + str(score) + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "    print('Writing submission: ', sub_file)\n",
    "    f = open(sub_file, 'w')\n",
    "    f.write('device_id,F23-,F24-26,F27-28,F29-32,F33-42,F43+,M22-,M23-26,M27-28,M29-31,M32-38,M39+\\n')\n",
    "    total = 0\n",
    "    test_val = test['device_id'].values\n",
    "    for i in range(len(test_val)):\n",
    "        str1 = str(test_val[i])\n",
    "        for j in range(12):\n",
    "            str1 += ',' + str(prediction[i][j])\n",
    "        str1 += '\\n'\n",
    "        total += 1\n",
    "        f.write(str1)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#res, score = run_xgb(result_train, result_test, result_train.columns.values[:len(result_train.columns)-1], 'group', eta=0.01, random_state=0)\n",
    "#print score\n",
    "#create_submission(score, result_test, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create_submission(score, result_test, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.cross_validation #импортируем кросс-валидацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "scaled_train = sc.fit_transform(result_train[result_train.columns.difference(['group','device_id'])])\n",
    "scaled_test = sc.fit_transform(result_test[result_test.columns.difference(['group'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scaled_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-03ceeb54e9a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlogr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.000001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#логистическая регрессия\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#lr_models.append(logr.fit(scaled_train_bag, features_target))#обучаем класификатор\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mbag_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'group'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'log_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m#к списку результатов добавляем средний получившийся на кросс-валидации результат по roc_auc score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mbag_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scaled_train' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "bag_score = []#список результатов\n",
    "c_est = [10**-2,10**-1,1,10,100,1000]#значения C для перебора\n",
    "lr_models = []\n",
    "for c in c_est:\n",
    "    start_time = datetime.datetime.now()\n",
    "    logr = LogisticRegression(penalty='l2',C=c,tol=0.000001,verbose=True)#логистическая регрессия\n",
    "    #lr_models.append(logr.fit(scaled_train_bag, features_target))#обучаем класификатор\n",
    "    bag_score.append(sklearn.cross_validation.cross_val_score(logr, scaled_train, result_train['group'].values,cv=3, scoring='log_loss').mean())\n",
    "    #к списку результатов добавляем средний получившийся на кросс-валидации результат по roc_auc score\n",
    "    print bag_score\n",
    "    print 'Time elapsed for c =', c,':', datetime.datetime.now() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print bag_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
